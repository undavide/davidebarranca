var hostname = "http://localhost:4000";
var index = lunr(function () {
    this.field('title')
    this.field('content', {boost: 10})
    this.field('category')
    this.field('tags')
    this.ref('id')
});



    index.add({
      title: "Photoshop JS Native Applications Course UPDATE",
      category: ["Development"],
      content: "As you have heard, I’ve spent the last few months working on my next video-course titled JavaScript Native Applications for Photoshop. The good news is that I am finally done with the recordings! They’ve already been edited, I just need to double-check them one last time and the content will be ready.\n\n\n\nBloom (the cat) is not included\n\n\nIt’ll take some extra time, because we’re talking about 28 videos for a whopping total of 9 hours of Full-HD content, where I am building the app from the ground up, i.e. starting with an empty folder.\n\nI also need to create thumbnails, marketing assets and the like (the part I admittedly enjoy the less). Keep an eye to this blog for the official release announcement!\n\nPricing and Availability\n\nThe full price is going to be:\n\n\n  $149 for individuals, with lifetime updates and HD videos for streaming.\n  $249 for enterprise licenses up to 10 seats, with lifetime updates and HD videos for download.\n\n\nFill the form below if you’re interested in receiving an early bird discounted offer before the course goes public.\n\n\n",
      tags: ["Native App","JavaScript","Machine Learning"],
      id: 0
    });
    

    index.add({
      title: "Photoshop JS Native Applications Course (work-in-progress)",
      category: ["Development"],
      content: "TL;DR\n\n\n  I am working on a new series of video-tutorials on Photoshop Development.\n  The subject is going to be Native Applications (.app for Mac and .exe for Windows, entirely written in Javascript) that connect to Photoshop.\n  Think CEP (aka HTML) Panels, but transplanted outside Photoshop.\n  The Demo App is 98% complete: for fun, it also implements a simple Machine Learning pre-trained model for image classification.\n  Availability: approx. Q3/2020, updates in the blog.\n  Pricing: $149 for individuals, $249 for companies; early bird discounts available.\n\n\nLong time no see\n\nI’ve slowed down a bit with blog-posting in the last year due to a mix of personal reasons (burnout recovery, house restoration, work, fighting with a neighbour-from-hell, this kind of stuff) plus a general lack of relevant things to say.\n\nIn the meantime, since late Q2/2019 I’ve been playing with the idea of building independent, native apps that can connect to Photoshop and act very much like CEP Panels would. As I’m slooooow and distracted, I’ve got to the point of having a working Demo App in my hands around March 2020 – not bad for a coding-sloth.\n\nWhy Native Apps?\n\nI got a bit tired of CEP: I wanted to allow myself the luxury of some more control over the development environment; if you’re a CEP developer I think you can relate. As anybody who’s been around in this business long enough knows, we’ve had the possibility to connect to PS from the outside for like 10 years (now you can call me a true sloth); reliable Native apps with JavaScript are here, so it was a no-brainer for me to give it a try, finally.\n\nPros:\n\n\n  You’re totally in charge over the technologies to use – no more “This has been fixed in PS 2029, but older PS versions are still out-of-luck”, or the cyclic “you’ve been doing X for a jolly bunch of years, guess what from now on it’s done the Y way” kind of troubles.\n  Still JS/Node.js stuff, easy transition if you come from a CEP background.\n  You can update to Chromium/Node-latest whenever you want for whatever PS version you want.\n  Native means independent, separate, dock-bouncing apps, with access to OS-level stuff like tray icons, notifications, FileSystem, you name it; Mac and Windows alike.\n\n\nCons:\n\n\n  Not all projects are a good fit for an independent app: if you need a tighter, super-fast integration with PS, stick to CEP or whatever default technology Adobe provides.\n  There’s a bit of a learning curve, but I’m here to ease the process.\n\n\nHow does it work, technically speaking?\n\nNW.js, Vue.js, Vuex, Socket.io, Adobe Generator Plugin, MobileNet, Node.js stuff, trial and error. For now you’ll make do with that.\n\nWhat will the Video Course cover?\n\nI’m going to build a demo app from scratch over the course of some hours of recording. Features are totally arbitrary, and it have been chosen to serve just one purpose: showing how things can be done in the context of a native app connecting to PS, i.e. solving specific problems that you’re likely to run into (e.g. passing data from/to PS, executing scripts, synchronous/asynchronous operations, storing permanent data between sessions, etc.)\n\n\n\nWhat does the demo app do?\n\n\n  the user selects a bunch of files from the disk\n  the user also toggles few Image Classification tags\n  the app retrieves ActionSets/Actions loaded in Photoshop\n  if each file’s subject matches the ones selected by the user (that one document actually depicts a tiger), Photoshop will process it with the selected Action\n  when the batch ends, a System Notification pops up some info\n\n\nAs simple as it looks, it’s enough to build a rather structured NW.js/Vue.js architecture that can serve as a blueprint for more complex projects of yours.\n\nEverything’s based on a solid build system, so 99% of the tasks are completely automated.\n\nCourse’s goals\n\nLike everything I’ve ever published, the main goal is to give you a substantial head-start, and save you all the time I’ve spent stuck trying to solve specific technical problems. As in life, there are many more ways to mess up than to get it right: this course will provide you with a selected range of working options to build commercial-grade applications.\n\nPricing and Availability\n\nI’m bugfixing the app, which functionally speaking is done. Knowing myself, it will take me some more months to record the actual content, so friendly advice: don’t hold your breath.\n\nIt’ll be priced according to previous courses of mine:\n\n\n  $149 for individuals, with lifetime updates and HD videos for streaming\n  $249 for enterprise licenses up to 10 seats, with lifetime updates and HD videos for download\n\n\nEarly bird discounts available. If you’re interested, please enter your email in the form below (no spam at all); you’ll receive a notification with a discounted offer before the course goes public.\n\n\n\nQuestions?\n\nUnless they’re along the lines of “Sir, why X instead of Y?”, I encourage you to post in the comments below. Thanks for reading and have a good day!\n",
      tags: ["Native App","JavaScript","Machine Learning"],
      id: 1
    });
    

    index.add({
      title: "The state of Hasselblad Flextight scanners, FlexColor and the 3F format",
      category: ["Photoshop"],
      content: "In case you missed it, Flextight scanners have vanished from the Hasselblad website around May 2019 and macOS Catalina doesn’t run FlexColor anymore. No software updates are planned, plus I’ve run into problems dealing with fff files in Photoshop 2020. Heavens!\n\nFlextight scanners\n\nFlextight X1 and X5, the flagship (and only left) models in Hasselblad’s lineup have been discontinued: EOL, kaputt. No official announcement has been published, but a number of users around the world have had the news confirmed either from dealers or servicing shops.\n\nIt’s pointless to ask the reason why, scanners have always been a low priority asset to Hasselblad. Flextight know-how and hardware were acquired in 2014 when the company merged with Imacon: reliable sources told me that the engineering team has always been quite small. My best bet is that the 2017 acquisition of Hasselblad by DJI, a Chinese drone-making company, gave scanners the coup de grace.\n\nThe good news is that servicing will be available for the foreseeable future, as customer service wrote me:\n\n\n  The Flextight scanners are full service supported in our factory service in Gothenburg, Sweden and a selected number of regional scanner partners are able to perform local service and preventive maintenance. We will continue to perform service for these scanners as long as we will have spare parts, therefore it is difficult to appreciate for how long.\n\n\nIn fact, this is what you can read when logging in the private area of the Hasselblad website.\n\n\n\nFlexColor software\n\nUnclear information has been given to users in the past, when many of us pinged Hasselblad about the pressing issue of macOS 10.15 Catalina blocking 32bit applications. Any hope to see a 64bit FlexColor rewrite has been definitely flushed away when customer support eventually started to reply that:\n\n\n  […] we currently have no plans to release an updated version of Flexcolor.\n\n\nAs expected, the software has been EOLed very much like the hardware. Not that it has ever received much care: the last available versions are 4.8.13 (2011, for Mac) and 4.8.9 (2012, for Windows).\n\n\n\n\n\nTo the best of my knowledge, the Windows version still runs on whatever version is Windows at now, but with macOS you’re capped to 10.14 Mojave.\n\nNeither FlexColor nor the scanners do require a high-performance computer, they’re probably going to work fine with old dedicated hardware for years to come. I think it’s possible to run Catalina and virtualize (with Parallels or VMWare) Mojave so that you can still run 32bit apps. Let’s hope that by the time the whole process becomes impractical or the spare parts are out of stock, the film we want to scan has been eroded away by chemical remains and fungi – checkmate, problem solved1.\n\nFinally, due to legal reasons (very likely the use of some internal libraries that prevent it) FlexColor can’t be open-sourced. There are rumors about VueScan willing (or having been asked) to implement a module that would drive the Flextight hardware, but I couldn’t been able to find any official statement.\n\nSo this is the state of affairs for the hardware, what about existing archives?\n\n3F files\n\nHere things are getting slightly more complex, please bear with me.\n\nAs we all know, FlexColor is used by archivists and photoshoppers alike to browse through, and extract .tif files from, huge collections of .fff (aka 3F) files. You may or may not know that the 3F format is nothing but a .tif file with some proprietary tags: to prove it, try renaming a .fff to .tif and open it in Photoshop. The extra stuff is:\n\n\n  the compressed/low-res bitmap preview that FlexColor uses to temporarily display the 3F while waiting to read the actual data from the file (only if the 3F isn’t too large, in which case you’re always only shown the preview)\n  metadata related to the scan acquisition and FlexColor settings (e.g. curves, FlexTouch, saturation, snapshots, etc.)\n\n\nI have grown accustomed to skip FlexColor altogether for anything but driving the scanner (acquiring the 3F), and browsing (i.e. visualizing the files). That is to say: I do not use FlexColor anymore for any 3F processing nor .tif extraction. I’ve never liked FlexColor rudimental, limited features – they were sort-of OK in the mid-2000s, or for large files batches quick post-processing. My client’s work today is more focused on a smaller selection of scans, to be post-produced very accurately, so I only use Photoshop.\n\nTo briefly sum up the way I use to work (which I can expand in a separate blogpost – leave a comment below in case it is of any interest):\n\n\n  I open the 3F in Photoshop (thanks to the Imacon3F.plugin or its .8bf Windows version), which in my case is going to appear as a color negative.\n  I apply an Invert adjustment layer and a couple of Curves adjustment layers to make it look more or less correct to my eyes – no need to be precise here.\n  I duplicate the background layer and perform all sorts of retouching voodoo (against dust, scratches, chemicals stains, fingerprints, semen, etc.), saving this intermediate layered file as a separate, temporary .psb.\n  When I’m happy, I flatten all the bitmap layers and discard all the adjustment layers, so that I’m back with a one-layer, color negative file again.\n  I re-open the original .fff file, drag and drop the retouched layer from the .psb, flatten, and save as .CLEAN.fff – this because one never knows what the future brings… I then trash the intermediate .psb.\n  I keep the .CLEAN.fff open: assign the FlexColor Input ICC, convert to Adobe RGB, Invert, and save my working file as a .psb. This is my starting point, to which I apply the usual Curves, and whatever it takes to make the image work.\n\n\nEnter another stray bullet in the Hasselblad game: Adobe. It turns out that Photoshop 2020 doesn’t want to open 3Fs anymore, popping up this new errors:\n\n\n\nIn recent versions, you already had to set the Open dialog’s settings to Enable: All Documents (and of course the Format: Imacon 3f) otherwise Adobe Camera Raw kicked in trying to open the file, mistaking it as coming from a digital back.\n\n\n\nThe last working version (at least on a Mac) is Photoshop CC 2019 – which is a big concern. Why? You may or may not have heard that in the last months Adobe has aggressively restricted the licensed versions range for Photoshop – e.g. you cannot, theoretically, install or run, say, Photoshop CC 2015 anymore. Without entering into yet another rabbit hole, it all boils down to royalties with third-parties, very likely Dolby2.\n\nIn the end, Adobe’s versions policy is (and it is fair to expect that it will be their default from now on) to license N-1 versions – in other words: you’ll be allowed to download and install the latest Photoshop, plus the version before it, period. At the time I’m writing this, it is PS 2020 + PS CC 2019. In one year, CC 2019 will be dropped, and it’ll be PS 2021 + PS 2020.\n\nWhich means: one year from now, the officially available Photoshop versions won’t open 3F files anymore, and working versions’ installers won’t be available for download either. LMAO.\n\nThere is also the tangentially related problem of macOS notarization/digital signature, from macOS Catalina onwards: .plugin files are requested to be at least3 signed by the developer, otherwise macOS GateKeeper will prevent to run them. This is not too much of a big deal, I have an active Apple Developer membership and I can sign the file myself if needed.\n\nTL;DR\n\n\n  Hasselblad Flextight X1 and X5 scanners (previously known as Imacon) are discontinued. Scanner servicing will be provided as usual as long as spare parts will be available.\n  FlexColor is abandonware: it doesn’t run on any macOS &gt; 10.14.\n  3F files don’t open anymore in Photoshop 2020; very likely the CC 2019 installer is not going to be available one year from now, and even if you have one, it’s definitely possible that macOS Catalina will prevent Imacon3F.plugin from working because it’s not signed/notarized.\n\n\nWorkarounds (if any)\n\nThe following is partly subjective.\n\n\n  Acquiring new scans, for the time being, requires FlexColor; so either you keep dedicated hardware running macOS Mojave, or you switch to Windows.\n  Browsing 3F collections is, and will keep being, a pain in the butt: FlexColor is the lesser evil, Adobe Bridge seems unable to load just the low-res preview that is embedded in .fff files and it takes forever to read the file content to create a thumbnail. The same holds true with Finder. I’m not aware of any other viable options for the Mac.\n  Extracting .tif files for 3F post-processing is not a problem, as long as the Photoshop plugin keeps working 🤞🏻.\n\n\nFeel free to comment below if you have updated news, workflow suggestions, or you just want to share your point of view as a Hasselblad user.\n  Thanks!\n\n\n\n\n  \n    \n      Please bear with me but I’m right in the middle of the postproduction of a batch of large format, color negative scans from the early 1980, and it’s a mess. Are you nostalgic of the old days of analogic photography? Bloody hell, no. &#8617;\n    \n    \n      Read here for more info about the Adobe-Dolby lawsuit. &#8617;\n    \n    \n      It is unclear to me, at the moment, whether a digital signature is enough or I would be required to notarize the .plugin as well – not a big deal either, but not running Catalina myself I cannot test it. &#8617;\n    \n  \n\n",
      tags: ["Hasselblad","Imacon","Flextight","FlexColor","3F"],
      id: 2
    });
    

    index.add({
      title: "Leanpub alternatives, IMHO",
      category: ["Digital Publishing"],
      content: "If you’re into technical writing, read along. I’ve used Leanpub’s services for two books of mine out of three, and I am still 100% sure that I couldn’t make a better choice. But when you’re more experienced, there might be reasons to look for alternatives: I’ll tell you mine, how I’ve sorted my doubts out, and what I’ve eventually chosen.\n\nTL;DR\n\nNotable options among others are Pandoc, SoftCover, LaTeX, Affinity Publisher. Read along to see the piece of technology that I’ve embraced, and why.\n\nMy own needs (which may differ from yours)\n\nI am into technical publishing/programming books: I’ve written three of them, that I refer to as “the two big ones” (more than 350 pages, Adobe Photoshop HTML Panels Development and Professional Photoshop Scripting), and “the short one” (less than 100 pages, The Ultimate Guide to Native Installers and Automated Build Systems).\n\n\n\n\n\nI do sell them on my own via Gumroad; I do the marketing/newsletter on my own; I find beta readers/tech editors on my own; I do design the book on my own. Basically, I’m ever such a grumpy, control-freak guy with some tech background who needs to produce the best looking .pdf that he can.\n\nStuff I very much like:\n\n\n  Being able to customize the page design\n  Relying on a repeatable, robust process\n  Open source / offline tools\n  Automated typesetting\n\n\nStuff I don’t particularly need/like:\n\n\n  Anything that goes beyond the PDF creation\n  Online tools\n\n\nTo some extent, I can trade easiness for customizability because I do not write regularly: I embarque on big projects about once every couple of years, I am not willing to reinvent the wheel each time.\n\nWhat follows is my personal quest for viable Leanpub alternatives: it is 100% based on my needs, skillset and amount of time I can currently devote on this.\n\nWhy am I looking for alternatives in the first place?\n\nIn my opinion, Leanpub is by far the most usable, well crafted service on the market for ebook authors, and I couldn’t recommend them more. My two big books have been produced by Leanpub, and could I go back in time I’d choose them again.\n\n\n\nProfessional Photoshop Scripting, a Leanpub book\n\n\nNot only they provide you with great tools for creating e-books, connecting authors with readers, managing sales, landing pages and the like, they are also heavily involved into advancing the industry with new standards such as Markua (a book-targeted, enhanced flavour of MarkDown). Lately, they have broaden their offer developing courses creation tools as well.\n\nIf you’re tackling your first book, then Leanpub is a no brainer. Stop reading this and go creating a new Author account on their website.\n\nThat said, the show stopper for me is that the PDF creation is a remote process – well managed, sync’d with Dropbox, hooked with GitHub/Bitbuckets, yet remote nonetheless.\n\n\n\nLeanpub remote PDF generation\n\n\nI don’t necessarily would like WYSIWYG, but synch’ing, waiting some minutes for the generation, and downloading is adding a friction that over time I’ve found unbearable. Especially when something doesn’t go the way you expect, i.e. if the layout requires many small tweaks, or worse the PDF generation crashes due to errors that aren’t easy to debug. When needed, Leanpub support never fails you; still, you need them to look at the log and find out what went wrong. Interrupting the creative flow is never a jolly business.\n\nFeatures wise, I have little to complain: some of the limitations I had when I wrote my two big books with Leanpub may have been overcome with the introduction of Markua, which wasn’t ready back then. Extra customization is always welcome, but the built-in set is comprehensive enough.\n\nThe Leanpub PDF generation\n\nTechnically speaking, at least with the MarkDown-in, PDF-out process, Leanpub relies on an intermediate LaTeX step; so it actually is MD to LaTeX to PDF (I suppose it is the same with Markua, but I can’t be sure). If you’re not familiar with it, LaTeX is a “document preparation system for high-quality typesetting” – or in layman terms, an horribly complex markup language.\n\n\n  MarkDown is a markup language too, but much simpler. It lets you write plain text with a very light and minimal markup so that titles are made with # symbols, the asterisks in **that** turn that bold, etc.\n\n\n\n\nA MarkDown example (this blogpost)\n\n\nIn a way, you might simplify matters saying that, among the rest, Leanpub has put together a very well designed set of LaTeX styles on top of a well-crafted MD conversion, so that the resulting PDF is remarkably good (setting aside .epub and .mobi versions, that are not a strict requirement for me).\n\nLeanpub’s Pros and Cons\n\nIMHO, on the plus side: writing in MarkDown is almost frictionless, the MD to PDF workflow frees your mind and lets you concentrate on producing the actual content, Markua is a promising evolution, they offer a ton of marketing features.\n\nIMHO, the cons: there is a limited set of available customizations, it’s a paid service (as it should be!), above all the PDF generation doesn’t happen locally but on their servers – which is the real obstacle for me.\n\nHow I have evaluated alternatives\n\nI am not the kind of guy who would write a book using Libre Office, and exporting it to PDF right away. It may be super handy for articles, yet I don’t think it is up to the task for books. Or possibly it is, but either I am not familiar enough with it to deal with named cross-references, etc. or it is not a good fit for programming books: e.g., my stuff involves a good deal of listings (programming code, mostly but not exclusively written in JavaScript) that require to be numbered and colored with proper syntax-highlighting, etc.\n\nA tradeoff that I can bear: giving up some of the easiness that comes with writing-in-MarkDown-and-forget-about-everything-else. I.e., I am ready to decouple the writing from the typesetting processes.\n\nPandoc\n\nPandoc is an open source set of tools that let you convert between formats - a lot of them, including MarkDown,  ePub, Microsoft Word, InDesign .idml, PDF (via LaTeX). If you need it, you can get get a .mobi from an .epub with the help of Calibre.\n\nPandoc is super-awesome, it is free, it runs as a command-line utility from the Terminal for Mac, Win and Linux, it provides the same sort of MD to LaTeX to PDF workflow that Leanpub sports. There’s an elephant in the room, though: you must rely on a set of custom LaTeX styles to tweak the PDF appearance. In other words, the default conversion is elegant as almost any LaTeX document is by default, but rather dull: if you want to use some of the fancier stuff and you don’t know your LaTeX… you’re totally out of luck.\n\nOf course there is plenty of templates. I did play a little bit with one called Eisvogel, but I couldn’t really figure out how to bend it to my needs, nor I did know how to “teach Pandoc” to interpret the MarkDown that I would like to be linked, say, to a custom LaTeX \\newcommand.\n\n\n\nThe Eisvogel LaTeX template for Pandoc\n\n\nIn the end, I was facing two distinct and quite steep learning curves: getting the hang of LaTeX and Pandoc at the same time, I got frightened and called quit.\n\nSoftcover\n\nI did use SoftCover for my short book a while ago, and I was quite pleased. Keeping it to the book generation process alone (Softcover offer sales services too), it is based on an open-source Ruby command-line set of tools to process MarkDown in, and output PDF, plus .epub, .mobi, and .html.\n\nSoftcover is more explicit in terms of intermediate steps, for it advocates the use of PolyTeX: “a strict subset of the LaTeX typesetting language”. Quite interestingly, you’re allowed to mix-in some LaTeX in the MarkDown, that works in combination with the Softcover provided styles. The more LaTeX you know, the better you can customize them.\n\n\n\nThe Softcover Manual\n\n\nAll in all, my experience with Softcover on the first book was OK. I had to tweak quite some things in the styles to my taste, and I’ve copied and pasted from TeX StackExchange more than a sensible developer would find appropriate – without really knowing what I was doing. Still, I did like more the design that comes with Leanpub out-of-the-box; on the plus side, I enjoyed very much the local PDF generation and PolyTeX features such as named cross-references.\n\n\n  Since I’ve mentioned them a couple of times already, a named cross reference is a piece of text (could be a title, a listing, a table, etc.) to which you attach a \\label, and to which you can refer to with the same label. Magic happens because you automatically get both the correct numeration and name (here, “figure 1.2”).\n\n\n\n\nNamed cross references\n\n\nUnfortunately, when I got back to my one year old short book markdown text to add some new content, the book generation failed. I couldn’t get to the root of the problem in a decent amount of time – a Ruby version issue, some dependencies missing, who knows. That mishap did leave me with a vague “lack of robustness” feeling: had I used Softcover more frequently, I would have been able to steer the wheel and correct any small issue that, inevitably, presented itself over time. But after such a long hiatus, I felt that using a tool on which I wasn’t in full control was too much of a gamble.\n\nAffinity Publisher\n\nI am a true believer on Affinity’s products lineup, so I’ve listed their Publisher instead of Adobe InDesign – even if I’m more familiar with the latter than the former1. That is to say that I have evaluated the possibility to build the book “manually”, with a page design application.\n\nI must confess that, even if I would have enjoyed the possibility to get a live, visual feedback and pixel precision positioning, I tend to keep InDesign and its breed at an arm distance whenever possible (which, thanks to my job, is not always): I’m afraid I just don’t like it. The idea to compose a short book, not to mention a long one, sounds dreadful to me.\n\n\n\nAffinity Publisher\n\n\nBesides, listings are not so easy to typeset on Publisher: you’d need to copy the code from, say, Visual Studio Code, paste it on Pages/Word, copy it again and eventually paste inside a Text Frame to retain the RTF information – i.e. to keep the Syntax Highlighting. Which is a PITA for code maintenance and it doesn’t really scale up when the book grows. I’ve logged a feature request for automated programming languages syntax highlighting on Affinity’s forums, and I’ve moved on.\n\nLaTeX\n\nFrankly, the idea of messing with master pages, character and paragraph styles, references, etc. on a page design application such as Affinity Publisher shedded a new, much brighter light on LaTeX. Which I started exploring. Given that I tend to fall in love with whatever piece of technology that you put in front of my nose, I tried to be unbiased.\n\nLaTeX is mostly used in academia, almost exclusively sciences and maths. It is known for providing an elegant typesetting out of the box, and equations so beautifully drawn that mathematicians could stare at them for hours on end.\n\n\n\nLaTeX equations example\n\n\nActually, it seems to have spread just because academic writers were such bad typesetters that they desperately needed some sort of tool that let them focus on the content only. That said, it strikes me that among the many hundreds of packages (i.e. add-on features) available for LaTeX, their documentation, that is obviously written in LaTeX, kind of stinks. Visually I mean, it’s ugly – which is quite ironic I suppose.\n\nLaTeX is unfriendly to say the least: the learning curve is undoubtedly steep. For some reason, it seems like documentation is chiefly reference-based, with rare occasional examples. You can easily find yourself browsing for 7 years old replies on TeX StackExchange thinking: “Sweet, that is probably gonna work” (would it be a JavaScript library, a 7 months old reply has already a good chance to be outdated).\n\nI subscribe 100% to the author of The LaTeX Fetish (Or: Don’t write in LaTeX! It’s just for typesetting), which has an interesting series of comments and replies. I could never write on LaTeX: it is too crowded, MarkDown is much less distracting to the eye. But I agree that LaTeX, a fossil from the past, still rocks as a typesetting tool.\n\nSo, I thought: if Pandoc, Softcover, and Leanpub are all LaTeX based; if I seem unable to just hack my way through them; wouldn’t it be better if I started with the real thing from scratch? And so I did.\n\nMy LaTeX journey\n\nFirst, I’ve set a simple goal: re-typeset the small book, that you may remember was built with Softcover so I had both the MarkDown and the PolyTeX code available.\n\nIf you want to try LaTeX, head here and download the distribution for Mac, Win or Linux. Speaking of applications (IDEs), there are freewares such as TeXstudio or TeXShop, but I’ve found more user-friendly Texpad (30$), or the free Visual Studio Code extension LaTeX Workshop.\n\n\n\nThe TexPad app, with almost-live preview\n\n\nI’ve started reading documentation and books: there are so many LaTeX Fetishists that manuals are abundant. In my experience, it is far better to start from a blank style (a file with a .sty extension) and add pieces one by one, learning what they do as you insert them, instead of borrowing a complete style from somebody else and hack it to your taste.\n\nLine after line, I’ve merged pieces from Softcover.sty,  Eisvogel.sty and stuff I’ve found online; apparently there is a package for everything, from footnotes to listings, quotes, colors, bookmarks, you name it2. It would be impossible to list them all here – the bottom-line is that creating a decent set of styles for a book that is at least up to Leanpub standards is in fact a doable task for anyone with some inclination to code tinkering. Dare I to say, I’ve even got better listings 😉\n\n\n\nCustom styled listings in LaTeX with the `listings` package\n\n\nAt this point I cannot call myself a LaTeX expert at all, but for sure a better hacker: at least I am able to understand what is in the realm of my possibilities and what’s better to keep away from. I’ve succeeded in re-typesetting my short book in two/three night-time weeks, for which I’m quite pleased.\n\nWhat only time will tell\n\nIt is to be seen whether I will perceive LaTeX as a robust solution, i.e. will the same source, untouched, compile flawlessly one year from now, on a different machine? I have the feeling that such is the case, unless my cat sits on the keyboard when I’m off the desk. LaTeX is just markup after all, and it is so modular that I doubt it will all crash.\n\nNow that I am slightly more acquainted with it, I may want to approach Pandoc again. No doubts that, as I’ve mentioned before, LaTeX isn’t a tool for writing but typesetting: I still miss the MarkDown to PDF workflow. If I can manage to create a template for Pandoc that takes into account all my listings styles and the like, I’ll be the happiest camper.\n\nConclusions\n\nAs it has already happened countless times to me, learning is rarely a direct path from point A to point B. I still have to find my dream workflow, but I think the one I’ve used – unless proven otherwise – is viable to say the least, and a good fit to the requirements I’ve set.\n\nIf you’re a technical writer feel free to comment below, I’m eager to listen to your experience. Thanks for reading!\n\n\n\n\n  \n    \n      Let’s not forget that, to date, Affinity isn’t Subscription based; and it has never told its users they can get sued for using old versions, like Adobe… &#8617;\n    \n    \n      My understanding is that LaTeX is a macro-language on top of TeX, which is way too complex to be used right away. In addition to what comes built-in with LaTeX itself, you’re usually advised to load packages: users contributed “macros” that make the process of implementing new features much easier. &#8617;\n    \n  \n\n",
      tags: ["Leanpub","Softcover","Pandoc","LaTeX"],
      id: 3
    });
    

    index.add({
      title: "Notarizing installers for macOS Catalina",
      category: ["HTML Panels tips"],
      content: "According to Apple (read here the whole article):\n\n\n  The Apple notary service is an automated system that scans your software for malicious content, checks for code-signing issues, and returns the results to you quickly. Notarization gives users more confidence that the Developer ID-signed software you distribute has been checked by Apple for malicious components. Notarization is not App Review.\n\n\nBasically, it is some kind of whitelisting. From macOS Catalina on (late September 2019), notarization is mandatory: your installers won’t work if you don’t do your homeworks.\n\nGatekeeper\n\nWhen your customers run your product’s installer.pkg, macOS Gatekeeper connects with Apple, checking whether the file you’re executing has been vetted and thus can be run safely. If this is the case, Gatekeeper pops up the usual warning anyway:\n\n\n\n\n\nBecause users can also be offline, you can staple the notary service’s response ticket directly to the installer – an optional but recommended step.\n\nWhat you need\n\n\n  An active subscription to the Apple Developer Program ($99 per year).\n  Xcode 10 or newer (free download via the App Store).\n  An app-specific password defined for your AppleID (see here for instruction).\n\n\nNotarization walkthrough\n\nI assume that you are at least vaguely familiar with the topic of code signing. I’m going to show you how to notarize a .pkg installer; caveats for slightly more complex scenarios (e.g. a .dmg that wraps a .pkg and/or .app plus other assets) will follow in a later section.\n\n1. Create a DeveloperID Installer certificate\n\nLog in the Apple Developer portal portal and follow the “Certificates, Identifiers &amp; Profiles” section. Create a new Certificate: when asked, select DeveloperID Installer:\n\n\n\n\n\nFollow the instruction to complete the process – you’ll be required to open the Certificate Assistant from the Keychain Access application to create a Certificate request, then upload it to the Developer Portal. When the process is complete, you’ll be presented with a download link for your certificate, to be imported via double click into the Keychain Access app.\n\nYou can verify that the certificate has been successfully installed opening the Terminal and pasting the following command:\n\nsecurity find-identity -v\n\n1) 6A7C3D1A28FA5483E5E8857F2A12EDC71731CAED \"Developer ID Installer: Davide Barranca (I2WD7PWGD9)\"\n  1 valid identities found\n\n\nKeep note of the string between quotes that says \"Developer ID Installer: ...\".\n\n2. Signing the Installer\n\nYou don’t just need to notarize an installer, it must be signed first. Assuming that the installer.pkg file sits in the folder you have cd to in the Terminal window, you sign the installer with the following command (the output I’ve got comes right below the first line):\n\nproductsign --sign \"Developer ID Installer: Davide Barranca\" ./installer.pkg ./installer_signed.pkg\n\n  productsign: using timestamp authority for signature\n  productsign: signing product with identity \"Developer ID Installer: Davide Barranca (I2WD7PWGD9)\" from keychain /Library/Keychains/System.keychain\n  productsign: adding certificate \"Developer ID Certification Authority\"\n  productsign: adding certificate \"Apple Root CA\"\n  productsign: Wrote signed product archive to ./installer_signed.pkg\n\n\nLet’s break down that:\n\n\n  productsign is the utility in charge of the signing.\n  --sign is the argument that defines the Identity associated to the Apple issued certificated, which follows as a string.\n  ./installer.pkg is the relative path (can be absolute as well) that points to the file you want to be signed.\n  ./installer_signed.pkg is the path of the new, signed file that is going to be created by productsign.\n\n\nOf course you should substitute the \"Developer ID Installer:\" string with the one you’ve noted in the previous section, as well as the files paths.\n\nYou can verify that the installer_signed.pkg has been really signed with:\n\npkgutil --check-signature ./installer_signed.pkg\n\n   Status: signed by a certificate trusted by Mac OS X\n   Certificate Chain:\n    1. Developer ID Installer: Davide Barranca (Y4PA7PZLM9)\n       SHA1 fingerprint: 3A 7C 3D 1E 28 AA 54 83 E5 E8 11 7F 2A 12 ED C7 17\n       --------------------------------------------------------------------    2. Developer ID Certification Authority\n       SHA1 fingerprint: 3B 16 6C 3B CC C4 B7 11 C9 FE 22 FA B9 13 56 41 E3       --------------------------------------------------------------------    3. Apple Root CA\n       SHA1 fingerprint: 61 1E 5B A2 2C 59 3A 08 FF 58 D1 61 E2 24 52 D1 98       \n\n\n3. Sending the notarization request\n\nKeep handy the app-specific password you’ve requested from your AppleID account (if you’ve not done it yet, follow the instruction here). Let’s say that it is cvbs-epfg-sizx-olwd. The command I’ve used to request the notarization is:\n\nxcrun altool --notarize-app --primary-bundle-id \"com.ccextensions.alce3\" --username \"YourAppleID@mail.com\" --password \"cvbs-epfg-sizx-olwd\" --file \"/full/path/to/the/installer_signed.pkg\"\n\n\nLet’s break down that.\n\n\n  xcrun is used to find and execute XCode commands.\n  altool is the executable that performs the notarization request.\n  --notarize-app is the flag that tells altool to request notarization.\n  --primary-bundle-id links to the installer I’m submitting its unique bundle id (in my case \"com.ccextensions.alce3\", substitute it with yours)\n  --username wants your AppleID.\n  --password wants the app-specific password you’ve requested (here, \"cvbs-epfg-sizx-olwd\")\n  --file is the flag that tells altool which file to upload. Please note that I had to specify the absolute full path (not the relative that starts with ./) between quotes\n\n\nThe command takes a while to upload the file, and gives you zero feedback; after a while, if the process has been successful, you get back something like:\n\n2019-09-18 16:30:35.659 altool[8788:92462] No errors uploading '/full/path/to/the/installer_signed.pkg'.\nRequestUUID = 181638fb-a618-2298-bff0-47fa79f01326\n\n\nKeep note of the RequestUUID string.\n\nAt this point your request for notarizationhas been sent to Apple, and you have to wait for them to process the file.\n\nMind you: if you get “Error: You must first sign the relevant contracts online. (1048)” (as it has happened to me several times) don’t panic.\n\nYou can launch XCode to check whether it asks you to accept its Terms and Conditions, or run the following command to agree from the Terminal:\n\nsudo xcodebuild -license\n\n\nThen try checking at iTunesConnect, in the section “Agreements, Tax and Banking”. The “Paid Apps” list item may have a Status of “Pending User Information”1, but there shouldn’t be any Agreement pending (usually their warnings are listed separately).\n\nIf, still, nothing works it may be a glitch on Apple’s side, so wait a couple of hours and try again.\n\n4. Checking the notarization status\n\nIn the Terminal, enter:\n\nxcrun altool --notarization-info 181638fb-a618-2298-bff0-47fa79f01326 --username \"YourAppleID@mail.com\" --password \"cvbs-epfg-sizx-olwd\"\n\n\nThe command uses now --notarization-info to ping the current status, and expects the same RequestUUID string that has been sent you in response of the original request. The result is something like that:\n\n2019-09-18 16:31:48.449 altool[9133:95654] No errors getting notarization info.\n\n   RequestUUID: 181638fb-a618-2298-bff0-47fa79f01326\n          Date: 2019-09-18 14:30:36 +0000\n        Status: in progress\n    LogFileURL: (null)\n\n\nPlease note the in progress status. At some point the result will be different, and hopefully similar to this one:\n\n   RequestUUID: 287628eb-e628-4998-b4b0-47fa79f07386\n          Date: 2019-09-18 14:30:36 +0000\n        Status: success\n    LogFileURL: https://osxapps-ssl.itunes.apple.com/awfully/long/URL\n    Status Code: 0\nStatus Message: Package Approved\n\n\nThe installer has been successfully notarized!\n\nIf, for some reason, you have to perform several requests and want to check the notarization requests history, run the following command:\n\nxcrun altool --notarization-history 0 --username \"YourAppleID@mail.com\" --password \"cvbs-epfg-sizx-olwd\"\n\nNotarization History - page 0\n\nDate                      RequestUUID                          Status  Status Code Status Message   \n------------------------- ------------------------------------ ------- ----------- ----------------\n2019-09-18 14:30:36 +0000 181638fb-a618-2298-bff0-47fa79f01326 success 0           Package Approved\n\nNext page value: 1768817026000\n\n\n\n5. Stapling the ticket to the file\n\nAs I wrote, this step is optional but highly recommended: Gatekeeper will be able to find the whitelist info in the file itself, without the need to perform an online check.\n\nxcrun stapler staple \"/full/path/to/the/installer_signed.pkg\"\n\nProcessing: /full/path/to/the/installer_signed.pkg\nThe staple and validate action worked!\n\n\nThe command here is much simpler: you don’t need to pass any RequestUUID string yourself, for stapler will do the call home at Apple’s on its own.\n\nYou can check stapling details with:\n\nstapler validate --verbose \"/full/path/to/the/installer_signed.pkg\"\n\n\nThe result is too long to paste here, and frankly I’ve no idea what it means: as long as it ends with \"The validate action worked!\" you should be fine.\n\nAnd… you’re done 🍾\n\nIn my experience the Notarization can take from 60 seconds up to many hours (one time I’ve had the Terminal waiting and checking the status from 3PM to midnight). In theory it should be fast, in practice you can occasionally run into long delays.\n\nCaveats for different scenarios\n\nThe example I’ve shown is for one installer.pkg file. Let’s say that you (as I do) deliver to your customers a product.dmg file, that wraps the installer.pkg, an additional uninstaller.app (say, an app-ified AppleScript) and some documentation as well.\n\nIn this case, you need to:\n\n\n  Notarize only the outmost container (here the .dmg).\n  Sign all the executable children elements (here the .app and .pkg) and the .dmg as well.\n\n\nPlease note that in order to sign a .dmg you need a “Developer ID Application” certificate, instead of the “Developer ID Installer” I’ve used for the .pkg\n\nLastly, things may get a bit convoluted when it comes to extra libraries/bundles that may be called by your panel – if you feel like it’s your case, please read this thread.\n\nDMG Canvas update\n\nThe software that I use, and recommend, to build .dmg files is Araelium DMG Canvas, which has recently bumped to version 3.x (a paid upgrade, around $10). One of the new features is the possibility to automate the notarization process while building the .dmg itself.\n\n\n\n\n\nTo tell you the truth, the first product I’ve used that for went flawlessly, and in a snap; the second product failed due to a timeout, which sounds very much an Apple issue rather than Araelium’s. Please note that DMG Canvas does the stapling too, which is quite handy, and can be automatized to fit your build system.\n\nSupport this site!\n\nPlease consider supporting my work with the purchase of these books and courses. You can find them all here, bundles available. Thanks! 🙏🏻\n\n\n\n\n\nPS. I will update the Ultimate Guide to Native Installers and Automated Build Systems as well, I just wanted you to get informed asap.\n\n  \n    \n      If you don’t plan to sell through Apple, there’s no point in filling the uber-annoying Tax forms, etc. &#8617;\n    \n  \n\n",
      tags: ["Notarizing","Apple","macOS Catalina"],
      id: 4
    });
    

    index.add({
      title: "Printing on demand a Leanpub book with Lulu.com",
      category: ["Personal"],
      content: "\n\n\n\n\"And this shall be mine, human...\"\n\n\nAuthoring a book is awesome – especially when you’re done with it – but holding an actual object in your hands still gives a special kind of thrill. I’ve just run an ultra-small print batch of my Professional Photoshop Scripting book, that I have self-published with Leanpub and printed-on-demand with Lulu: in this post I’m going to describe the whole process, what I’ve learned from it, and what I’d do differently to make the result even better.\n\nTL;DR\nThere is some room for improvement but all considered I am pleased!\n\nDisclaimer\nIn my daytime job I also do some books designing for my client, when asked; I have a decent understanding of printing process’ technical aspects in general, and I am familiar with the related software. In a previous life of mine I’ve been a color-management consultant for press/pre-press too. Nonetheless, this whole “get the book printed!” business took me by surprise, and I wanted to go through it as swiftly as possible; as a consequence, I admit to have taken, sometimes, the quickest path (i.e., not the best one).\n\nWhy have I got a printed version of my book in the first place?\n\nI didn’t want to; I mean, it wasn’t planned when I was writing it. Jeffrey Tranberry (Sr. Product Manager, Digital Imaging at Adobe Systems, and long time Scripting advocate) has very kindly accepted to write the book’s foreword: he’s gotten in touch with me a few weeks later asking if it was possible for him to receive a signed copy. Sure, I’d be glad to! Although it took me a moment to get accustomed to the idea that I, Davide, was the one supposed to sign a book for Jeff and not the other way around – in 2012 he wrote Power, Speed &amp; Automation with Adobe Photoshop with Geoff Scott, the only other book on earth that touches PS Scripting.\n\nThe main problem was that I had no actual printed book to send, just a PDF. I felt morally obliged to fulfill Jeff’s fair request as soon as possible, so I started my research for an online, print-on-demand service.\n\nCreating an ebook with Leanpub\n\nMy starting point was an ebook that I self published with Leanpub. If you’re not familiar with them, you are allowed to:\n\n\n  […] write books in Markdown (our dialect is called Markua), Google Docs or our visual editor, and publish an ebook (in PDF, EPUB and MOBI) with one click. The Leanpub store helps you sell your book while it’s in-progress, getting reader feedback and earning 80% royalties. Or, upload a completed ebook to sell in our store.\n\n\n\n\nThe Leanpub book's author backend page\n\n\nI couldn’t recommend Leanpub more, seriously: as a company, they offer extra services that I don’t really need (book landing page, e-commerce, in-progress publishing, etc), but their writing platform alone is superb. I’ve already used Leanpub for my previous book Adobe Photoshop HTML Panels Development, hence I was no stranger to the whole process.\n\nI took some R&amp;D time to evaluate alternative options within my reach: I could use InDesign, iBooks Author, write in MarkDown and convert to PDF with Softcover or Pandoc1. After some pondering, I decided that they all required too much efforts on my side to customize the result compared to Leanpub design, which was excellent out-of-the-box; and so I went with it. The book in itself was already a tough enterprise to undertake, I’d be better off spending my time on content creation.\n\nWriting in MarkDown\n\nTo briefly describe the process, I’ve written the entire book in MarkDown – a textual format with handy shortcuts, e.g., _italic_, **bold**, [^footnotes], ![Images](graphic.png) and everything else a writer may need –, keeping the files in synch with Leanpub via Dropbox2.\n\n\n\nUsing the MWeb app to write and preview MarkDown.\n\n\nI’ve used Atom, MacDown and MWeb to write MarkDown, yet there’s plenty of apps (free or paid) available, even if to the best of my knowledge none of them supports the Leanpub’s flavour.  In the last years they have been working on the specs of Markua, a MarkDown superset that specifically targets eBooks creation; it is already available as an option to authors, but besides Leanpub’s Markua online editor I don’t think there are apps for it.\n\nYou can trigger the book compilation on-demand, either the entire content or just an excerpt to make the task quicker; the compilation itself is performed on their servers and the results (PDF, ePub and MOBI files) uploaded to a shared Dropbox folder automatically. This is the main Leanpub’s downside, in my opinion: depending on the amount of content you’ve written, the PDF generation process can take few minutes to complete: besides the transformation from MarkDown to PDF3, you have to wait for the files to get uploaded to Dropbox, and downloaded locally. Alas, we’re not used to waiting anymore.\n\nSpeaking of the PDF, which is what you’d use for printing purposes, their book design has a classic and professional look, very well suited to technical publishing like my project.\n\n\n\nThe Leanpub book's design is professional and well balanced\n\n\nKeep in mind that using a markup language also means that your content follows well defined formatting rules; while you focus on the writing, the book “composes itself”, so to speak. On the other side you don’t have pixel-perfect elements positioning as with, say,  InDesign. Some trial and error is intrinsic to the process, but it inevitably extends the waiting time in-between PDF generations.\n\nExporting for print\n\nLeanpub has a specific Export option called Print-Ready PDF, that is the one you should pick when your goal is ink on paper. It slightly differs from the traditional screen output, quoting from their website:\n\n\n  It has no cover image. Services like KDP and Lulu want you to upload a wraparound cover, so our ebook cover image would be redundant. Instead, it starts with the inside cover.\n  It has alternating page numbers, so that they end up on the outside corners. (The outside corners are the ones which are away from the book spine.)\n  It has alternating margins: the inside margins are often wider than the outside margins, so that you can actually see the text on the inside margins. (The inside margins are the ones which are next to the book spine.)\n  It has occasional blank pages to ensure that chapters start on the right side.\n\n\nThere is an alternative Export InDesign Files option that outputs InCopy .icml files, one per chapter: your job is to manually insert each chapter in a provided, dummy InDesign .indd file, which you can then customize to your book designer taste. Since I was in hurry, and Leanpub had a (now fixed) bug in the InDesign Export, I postulated that the Print-Ready PDF was a good enough starting point for me.\n\nChoosing a print-on-demand service\n\nMy dear friend and Photoshop retoucher Daniele Di Stanio promptly pointed me to an article titled “My experiences printing a small batch of books”. The goal of the post’s author was to publish\n\n\n  “a simple, old-school book with… letters in it”\n\n\nI.e., a novel. My book, instead, is in the technical publishing category (software development), with illustrations, screenshots, few photographs and a lot of syntax highlighted code. Our needs didn’t really match, but it was a well-thought comparison of Blurb, BookBaby, Lulu and Nook Press (now Barnes and Noble Press) anyway. For a while, I’ve evaluated local, Italy-based services as well, yet the price-tag was way too high; I also tried to understand how Kindle Direct Publishing works, but I couldn’t get whether the printed books must be sold via Amazon or could be directly shipped to the author, so I gave it up4. If you have experience of KDP, please let me know in the comments.\n\nEach and every print-on-demand (POD) service provider offers some sort of “Make your book” page where you choose the book’s options, such as Size, Binding, Paper and Inks, etc. and gives you an estimate of the cost per copy. Depending on the print batch size, small discounts may apply as well. I suggest you to go through that process for all the PODs that you want to test, to understand which one better suits your needs and budget.\n\nPrinting with Lulu\n\nAt the end of the day, I decided to go with Lulu; probably other PODs could have worked fine too – after all I don’t have ultra-specific needs –, but one can’t keep splitting hairs in two for too long.\n\nBook’s options\n\nIn Lulu’s Book Pricing Calculator page I could test how the various options affected the price per copy; besides a large variety of book sizes, the main choices were:\n\n\n  Binding\n    \n      Hardcover Casewrap (traditional hardcover, stitched)\n      “Perfect Bound” (milled/glued)\n      Coil bound (with plastic coil)\n    \n  \n  Inks\n    \n      Standard B/W\n      Standard Color\n      Premium B/W\n      Premium Color\n    \n  \n  Paper\n    \n      60# Cream Uncoated\n      60# White Uncoated\n      80# White Coated\n    \n  \n\n\nFew remarks.\nLulu’s Producs descriptions fall a bit short of tech specs: each option is related to the final result instead.\n\n\n\nFor instance, Standard Color is “Ideal for text-heavy content with a few color images, graphs, or illustrations that have low to moderate ink coverage” whereas Premium Color’s “Ideal for image-heavy content with heavy ink coverage on the pages”. I can understand this, but it tells nothing about the kind of inks used, print resolution, the Total Ink Coverage allowed, etc.\n\n\n\nWhile Binding options are illustrated with photographs of the final object, no such close-up pictures are given for Paper types or Inks. I suppose 60# and 80# refers to the paper weight, in grams per square meter.\n\n\n\nGiven that my original PDF was US Letter (8.5x11 inches) sized, 400-ish pages long, this is the cost per copy for a variety of different Ink, Paper, Binding combinations:\n\n\n  \n    \n      Ink\n      Paper\n      Binding\n      Price\n    \n  \n  \n    \n      Color Standard\n      60# Uncoated White\n      Perfect Bound\n      $17.40\n    \n    \n      Color Standard\n      80# Coated White\n      Perfect Bound\n      $19.40\n    \n    \n      B/W Standard\n      80# Coated White\n      Perfect Bound\n      $9.80\n    \n    \n      B/W Premium\n      80# Coated White\n      Perfect Bound\n      $15.40\n    \n    \n      B/W Premium\n      80# Coated White\n      Casewrap\n      $22.75\n    \n    \n      Color Premium\n      80# Coated White\n      Perfect Bound\n      $65.40\n    \n    \n      Color Premium\n      80# Coated White\n      Perfect Bound\n      $65.40\n    \n    \n      Color Premium\n      80# Coated White\n      Casewrap\n      $72.50\n    \n  \n\n\nMy book is quite long, so keep that in mind when evaluating prices. Printing with Lulu goes from a nice $9.80 for a Black &amp; White Standard Inks on the best paper with glued/milled binding, to a surprising $72.50 for the top quality options in all departments.\n\nYou can see that the paper choice doesn’t really influence the price that much, $2.00 spread over four hundred pages is half of a Cent difference per page, so 80# Coated White was a no brainer.\n\nInks make the greatest difference of all: +$5.60 to upgrade from B/W Standard to B/W Premium; +$4.00 from B/W Premium to Color Standard, and a whopping +$46 to get from Color Standard to Color Premium! I understand that the more expensive option is for Photo Books, which mine is not, out of question/budget. I was left deciding between B/W Premium and Color Standard: since I have several graphics in color, pictures and syntax highlighted code, I thought that it’d be cheap to print grayscale so I went with Color Standard Inks. Finger crossed for the “few color images, graphs, or illustrations that have low to moderate ink coverage”.\n\nI should have picked Casewrap (the hardcover, stitched binding option), but +$7.10 looked too much for the cheapskate I’ve become in my forties, so I got Perfect Bound with a glossy finish.\n\nMy final price was around $20 per copy; I ordered the titanic amount of 3 copies, which added $23 in Ground Courier Shipment, 4/5 days to wait. With my country’s VAT, all this print-on-demand test of mine was worth something more than $90, or about $30 per copy in total.\n\nPreparing the PDF\n\nThis section is going to be a bit technical: if you happen to read this post because you want to Lulu-print your own book and it’s not text only (e.g., a novel or poetry), try asking a graphic designer for help. Being subscribed to the Adobe Creative Cloud, I have access to software such as Acrobat DC, InDesign and Photoshop, that I’ve used extensively here.\n\nGrab all Lulu’s relevant documentation PDFs from this page, and give it a read.\n\nFirst, duplicate your Leanpub Print-Ready PDF. Open it in Acrobat and add the recommended bleed – in my case 0.125”; make sure you have a single page (not spreads) PDF with an even number of pages.\n\n\n\nSpeaking of Transparency Flattener, Lulu provides a Lulu-High-Res-Outlines.flst preset file: you can’t use it in Acrobat, only when exporting to PDF from InDesign. But from InDesign you can edit it to find out what the settings are.\n\n\n\nI am fine with everything but converting the text to outline, so I run the built-in Acrobat Transparency Flattener at high resolution:\n\n\n\nI’ve then run a preflight against PDF/X-1a, fixing the errors when possible. I’ve manually substituted images which were too low-res for the print version (but OK for screen/display visualization).\n\nWhen it comes to Color Management, things are a bit fuzzy: Lulu provides two sets of JobOptions, here and here, with of instruction contrasting with each other, or with information that Lulu provides in the Guides. For instance, I’ve read to convert everything to sRGB, and Leave Color Unchanged at the same time.\n\nI’ve asked in their forums, and they opened a support ticket for me. After a couple of very kind emails, I got this:\n\n\n  They have ongoing partnerships with several print facilities in different places in the world (US, Canada, Europe, and Australia).\n  They have agreed with each contractor on a set of quality standards (based on ink/paper), although their print partners are largely autonomous.\n  They tend to suggest sRGB to standardize the color input (and avoid problems).\n  It is not possible to micro-manage (prepress-style) graphic elements in the PDF (custom CMYK separations, GCR, etc) for each print shop may do CMYK repurposing on their own e.g., for color management or ink saving purposes.\n\n\nThis also explains why the product description is not very much detailed, but based on broad quality definitions. Reluctantly, I made sure that everything was converted to sRGB and did upload the PDF to Lulu’s servers – hoping that the pre-press gods wouldn’t notice my sin.\n\n\n\nIn case you’re wondering, I didn’t go on asking which print facility my order would have been sent to (somewhere in south UK, it turned out), with the idea of getting in touch with their prepress department.\n\nFor the cover, Lulu provides an InDesign template and a Spine width calculator (based on the binding type and pages number) – creating the design from scratch wasn’t difficult.\n\n\n\nThe whole online order creation process is guided, and almost impossible to mess up when you have all the assets ready. I paid with PayPal, and the book got printed I think the next day, and arrived in my hands in a few days later as expected.\n\nThe printed result\n\nIt’s hard to be judgmental the very moment you hold for the first time a printed copy of the work that took 1.5 years of your life to complete: it looked awesome, and at the same time I was so fed up with the entire project that I gave it a quick review, wrote an affectionate dedication to Jeffrey, prepared the package and sent it to Adobe’s offices in Minnesota, US.\n\n\n\nThat said, you can read below a frank bullet-points list of what I liked, what I didn’t really like, what was my fault and what I’d do differently next time to get a better result.\n\n\n\n\n\nThe good parts\n\n\n  The paper is nice: it looks very much like an 80g/mq, the coating is OK, it surely has some optical brighteners but it’s not obviously bluish/greenish.\n  The print quality is OK too. I was a bit worried about some pages with higher ink coverage, but in general there are no ghosting problems, stains and such. The color rendition and gray balance are fine too. Usually, software development books are printed in B/W, but given the limited batch I’ve ordered, it made little sense to save few bucks on B/W inks. Would I need a larger number of copies, I’d surely go for it. In case, it would be nice to have an 80 grams uncoated paper, but Lulu offers 60 only without coating.\n  The binding is robust, not the kind of book that looks like it will shred pages around the fifth time you open it.\n\n\n  \n\nPerfectBound looks sturdy indeed\n\n\nThe bad parts\n\nOr at least the ones I personally find not up my expectations.\n\n\n  The package wasn’t particularly good: the usual cardboard box with little protection against corners damage.\n  The soft cover is rather fragile, especially along edges, where the glossy coat may get slightly peeled off. I’d be curious to see whether a matte finish is less delicate.\n\n\n  \n\nGlossy finish peeling off\n\n\n\n  Not being able to micro-manage the color separations still bugs me a bit.\n  There is some banding here and there, that somehow I expected on a digital printed book. Probably the pricey “Color Premium inks” option deals with it.\n\n\n  \n\nDark and light Banding on the sky\n\n\nMy mistakes\n\n\n  The book size I chose is too big to my taste. This is something I couldn’t change5: it would have meant re-checking all the elements flow, and I had no time for that. When you start writing your eBook, even if you don’t plan to print it, think about the physical size of the page: PDF area is one thing, paper surface is another. I went with US Letter (8.5x11 inches, 21.6x27.9cm): other books in the software development category are smaller, like 18x23.5cm (O’Reilly) or 1cm wider (Packt), so around 7-7.5x9.2 inches. Mine looks unnecessarily large.\n  I didn’t check all the images resolution properly, so you can spot some pixelation here and there.\n  Never, ever take dark interface programs’ screenshots. Dark interfaces are cool to use, eye-relaxing to look at, moderately nice in PDFs, dreadful in print, for legibility is compromised. My code’s syntax highlighting was great on the printed page, for it uses a light theme: don’t even think about using a dark theme for it. Also, I’m not 100% sure to have converted to sRGB all the screenshots when taking them6.\n\n\nIt’s not my mistake but it’s not clear either (I’ve to check with Leanpub on that) whether or not the Print-Ready PDF export option downsamples images. It may make sense for display PDFs, not so for Print where it is desirable to keep the max resolution available.\n\nConclusions\n\nAll in all, I am really pleased with the result. It took me some time to figure out the workflow for I was a first timer and I got some things wrong, but I can do better next time (even if I do not plan to offer my book in a printed version anytime soon). The materials are OK, the price is fair for a micro print batch such as mine. I’m curious to hear your experiences with print-on-demand services, so please share it in the comments section.\n\nBut I didn’t tell you the entire story…\n\nJeff offered me the possibility to send him two copies of my book: one as a well deserved gift to him, the other… would be sent me back with signatures by the Photoshop teams from both the Minnesota and California offices! Accidentally, “somebody” 🙏🏻 let also some tattoos, stickers, a pin and a shirt slip into the package, so I’ve now some Photoshop branded swag in the house, for my daughter to steal me7.\n\n\n\nAdobe Photoshop team signatures from Minnesota and California offices\n\n\nIsn’t that awesome?! ❤️\n\n\n  \n    \n      Pandoc is an open-source conversion tool that is also able to export to PDF from MarkDown, but the styling part is time consuming and you got to have quite some LaTeX knowledge. Softcover is a free alternative to Leanpub for the MarkDown to PDF part, but it requires more customization efforts, a bit of LaTeX skills and the resulting book style isn’t as good as Leanpub’s. Both have the notable upside that the entire process is local, whereas Leanpub creates the PDF server-side. &#8617;\n    \n    \n      The whole book was within a git repository, with an optional hook so that each push triggered the book compilation. &#8617;\n    \n    \n      Leanpub conversion uses an intermediate LaTeX step, so it’s actually MarkDown to LaTeX to PDF. It is very likely that, internally, thay make use of Pandoc heavily tweaked with custom LaTeX styling. Which is what allows Text Blocks with Icons, Syntax Highlighting and all the features that make Leanpub ebooks so nice. &#8617;\n    \n    \n      It all may sound a bit sloppy now, but back then I really felt like I was in hurry: with more time, I could have evaluated options more carefully. &#8617;\n    \n    \n      Leanpub lets you pick a different page size when you export the Print-Ready PDF, but as I’ve written I had no time to check +400 pages again to look for, and fix, misplaced elements. &#8617;\n    \n    \n      On Mac, when you take a screenshot it is assigned the Display’s ICC profile. If you save the PNG/JPG with the embedded ICC, it gets properly converted to sRGB in the PDF post-production stage; if, by mistake, you discard the ICC profile while saving, sRGB is assigned (and therefore not converted) in the PDF, and colors will shift. &#8617;\n    \n    \n      She’s got a Magic Wand tattoo on her forearm that she wiggles around the house like Hermione Granger. &#8617;\n    \n  \n\n",
      tags: ["Leanpub","Lulu","Print-on-demand"],
      id: 5
    });
    

    index.add({
      title: "Migrating from WordPress to Jekyll on Netlify",
      category: ["Personal"],
      content: "After years, I’ve been able to kiss WordPress bye-bye and migrate to a fully static site build with Jekyll and deployed to Netlify. In this post I’ll tell you why, and show you how.\n\nWhy not WordPress?\n\nI’ve nothing against WP in principle, it’s not the right tool for me. I blogpost on average once a month, it makes no sense for me to be bound to a Linux hosting with mySQL access, a BackupBuddy subscription plan and a dozen of other WP plugins to run this site. Have you seen my posts? A couple of images, few snippets of code, some text. Really, WP is plain overkill; plus, I don’t want to worry about WP updates, PhpMyAdmin, DB access errors, log-ins, plugin incompatibility and fancy dashboards anymore.\n\nNow I have a simpler, static site which I update writing text files on my disk, committing to a free git repository, which Jekyll files are automatically built, hosted and served over https for free by Netlify.\n\nStatic vs. Dynamic\n\nAs opposed to WP, where each PHP template1 is filled on-demand – i.e. when a user requests a page, fetching the content from a database and returning the processed data as html – a Static site is pre-compiled so to speak, and simply made available online all at once.\n\nAs a consequence, a WP site needs a machine running some server-side scripting language such as PHP, a database like mySQL, and some processing resources; a static site is happy when it is hosted on a server that is (a) turned on, and (b) connected to the net.\n\nSSGs\n\nIf you’re not familiar with the concept of Static Site Generators, they’re command-line tools that get a bunch of HTML/JS/CSS with template code and markdown files as input, and output a full static website. Your job is then to move the files on the server.\n\nThere are several SSGs available: to the best of my knowledge, the most popular ones are Jekyll (written in Ruby), Hugo (written in Go) and Hexo (Javascript). Each one of them has its peculiar templating system and folders structure.\n\n\n\nIf you feel inclined, a way too big list of SSGs is found here.\n\n\nAll of them share the sublime idea that you compose your writing (both pages and blog-posts) in MarkDown: a text file with a basic set of formatting rules, such as **bold**, _italic_ etc. Hence, all your website content is not hidden into some resources-hungry DataBase, but exposed to you as plain text files with a .md extension.\n\nWhich SSG to pick\n\nIf you start fresh (i.e., you don’t run a SSG already) or you don’t have plenty of time on your hands, I suggest you to look at the available templates for Jekyll (free or paid), for Hugo and for Hexo. Beware high expectations: they’re all quite bare. Pick the one you like the most, and then learn that templating language to customize it.\n\nOn a superficial level, that’s all you need. To me – and I’m not really into SSGs enough to get all the nuances – besides the templating language, the only other difference is compilation speed. Being written in Go, Hugo goes like a rocket. My website is compiled by Hugo in 1.5 seconds, whereas Jekyll takes ~14 seconds, and Hexo is not that much better.\n\n\n\nKeep in mind that each time you modify a thing and save, the process re-generates (if you want, incrementally) the whole website, so 14 seconds to see whether a CSS rule or a template tweak really do what you originally meant them to do, may be a long time to wait – the fifth time in a row that you hit save.\n\nI am on Jekyll, using a mildly customized version of the Steve theme, which costed me like $15. I already run CC-Extensions on Jekyll, I’m mildly familiar with it, so I’ll keep being patient if it takes seconds to compile.\n\nWordPress migration\n\nI’ve followed this very checklist myself. Perhaps things can be made simpler, but this has worked fine for me – feel free to google stuff if you need more detailed information.\n\n\n  Migrate all your comments to Disqus: sign up and follow the instruction to install Disqus on WordPress (you’ll need to get this WP Plugin).\n  Do a full backup: I’ve always used BackupBuddy, which isn’t cheap but works like a charm – perhaps also the built-in WP Export is OK. You need to backup both the WP assets and the DB.\n  Install MAMP or a similar software and restore your WordPress installation on a local server (e.g. on your laptop). This will make the following step faster.\n  On your local WP, install both WP to Hugo and Jekyll Exporter migration tools, and perform both Exports. You’ll get two .zip files with a bunch of MarkDown in them. I’ve found that the Hugo version of the posts returns a better MarkDown conversion – but the files aren’t named as they should (i.e., prefixed with the date, like 2018-11-29-something.md).\n\n\nJekyll import\n\n\n  I have then manually reviewed the markdown files of the majority of my posts (~150) coming from the export, deleting items in the YAML FrontMatter2 that aren’t meaningful, and fixing the markdown.\n  Make sure you keep the original permalink (the post URL): this way each post will have the same URL of your old WP site. This way people who get to you from other sites’ links don’t get 404’ed, and you keep analytics intact.\n  Remove in the MarkDown all the links to http://localhost:8888/. For instance, my Hugo export (the one with nicer markup) has all the posts’ assets with urls like http://localhost:8888/wp-content/uploads/2018/03/logo.png. Do a batch search and replace and turn them to /wp-content/uploads/2018/03/logo.png.\n  Move the markdown posts in Jekyll’s _posts folder, and also move in the website root the exported /wp-content folder, which contains all the images coming from WP3\n\n\nAn example of the YAML FrontMatter for this very post:\n\n---\ntitle: Migrating from WordPress to Jekyll on Netlify\ndate: 2019-03-05T11\nauthor: Davide Barranca\nexcerpt: Bye-bye Wordpress.\nlayout: post\npermalink: /2019/03/migrating-from-wordpress-to-jekyll-on-netlify/\ndescription: How to migrate from WordPress to Jekyll on Netlify\nimage: /wp-content/uploads/2019/03/jekyll.png\ndraft: true\ncategory:\n  - Personal\ntags:\n  - Jekyll\n  - Netlify\n---\n\nGitHub\n\nThe whole idea around this SSG thing is that both the site build and the site updates must be easy. I’ve created a git repository on GitHub and pushed my Jekyll website there. Be aware that GitHub itself provides you for free with GitHub Pages, a service based on Jekyll that automatically builds your site each time you push a commit to a gh-pages branch, and hosts the result on https for free.\n\nThere are some limitations in terms of Jekyll plugins (see the whitelist here), so I’ve decided to try a different approach.\n\nNetlify\n\nGo create a free account on Netlify, which provides an amazing service similar to GitHub pages. Then link your GitHub/Bitbucket repository, define a build command (mine is simply jekyll build), and they will serve the Jekyll output on a dedicated, public subdomain – that you can use to check the site or point collaborators/clients to.\n\nAt this point, you can link your existing domain (the process is quite easy): Netlify will give you few domain name servers to set e.g. on GoDaddy, or wherever your domain is hosted. You’ll be asked also to add to Netlify all the existing CNAME and MX records from your host (copy them from GoDaddy – they are for email, FTP and such).\n\nThen you’ll have to wait some hours for the DNS propagation, during which your website won’be served through https – the free Let’s Encrypt certificate will be issued shortly, and your static site will finally be on SSL.\n\n\n\nConclusions\n\nI am genuinely happy to have streamlined my blogging workflow. There’s lot of room for improvement – e.g. on the theme, SEO, social cards etc. – but given the little time it took:\n\n\n  I have gotten rid of WordPress and related expenses (pricey hosting and plugins).\n  Posts are easier to access, create and edit.\n  I generally feel more in control, and less subject to random, time consuming issues.\n  I haven’t lost anything relevant in terms of functionality.\n\n\nIf I had more time I’d explore Jekyll and Netlify features more in depth, or even consider adapting my theme to Hugo to save some building time in the future. Luckily, I haven’t got any spare time left 😅 So I’ll just call quit and feel good.\nThanks for reading!\n\n  \n    \n      For a lack of better word. &#8617;\n    \n    \n      It’s the header content of each .md file, which is wrapped with three dashes ---. It contains the post’s metadata (e.g. the title, the excerpt), that is used by Jekyll to display it. &#8617;\n    \n    \n      There is a lot of redundant stuff in there, for WP creates several versions of all images you’ve imported at different resolutions. &#8617;\n    \n  \n\n",
      tags: ["Jekyll","Netlify"],
      id: 6
    });
    

    index.add({
      title: "Adobe Extension Manager Error Codes",
      category: ["CEP"],
      content: "Here is a list of Adobe Extension Manager ExManCmd error codes.\n\nI don’t use zxp files to install my products, but some people do (e.g. with Anastasiy’s Extension Manager, that under the hood calls ExManCmd) and error numbers are always hard to decode. The list has been kindly provided by Adobe’s Erin Finnegan.\n\nExManCmd Errors\n\nEXMAN_FAILED_INSUFFICIENT_PRIVILEDGE = -151\nEXMAN_FAILED_OPEN_FILES = -152\nEXMAN_FAILED_READ_FILES = -154\nEXMAN_FAILED_WRITE_FILES = -155\nEXMAN_FAILED_COPY_FILES = -156\nEXMAN_FAILED_DELETE_FILES = -157\nEXMAN_FAILED_CREATE_FILES = -158\nEXMAN_FAILED_FILE_TYPE_NOT_MATCH = -159\nEXMAN_FAILED_FILE_NOT_FOUND = -160\nEXMAN_FAILED_ITERATE_DIRECTORY = -161\nEXMAN_FAILED_FILE_EXISTS_ALREADY = -162\nEXMAN_FAILED_CHANGE_PERMISSION = -163\nEXMAN_FAILED_CREATE_DIRECTORY = -164\nEXMAN_FAILED_DIRECTORY_EXISTS_ALREADY = -165\nEXMAN_FAILED_IS_NOT_DIRECTORY = -166\nEXMAN_FAILED_IS_NOT_FILE = -167\nEXMAN_FAILED_PATH_INVALID = -168\nEXMAN_FAILED_PATH_NOT_FOUND = -169\nEXMAN_FAILED_ACCESS_DENIED = -170\nEXMAN_FAILED_FILE_LOCKED = -171\nEXMAN_FAILED_SHARING_VIOLATION = -172\nEXMAN_FAILED_JUDGE_FILE_EXIST = -173\nEXMAN_FAILED_FILESCHEMA_ILLEGAL = -174\nEXMAN_FAILED_ELEVATE_START = -175\nEXMAN_FAILED_ELEVATE_FILEDEL = -176\nEXMAN_FAILED_ELEVATE_FILECOPY = -178\nEXMAN_FAILED_ELEVATE_MAKEDIR = -179\nEXMAN_FAILED_UNHANDLED_EXCEPTION = -180\nEXMAN_FAILED_NO_INSTALLED_FILES = -181\nEXMAN_FAILED_ELEVATE_EXIST = -182\nEXMAN_FAILED_IS_NOT_LINK = -183\nEXMAN_FAILED_ELEVATE_ADDPERMISSION = -184\nEXMAN_FAILED_EXTRACT_ZXP = -201\nEXMAN_FAILED_DECRYPT = -202\nEXMAN_FAILED_ENCRYPT = -203\nEXMAN_FAILED_PARSE_XMANCONFIG = -251\nEXMAN_FAILED_PARSE_MXI = -252\nEXMAN_FAILED_DESTINATION_MISSING_TOKEN = -253\nEXMAN_FAILED_PARENT_FOLDER_IN_PATH = -254\nEXMAN_FAILED_SOURCE_FORMAT_ERROR = -255\nEXMAN_FAILED_UNKNOWN_TOKEN = -256\nEXMAN_FAILED_MANIFEST_NODE_NOT_FOUND = -257\nEXMAN_FAILED_XML_NOT_FOUND = -259\nEXMAN_FAILED_ELEMENT_NOT_FOUND = -260\nEXMAN_FAILED_INVALID_VALUE_IN_MXI = -261\nEXMAN_FAILED_OPEN_XMANCONFIG_URI = -262\nEXMAN_FAILED_DOWNLOAD_XMANCONFIG = -263\nEXMAN_FAILED_PARSE_TODOFILE = -264\nEXMAN_FAILED_PARSE_XML = -265\nEXMAN_FAILED_GENERATE_XMANCONFIG = -266\nEXMAN_FAILED_INVALID_MANIFEST = -267\nEXMAN_FAILED_INVALID_MANIFEST_VERSION = -268\nEXMAN_FAILED_INVALID_MANIFEST_BUNDLE_ID = -269\nEXMAN_FAILED_INVALID_MANIFEST_BUNDLE_VERSION = -270\nEXMAN_FAILED_MANIFEST_EXCEPTION = -271\nEXMAN_FAILED_RESOURCE_EXCEPTION = -272\nEXMAN_FAILED_BLOCK_LIST_EXCEPTION = -273\nEXMAN_FAILED_INVALID_BLOCK_LIST = -274\nEXMAN_FAILED_CLOSE_HEADLIGHTS_SESSION = -302\nEXMAN_FAILED_INIT_HEADLIGHTS = -303\nEXMAN_FAILED_NULL_POINTER = -351\nEXMAN_FAILED_BUFFER_NOT_ENOUGH = -352\nEXMAN_FAILED_REJECT_LICENSE_AGREEMENT = -401\nEXMAN_FAILED_SIGNATURE_VALIDATION = -402\nEXMAN_FAILED_NO_PRODUCT_SUPPORT_THIS_CEP = -403\nEXMAN_FAILED_CEP_NOT_SIGNED = -404\nEXMAN_FAILED_REJECT_SIGNATURE_VALIDATION = -405\nEXMAN_FAILED_EXTENSION_NOT_FOUND = -406\nEXMAN_FAILED_REQUIRED_EXTENSION_NOT_INSTALLED = -407\nEXMAN_FAILED_REQUIRED_EXTENSION_NOT_ENABLED = -408\nEXMAN_FAILED_EXTENSION_BLOCKED = -409\nEXMAN_FAILED_INVALID_ZXP_FILE = -410\nEXMAN_FAILED_NO_SUPPORTED_PRODUCT = -411\nEXMAN_FAILED_CONFLICT_EXTENSION_FOUND = -412\nEXMAN_FAILED_INVALID_EMBEDDED_EXTENSION = -413\nEXMAN_FAILED_EMBEDDED_EXTENSION_NOT_SIGNED = -414\nEXMAN_FAILED_RESOURCE_FOLDER_NOT_FOUND = -415\nEXMAN_FAILED_CREATE_EXTENSION = -416\nEXMAN_FAILED_WRONGDEPENDENCY = -417\nEXMAN_FAILED_NEWER_EXTENSION_INSTALLED = -418\nEXMAN_FAILED_DEPENDENCY_CHANGED_WHEN_UPDATE = -419\nEXMAN_FAILED_MXP_NO_SUPPORTED = -420\nEXMAN_FAILED_DOWNLOAD_EXTENSION = -421\nEXMAN_FAILED_REJECT_OVERWRITE_CONFLICT_EXTENSION = -422\nEXMAN_FAILED_USER_CANCEL_DOWNLOAD = -423\nEXMAN_FAILED_CAN_NOT_INSTALL_FOR_ALL = -424\nEXMAN_FAILED_PRODUCT_NOT_FOUND = -451\nEXMAN_FAILED_TOKEN_NOT_FOUND = -453\nEXMAN_FAILED_PRODUCT_NOT_SUPPORTED = -454\nEXMAN_FAILED_PARSER_PRODUCT_TOKEN = -455\nEXMAN_FAILED_POINT_PRODUCT_NEED_QUIT = -456\nEXMAN_FAILED_SPECIFIER_NOT_FOUND = -457\nEXMAN_FAILED_REQUIRED_PRODUCT_NOT_INSTALLED = -458\nEXMAN_FAILED_REQUIRED_VERSION_NOT_INSTALLED = -459\nEXMAN_FAILED_DBWRAPPER_COMMON = -500\nEXMAN_FAILED_DBWRAPPER_CONNECTFAILED = -501\nEXMAN_FAILED_DBWRAPPER_DISCONNECTFAILED = -502\nEXMAN_FAILED_DBWRAPPER_QUERYFAILED = -503\nEXMAN_FAILED_DBWRAPPER_NOTSTARTUP = -504\nEXMAN_FAILED_DBWRAPPER_NOTSHUTDOWN = -505\nEXMAN_FAILED_DBWRAPPER_DATANOTFOUND = -506\nEXMAN_FAILED_DBWRAPPER_CREATTABLFAILED = -507\nEXMAN_FAILED_DBWRAPPER_NAME_EXIST = -508\nEXMAN_FAILED_UPDATE_DATABASE = -509\nEXMAN_FAILED_INIT_BT_FAILED = -552\nEXMAN_FAILED_VALIDATE_LICENSE = -601\nEXMAN_FAILED_GET_USER_ACOUNTDAT = -602\nEXMAN_FAILED_INVALID_USERACOUNT = -603\nEXMAN_FAILED_GET_LICENSE_CHECK_RESPONSE = -604\nEXMAN_FAILED_COMMON_EXCEPTION = -651\nEXMAN_FAILED_SSL_CONTEXT_EXCEPTION = -652\nEXMAN_FAILED_SSL_EXCEPTION = -653\nEXMAN_FAILED_COMMON_SET_MGMT = -851\nEXMAN_FAILED_STRING_CONVERT = -901\nEXMAN_FAILED_USER_NOT_FOUND = -902\nEXMAN_FAILED_FETCH_USER_INFO = -903\nEXMAN_FAILED_GET_USER_HOME_FOLDER = -904\n\n",
      tags: ["Adobe Extension Manager"],
      id: 7
    });
    

    index.add({
      title: "Professional Photoshop Scripting is published!",
      category: ["Scripting"],
      content: "After 10 months since the launch of the Early Access Program, I am incredibly happy to announce that my book is now complete! Read the full report below.\n\n\n\nThe Final Version 1.0!\n\nThe book is now 12 Chapters/409 pages, fully CC 2019 compliant: compared to the last EAP version 0.2, I’ve added one Chapter, an Appendix, a proper Acknowledgement section, some extra-content on ActionManager, and given it a final round of proofreading and fixes.\n\n\n\nI’ve also finally credited my dear friend and colleague Sandra Voelker for her work as the Technical Editor – she’s been providing suggestions and corrections of remarkably high value.\n\nI am also especially honored to tell you that the book contains an awesome Foreword by Jeffrey Tranberry, Sr. Product Manager, Digital Imaging at Adobe Systems, and author with Geoff Scott of the book “Power, Speed &amp; Automation with Adobe Photoshop” (2012).\n\n\n\nAll in all, I am quite happy about how it has turned out: it took me just two years and a half! Dare I to say, you won’t find anywhere else such a comprehensive learning path that goes from the basics of the ExtendScript language, up to advanced topics such as ActionManager (~70 pages) and Adobe Generator (~60 pages).\n\nThe book V1.0. is for sale right now at PS-Scripting.com, where you can also Get Sample Content!\n\nI really hope you’ll enjoy it! As I wrote in an earlier post, my intention is to match the Adobe Photoshop HTML Panels Development course and add a series of video tutorials: I tested the equipment but stopped for the final rush needed to publish the book.\n\n\n\nMany thanks to all who have contributed, helped, and supported the project buying the EAP 🙏🏻 Your support has been truly appreciated.\nThank you all!\n\n👉 Book Website\n",
      tags: ["Professional Photoshop Scripting"],
      id: 8
    });
    

    index.add({
      title: "Spectrum CSS VueJS Component: DropDown",
      category: ["CEP"],
      content: "In a previous post I’ve introduced the recently open-sourced Spectrum CSS. Here, I’ll be demonstrating how to use them to build a simple Vue.js component: a DropDown menu.\n\nSpectrum CSS are really appealing, but as I’ve mentioned earlier, they’re not bundled with any JavaScript – besides what you may borrow from the online documentation page. While elements such as Sliders are plug &amp; play, for they actually do use a &lt;input type=\"range\"&gt; wrapped in a lot of finely styled &lt;div&gt; tags, others such as the DropDown are not.\n\n\n\t\n\tScreenshot from the Spectrum CSS documentation\n\n\nI.e. if you click on a Closed dropdown, it won’t open (the above is just a screenshot so it’s pointless to try, but the result would be the same). The reason being that a Spectrum DropDown is not really a &lt;select&gt; element, as you can see in the snippet below:\n\n\n\nAs far as I get, re-creating an element from scratch is a standard practice to avoid rendering differences between browsers (the &lt;select&gt; is a good example here).\n\nAnyways: no &lt;select&gt; means no default behavior on click, change, etc. In other words, you are the one in charge of populating the &lt;div&gt; elements above and adding relevant event handlers so that the DropDown stops being a nice but still and useless object.\n\nThis is a perfect test case for encapsulating everything in a Vue.js Component! The only drawback is that I don’t feel particularly comfortable with the idea of creating Components from scratch. All my CEP Panels use Vue.js now, that’s true, but frankly in a very simple and not over-engineered fashion: I’ve always used one shared data, the farther I’ve ventured into fanciness was setting up an Event Hub.\n\nThanks god I’ve a PhD in Copy&amp;Paste, so I’ve been able to adapt this one:\n\n\n\n… into a proper Vue Component that wraps the Spectrum DropDown original markup. The .vue file I came up with is as follows:\n\n400: Invalid request\n\n\nI won’t go too much into the details here. If you look at the &lt;template&gt; tag and compare it with the original Spectrum html, you can spot the differences: I’ve added a couple of classes that depend on the selected item or the showMenuboolean, and used a v-for loop to populate the options.\n\nThe &lt;script&gt; tag is where the actual logic belongs: I’ve added two methods, one to show the opened dropdown and one to emit an event to the parent when something is selected. From the props you’re able to tell how to consume such element, e.g.:\n\n&lt;drop-down :options=\"arrayOfObjects\" v-on:updateOption=\"methodToRunOnSelect\" placeholder=\"Select a thing...\"/&gt;\n\nThe result is this one:\n\n\n\t\n\tThe working DropDown Vue.js Component! (Wife was unimpressed)\n\n\nIt is quite bare, I may want to add transitions and such… but it works!\n\nTo tell you the truth, I thought I couldn’t be able to make it: I had a look at proper Vue.js UI Kits (like this one) and they exceed, by far, my understanding. Luckily I’ve been able to borrow code and adapt it, so I may be doing it again in the future for other elements that I would need.\n\nIf you’re up for the same thing, or you know how to make my code better, please let me know in the comments. Bye!\n",
      tags: ["CEP","Vue.js"],
      id: 9
    });
    

    index.add({
      title: "ESTK to be replaced by a Visual Studio Code plug-in",
      category: ["Scripting"],
      content: "Please do yourself a favour and read this Medium post by Lead Technical Evangelist Ash Ryan Arnwine about the future of the ExtendScript ToolKit.\n\nI’ll keep it short because I urge you to read the original news. No ETA yet, but ESTK is gonna die soon won’t be re-written to comply with Apple’s 32bit applications ban (macOS-next). Nobody in their right mind would have thought it anyway, but we’ve got an official confirmation.\n\n[confetti fall from the sky]\n\nGo download Visual Studio Code and familiarize with it, while waiting for the new Adobe Scripting plugin (Mac/Win compatible) to be released.\n\n\n",
      tags: ["VSCode"],
      id: 10
    });
    

    index.add({
      title: "Adobe Spectrum CSS open-sourced!",
      category: ["CEP","Scripting"],
      content: "In case you’ve missed the news, Adobe has open-sourced the Spectrum CSS – the stylesheets they’re using for Photoshop’s own CEP Panels 🍾 The GitHub repository is found here.\n\n\n\nThe CSS is demoed in this page, where you can pick four themes (Lighter, Light, Dark, Darkest) in four scales (Medium, Large, Medium diff, Large diff). The component list, compared to Topcoat which I’ve been using extensively in my projects, is quite larger – and frankly perhaps a tad overkill for CEP Panels, e.g. I doubt I would ever use a Calendar. But I’ve never complained of abundance: nice and very welcomed features are all kind of sliders, including ranges, and split-buttons, steppers, etc.\n\n\n\nYou can read in the documentation page that:\n\n\n  We have found that JavaScript is where a framework or library quickly becomes opinionated and stops being easy to use with or across other frameworks. To avoid this problem, Spectrum CSS avoids implementation details that require JavaScript. Where an element might require multiple states, the documentation here will simply show all the states in a flat, static example. We leave it to the frameworks implementing Spectrum CSS to create JavaScript that suits their needs.\n\n\nWhich means that we’re left on our own, but that’s not a big issue I suppose – hopefully there will be a Vue.js Spectrum components implementation in the near future :-)\n\nThe state of the Blog\n\nThat was it for Spectrum CSS, I take the chance here now to inform you about blog updates, etc. As a matter of fact, I’ve slowed down things quite a bit as you’ve noticed: I took fewer Panel jobs and tried to focus on books and courses. I’ve had greatly unwelcome issues with my house’s restoration works (started a couple of years ago – both the works and the issues…) that proved to be a time/concentration/money sink. As a result, progress on development-related tasks have been erratic, to say the least. Recently, I try to record a couple of test videos to be bundled with the Professional Photoshop Scripting course, which will look more or less like this:\n\n\n\nThere is no ETA yet – I will be able to tell you more as soon as I get the estimate for the roofing works, which will be followed by the insulation works, then the stairs, etc. Hopefully, before my hair gets completely gray 😉 Thanks for reading and for your support!\n",
      tags: ["CSS"],
      id: 11
    });
    

    index.add({
      title: "Professional Photoshop Scripting EAP Update",
      category: ["Scripting"],
      content: "I’ve added 60 pages to accommodate an amazing new chapter on Adobe Generator!\n\nThe book is now 388 pages strong and packed with exclusive content found nowhere else™.\n\n\n\nAdobe Generator is an amazing and very little known technology available in Photoshop since version CC. It features a Node.js instance running in the background that you can use in a variety of interesting ways.\n\n\n\t\n\n\n\n\nI’ve built several Generator Plug-ins expressly for this book: Pixmap extraction, Artificial Intelligence via an external service, Photoshop remote control from the Browser, bi-directional Socket communication, just to name a few. I’m sure you’ll enjoy them! Did you know that Adobe Generator has got its own icon? 🙂 This marks Professional Photoshop Scripting EAP version 0.2 – I would say that the main content is already in place. I plan to gather minor topics in one or two chapters, then proceed with the publication. You can still buy it at a highly discounted price now!\n\nThe Final Product:\n\n\n  Final Book + Code, $149\n  Final Book + Videos + Code, $249\n  Final Book + Videos + Code (Team/Enterprise License), $499\n\n\nEarly Access Program\n\n\n  Early Access Book + Code, $99\n  Early Access Book + Code (Team/Enterprise License), $199\n  Discount Coupon for the Videos when they will be recorded\n\n\nVisit ps-scripting.com for details. Cheers!\n",
      tags: ["EAP","Professional Photoshop Scripting"],
      id: 12
    });
    

    index.add({
      title: "Automated check for corrupted image files with Python and ImageMagick",
      category: ["Coding"],
      content: "How do you check if an image file (tiff, psd, psb) is corrupted, other than looking at its thumbnail with Bridge, or opening it on Photoshop? With a small Python script and ImageMagick!\n\nBackground\n\nThe client of mine I work as a retoucher for had some problems with the so-called Data Migration (the dull, time consuming, and error prone process of transferring a lifetime backup from old, once very expensive external drives to a set of new, somehow still equally expensive external drives). As a result, he got some corrupted files here and there in the destination drives – that’s the reason why you migrate data: the source has insufficient capacity, it has become unstable, obsolete, or both combined. Problem is that we’re talking about several TB of data, mostly as .psb files (ranging from about 1 up to 20GB each), and it goes without saying that opening them all in Photoshop is not an option; nor you can trust Adobe Bridge thumbnails – provided that you’ve set the preferences to render previews for big files too – because it’s a manual process anyway. Even if I’m paid by the hour, staring at thumbs is not my preferred way to get blind. After some research, I’ve found no way (other than the one I’m about to describe) to check for psd/psb files corruption in an automated fashion. Which seems to me quite odd – if you have better, i.e. faster and/or simpler, solutions, please do suggest them to me in the comments below.\n\nWhat you need\n\nPython 3, and ImageMagick. Both will work either on Mac or Windows: I’ve no experience of them on the latter platform, so I will just assume that you will be successful in following the installation instruction provided in the official home pages. PC owners: read at least the part relative to ImageMagick. Mac users: read it all.\n\nPython 3\n\nIf you’re on a Mac like me, you already have Python installed. Chances are that it is version 2.7, or another one but 3. Open the Terminal and type:\n\n$ python --version\nPython 2.7.10\n\nThis is what I still get after having installed Python 3.6 myself, via Homebrew. I’m no Python expert, so it took me some Google time to understand that on a Mac you can still have the python command pointing to the System’s old 2.7 version, even if you’ve freshly installed the new one. Solutions involve manually changing symlinks (power users advise against it), or using one of the available packages to create isolate Python environments (e.g. virtualenv, pyenv, etc. A list of them is found here). I couldn’t make neither of them to work in a reasonable amount of time, so I’ve resorted to simply use the python3 command, e.g.\n\n$ python3 --version\nPython 3.6.2\n\nImageMagick\n\nImageMagick is a multiplatform, open source commandline utility that performs a huge amount of tasks on all kinds of image files. I’ve installed via Homebrew, but it turns out that, at least on the Mac, it doesn’t come by default with the proper Delegates (aka Libraries) to deal with .psb files, which is what I needed the most. Finding the proper way to do so proved almost impossible to me: while reading the source code documentation (the last thing I wanted to do was to compile it from the source), I’ve discovered that via Homebrew you can list all the possible installation options for a package:\n\n$ brew options imagemagick\n\n--with-fftw\n  Compile with FFTW support\n--with-fontconfig\n  Build with fontconfig support\n--with-ghostscript\n  Build with ghostscript support\n--with-hdri\n  Compile with HDRI support\n--with-liblqr\n  Build with liblqr support\n--with-librsvg\n  Build with librsvg support\n--with-libwmf\n  Build with libwmf support\n--with-little-cms\n  Build with little-cms support\n--with-little-cms2\n  Build with little-cms2 support\n--with-opencl\n  Compile with OpenCL support\n--with-openexr\n  Build with openexr support\n--with-openjpeg\n  Build with openjpeg support\n--with-openmp\n  Compile with OpenMP support\n--with-pango\n  Build with pango support\n--with-perl\n  Compile with PerlMagick\n--with-webp\n  Build with webp support\n--with-x11\n  Build with x11 support\n--with-zero-configuration\n  Disables depending on XML configuration files\n--without-freetype\n  Build without freetype support\n--without-jpeg\n  Build without jpeg support\n--without-libpng\n  Build without libpng support\n--without-libtiff\n  Build without libtiff support\n--without-magick-plus-plus\n  disable build/install of Magick++\n--without-modules\n  Disable support for dynamically loadable modules\n--without-threads\n  Disable threads support\n--HEAD\n  Install HEAD version\n\nSo, after a first installation (without psb support), with no clear hint about the proper option(s) to use in my case, and even less spare time to test, I’ve chained them all – at least the seemingly appropriate ones, with little worries about being redundant. At all events, no one was watching me, nor would have ever known :-) The embarrassing line I’ve used is:\n\n$ brew reinstall imagemagick  --with-fftw --with-fontconfig --with-ghostscript --with-hdri --with-libde265 --with-liblqr --with-librsvg --with-libwmf --with-little-cms --with-little-cms2 --with-opencl --with-openexr --with-openjpeg --with-openmp --with-pango --with-perl --with-webp --with-x11\n\nIt worked, so I was a happy camper.\n\nThe Python Script\n\nWhich is far from perfect, but it does the job – I’m sure that a proper Python developer can make it much better: it comes from surgical copy&amp;paste from various Google Search result, plus a very light editing on my side.\n\nimport os\nfrom subprocess import Popen, PIPE\n\nfolderToCheck = '/Volumes/16TB/whatever/path'\nfileExtension = '.psb'\n\ndef checkImage(fn):\n  proc = Popen(\\['identify', '-verbose', fn\\], stdout=PIPE, stderr=PIPE)\n  out, err = proc.communicate()\n  exitcode = proc.returncode\n  return exitcode, out, err\n\nfor directory, subdirectories, files, in os.walk(folderToCheck):\n  for file in files:\n    if file.endswith(fileExtension):\n      filePath = os.path.join(directory, file)\n      code, output, error = checkImage(filePath)\n      if str(code) !=\"0\" or str(error, \"utf-8\") != \"\":\n        print(\"ERROR \" + filePath)\n      else:\n        print(\"OK \" + filePath)\n\nprint(\"-------------- DONE --------------\");\n\nHow it works\n\nThe basic is the identify call (that comes from ImageMagick), which is set to -verbose. This is what performs the check: the rest is just looping through the filesystem, looking for the appropriate file extension, and logging a message.\n\nHow to use it\n\nSave this on a file with a .py extension, and then run it with the python3 command on a terminal, e.g.\n\n$ python3 check.py\n\nBefore doing so, do change the content of the folderToCheck variable with an actual folder on your disk (with absolute path), and the fileExtension too: I’ve used .psb, but you can change it to .psd, .tif, .jpg, etc. As a result you’ll get a log in the Terminal; I’ve used a nifty, cheap application called Code Runner for such tests, and this is the result:\n\n\n\nAs you see, I’m just logging OK/ERROR with the path, very basic. What to do with this newly acquired piece of knowledge is up to you. Please note that:\n\n\n  The script processes nested folders too.\n  It is awfully slow and hungry: it eats CPU cycles and RAM. But it’s automatic, so heck!\n  The file extension is case sensitive, so \".JPG\" is different from \".jpg\".\n\n\nHow to make it better\n\nFew suggestions for the skilled Python developer (which I’m not, alas)\n\n\n  Write on a log file instead of the console\n  Keep track of the processing status and resume from there\n  Display the advancement status (say, “image 34 of 320”)\n  Make the file extension case insensitive.\n\n\nIf you know how to do any of this, please share your knowledge in the comments! Thank you!\n",
      tags: ["Python","imagemagick"],
      id: 13
    });
    

    index.add({
      title: "Professional Photoshop Scripting Course &#8211; Early Access Program",
      category: ["Scripting"],
      content: "I’m glad to announce that I have opened the Early Access Program to my new and very much awaited Professional Photoshop Scripting course! Read along. The course that I have announced in the past is now near completion: I would say 3/4 of the topics that I plan to cover have been written and edited – I’m at page 339 and counting! It took me way more than expected, though.\n\n\n\nEarly Access Program\n\nIn a fashion that is nowadays quite common among technical writers, I have opened what is referred to as an “Early Access Program”. If you’re not familiar with it, the idea is that you are able to read the book as I finish writing it. The advantages:\n\n\n  You don’t have to wait for the final book to get started.\n  You’ll be sent a notification and download link as soon as further updates are ready (new Chapters, fixes, etc).\n  You can get the course at a ludicrously discounted price now, compared to book’s street price when it’ll be published.\n  You can contribute to the writing process with suggestions and feedback.\n\n\nSounds good?\n\nFull Course and Pricing\n\nThis one will follow exactly the same price/bundle scheme of my previous Adobe Photoshop HTML Panels Development course.\n\nThe Final Product:\n\n\n  Final Book + Code, $149\n  Final Book + Videos + Code, $249\n  Final Book + Videos + Code (Team/Enterprise License), $499\n\n\nEarly Access Program\n\n\n  Early Access Book + Code, $99\n  Early Access Book + Code (Team/Enterprise License), $199\n  Discount Coupon for the Videos when they will be recorded\n\n\nTo be clear, the Early Access Program entitles you to all the work-in-progress updates, and the Final Release version too. Since I will record the videos when the book is completed, you’ll have access the video content as an optional upgrade.\n\nLaunching ps-scripting.com\n\nThe course is for sale since April 23rd 2018, please head to the dedicated website www.ps-scripting.com! And help me to spread the world. 🙏🏻\n",
      tags: ["Professional Photoshop Scripting","EAP"],
      id: 14
    });
    

    index.add({
      title: "Luminosity Masks: How Does It Really Work?",
      category: ["Photoshop"],
      content: "In this guest post, the Photoshop Plug In developer Scott Murdock tackles the apparently familiar topic of Luminosity Masks – with very interesting findings for anybody using them. Enjoy!\n\nEverywhere you go you read about Luminosity Masks – I’m really surprised it doesn’t have its own Wikipedia Page! (Maybe I’ll take care of that).  It is truly a great tool to have in your toolbox, very powerful when used correctly. It’s greatness by simplicity. But how does it works? Not in the sense “how to create it”, but what really happens behind the scenes? Well, let’s try answering that question, and doing it in a deep (and hopefully interesting) way.\n\nThe Basics\n\nStep 1 – It all starts with a Grayscale image\n\nYou can have images with all the colors in the world, but the first step in creating a Luminosity Mask is getting a Grayscale image (a Single Channel Image). In most cases, it means to calculate the Luminosity Channel of the given image. Each one has his own recipe: most actions/panels based solutions use Photoshop Luminosity selection (Ctrl + Left Mouse Click on the RGB Channel in the Channels Palette).\n\n\n\nBy the way, any “Single Channel” image can do here. The best choice (i.e. Channel, or processed grayscale layer) is the one where the image features you want to select are more evident and easily separable from the rest – in the above example the trees are well distinct from the sky for instance, and the lake’s gradient below helps as well. The Channel of choice could be the Blue from RGB, the Magenta from CMYK, or a combination of all the 10 Channels an image provides (R, G, B, C, M ,Y, K, L, a, b): whatever, be creative. In case of a grayscale image, life is easier, just move over to Step 2.\n\nStep 2 – Apply Pixel Wise Transformation on the Grayscale Image\n\nNow, this is where the magic happens. The idea is very simple: given the Gray Scale image as input, the output per pixel is a function of its value only. Well, this sentence might take some of us back to horrible school days but it is really simple when you think about it. Pixel comes in, states its value, and gets an output value based only on its value alone, and voilà! We have a Luminosity Mask. The name says it all, the Mask depends solely on the Luminosity (Tonal Range value) of the pixels. It has nothing to do with their location, not their surrounding pixels. Just using the Luminosity value. Nothing more, nothing less – power by simplicity.\n\n\n\nThe above Figure represents a Mask Generator. The input pixels values are in the upper section. They get processed by this black box, which operates based on a function  and the pixel output that is generated is found in the bottom section. At the output, everything is black (Low Values) with the exception of pixels around 128, that are mapped to white (High Values). This suggests that a “Midtones Mask” has been generated. Simple fact: images are discrete in their values. For instance, in the case of an 8bit image, the discrete values are in the range {0, 1, …, 254, 255}, occupying 256 available slots, i.e. \n\nSo a Mask Generator (Luminosity Mask Generator) has to designate an output value for each value in the input discrete domain. If the output image is also, let’s say 8 Bit, then the output values are also within the range {0, 1, ..., 254, 255} which means one need to map 256 values into 256 values. In the Computer Science world this process is done using a Look Up Table (LUT). Over time some masks got their own naming according to the properties of the values assigned:\n\n\n  If it designates high output values to low input values, and low values to the rest it is called Shadows Mask Generator. The output mask is called Shadows Luminosity Mask which reveals shadows and blocks everything else.\n  If it designates high output values to mid input values, and low values to the rest it is called Midtones Mask Generator. The output mask is called Midtones Luminosity Mask which reveals midtones and blocks everything else.\n  If it designates high output values to high input values, and low values to the rest it is called Highlights Mask Generator. The output mask is called Highlights Luminosity Mask which reveals highlights and blocks everything else.\n\n\nThis is the mask generation transformation (aka mapping), and basically this is all theory there is to know.\n\nExamples\n\nLet’s do some stretching by examining two simple examples of main building blocks in the Luminosity Mask world. We will assume an 8bit image, hence input and output pixel values are given by {0, 1, 2, ..., 254, 255}. The most basic Mapping / LUT / Function (all are different names to the same idea) is the identity mapping:\n\n\n\nNamely, the output value – that is $ f \\left( x \\right) $ – is identical to the input value, which is $ x $. This mask is called Highlights Luminosity Mask. Why? Because low input values (Shadows) are mapped to low output values (Dark pixels), and high input values (Highlights) are mapped to high output values (Light pixes). The result is a mask where shadows are dark (not selected) and highlights are light (selected) – hence the name. Another basic mask is given by the negative (inverse) of the Highlights Mask which is the Shadows Luminosity Mask:\n\n\n\nHere, to low input values (shadows) correspond high output values, and to high input values correspond high output values (light pixes). The result is a mask where shadows are light (selected) and highlights are dark (not selected). Using those 2 building blocks one could generate many other masks targeting different Tonal Ranges – something we will get to later.\n\n\n\nAs can be seen above, the Midtones Mask is generated by scaled multiplication of the Highlights Mask and the Shadows Mask. This is one way to achieve this, not necessarily what’s used usually (we’ll talk about that). Moreover, as can be seen by the Harmonic Function, one could do any mapping one wish.\n\nRemark: in practice, data is scaled into [0, 1] range as operations, such as multiplication, makes more sense in that domain. So the range {0, 1, 2, ..., 254, 255} becomes {0 / 255, 1 / 255, 2 /255, ..., 254 / 255, 255 / 255}. This is exactly what’s done in the above figure.\n\nIn Practice\n\nSo now we know what a Luminosity Mask Generator is, and what it is doing. On the next step, let’s try to understand how this is done in Photoshop in most cases. As discussed above, one need to create a LUT and there are two main approaches doing so: the Calculations Tool or the Curve Tool. One can apply each of those on Grayscale Image and the output is basically a Luminosity Mask.\n\nCurve Tool\n\nThe Curve Tool is a LUT table visualized by a Curve. It practically lets the user draw the LUT using a flexible curve.\n\n\n\nOn the figure above, one could see Photoshop’s Curve tool. On its bottom, horizontally, you can see the input values. On the left, vertically, you can see the output value. You match between each value just by altering the curve according to your wish. Basically, school days function, that’s what it is, drawing a function. Luminosity Mask Recipe by Curve Tool\n\n\n  Create a Grayscale version of the image in a new layer (Extract Luminosity / select one of the channels / desaturate the image / use Channel Mixer, etc…).\n  Open the Curve Tool.\n  Draw the desired LUT.\n  Use the result as a mask -&gt; Luminosity Mask.\n\n\nCalculations Tool\n\nUsing Calculations Tool one could apply simple Math operations on Layer / Channel / etc… Namely we can combine Math operations (Add, Subtract, Multiply and even more esoteric functions) by repetitive use of the Calculation tool. For instance, using the Calculation Tool we could easily generate the Midtones Mask from above by multiplying a layer and its inverse and scaling result by 4. So it gives us the option to use +, –, *, / on images, but not much more than that.\n\nComparison\n\nLet’s summarize the differences between those 2 approaches:\n\nCurves\n\n\n  Pros\n    \n      No limits what so ever on the shape of the selection.\n      Complex mask can be achieved in operation.\n    \n  \n  Cons\n    \n      Photoshop Curves are quantized into 256 levels which makes them less smooth.\n      No parameterization (Unless scripted) hence hard to be accurate and consistent.\n    \n  \n  Remarks\n    \n      Used by many Luminosity Mask panels out there, yet unless result can be achieved using Calculations, quality wise it is better use Calculations.\n    \n  \n\n\nCalculations\n\n\n  Pros\n    \n      Can be translated into exact Mathematical expression.\n      Smooth result and gets better as the mode (8, 16, 32 Bit) get higher.\n    \n  \n  Cons\n    \n      Limited to what can be done using the Blend Mode operations on the base Grayscale image.\n      Requires repetitive operations to get special selections (Slow).\n    \n  \n  Remarks\n    \n      Usually used for its quality yet limited either by speed or can get arbitrary selection.\n    \n  \n\n\nBuilding Masks\n\nIn this part we will show how most of the masks out there are built using the Lego bricks we created.\n\n\n\nAs one could see above, using Addition, Subtraction and Multiplication (Intersection), all operations available on Layers / Channels / Masks in Photoshop, one could easily generate all those “Classic” Luminosity Masks one could find in the wild (wild world of the Luminosity Masks Panels). Those with sharp eye would pay attention to something strange: Midtones Mask 001 is all black, see the function below:\n\n\n\nYet in practice, in all products out there… It is not?! So what’s going on?  Clearly they all state that the Midtones Mask is created by subtracting the Highlights and Shadows masks from the all white mask. So it is, by all means, should be all black mask while it is not. Well, what you see above is ideal Masks, while Photoshop can not generate them in this quality.  The current methods to create them usually use Photoshop’s steps which aren’t doing this exact Math. We’ll talk more on those pitfalls and strange behavior of classic Luminosity Masks (and their generation) in the next writing.\n\nSummary\n\n\n  Now we’ve understood what Luminosity Masks really are, the actual operations and Math behind them.\n  We have shown the Luminosity Mask generation is no more than the most simple operation on grayscale image - Apply LUT / Pixel Wise Mapping.\n  What we saw is that there 2 main approaches for Luminosity Masks in Photoshop. While one gives the most flexibility and efficiency (Curve Tool) it lacks with quality and the other which generates smooth masks (Calculation Tool) has speed issues when trying to generate complex masks and some miss calculations in the process.\n\n\nHow can we solve those? Well, do the algorithm outside the limitations of Photoshop. This is what Fixel Zone Selector is all about. In the next part we’ll have a deeper dive and talk about Zone Selector’s approach to Luminosity Masks.\n\nImage Credit\n\n\n  Lighthouse Image - Credit to magnetismus.\n  Schwaigsee Lake - Credit to Alfred Borchard.\n  Simple Living - Credit to Malcolm Carlaw.\n\n\nResources\n\n\n  Luminosity Mask: The Complete Kickstarter’s Guide (video).\n  How to Generate the Classic Luminosity Masks Using Mask / Channel Operations (Add, Subtract, Intersect Multiply) (video).\n  How to Generate the Classic Luminosity Masks Using Calculations (16 Bit Mode).\n  Selecting Using Luminosity Masks (Using Curve Tool) (video).\n\n\nAuthor of this guest post: Scott Murdock.\n",
      tags: ["Luminosity Mask"],
      id: 15
    });
    

    index.add({
      title: "A New Course on Adobe CEP Panels: Native Installers and Automation!",
      category: ["CEP"],
      content: "After Adobe Photoshop HTML Panels Development, I’ve now published a new Course, titled “The Ultimate Guide to Native Installers and Automated Build Systems”!\n\n\n\nFor which title the gods of software development and copywriting will forgive me. Curious?\n\nIn the business of Adobe HTML Panels, sooner or later you’ll ask yourself: “how the heck do I do… whatever it must be done to let my customers run my software on their Computers?” The typical stream of consciousness is as follows.\n\n\n  I know I know: it’s just a matter of moving a folder to the right place, isn’t it? And perhaps few extra assets here and there… of course, I don’t want other developers to reverse engineer my own code, so it must be protected somehow. OK, I forgot signing and timestamping. And Code Certificates: should I buy one? Which one? Do they all work the same? I’ve heard that some classy people build Native Installers, like Apple and Windows programs with nice graphic interfaces, that defeat Gatekeeper and SmartScreen security warnings like a pro! Gee, do I have to go through all this, by hand, all the time? Each time I change my code, on each product? For real?\n\n\nStream of consciousness ends on a sad note. Hey, look at that! (Spring from Vivaldi’s The Four Seasons plays in background)\n\n\n\nThis is what the cool guys are doing. Type a command, and look at things that build themselves. What is looping above is a sample project, including:\n\nInput:\n\n  HTML Panel\n  Multiplatform Photoshop Plugin as .plugin and .8bf files (optional).\n  Readme documentation as a .pdf file (optional).\n\n\nOutput\n\n  Signed and Timestamped Panel folder with:\n    *   .js files protected with advanced obfuscation algorithms.\n    *   .jsx files exported as .jsxbin with different advanced obfuscation algorithms.\n  Zipped version for convenience.\n  Hybrid Extension as a .zxp file, ready for submission on Adobe Add-ons.\n  Mac .dmg package, that in turn contains:\n    *   a .pkg native installer,\n    *   an uninstaller application,\n    *   the .pdf documentation.\n  Windows .exe native installer that deploys Panel, Photoshop Plugin and Documentation\n    *   Uninstaller provided in Windows’ Control Panel.\n  Both platforms’ native installers are signed with paid Code Certificates to comply with Windows and macOS security policies.\n\n\nAfter many years of trial and errors, I’ve finally settled to the above setup for my own commercial products, which takes 20 seconds of computer work to automatically build all the above. The final result looks like that, first on Mac:\n\n\n    \n\n\nThen on Windows:\n\n\n    \n\n\nThe Course\n\nHow to get there? I’ve explained it all, step by step, in The Ultimate Guide to Native Installers and Automated Build Systems, my new Course!\n\n\n\nIt comes as a PDF ebook, bundled with the entire Sample Project code – both the dummy Panel and assets, plus the automation. The course draws the big picture first, aka what needs to be done; then shows you how to manually perform each step, why, suggesting best practice and which software to use; finally, it teaches you how to write and customize the code to build the entire automation pipeline.\n\n\n\nLoading...\n\nFAQ\n\n\n    Does the Course apply to Photoshop only?\n    I've used PS as my Adobe app of choice, but no, the course applies to every CEP-enabled app you're developing extensions for.\n\n    Mac, Windows or both?\n    I'm on Mac: it's the only platform that allows you to build both Mac and Windows installers; yet the automation code can run on Windows as well, with the exception of the Mac installers part.\n\n    Does the bundle include Code Certificates?\n    No. On the one side, paid Code Certificates are strongly suggested but optional. On the other side, I could not buy them in your place! In fact, it took me quite some time and effort to acquire them just for myself.\n\n    Does the Course cover Panels development?\n    This one is about installers and automation only. If you want the Big One (300 pages, 3 hours of videotutorials, 28 sample Panels code), head to the Adobe Photoshop HTML Panels Development Course.\n\n    Why there's a 1 in the cover?\n    You have a sharp eye! I plan to expand the collection with more specific and smaller Courses like this one, in the future. Feel free to suggest topics you'd like to see covered in the comments! And yes, I have a 🦊 as the Terminal prompt, isn't it cute?\n\n\nThank you!\n\nOver the years, I’ve always posted articles on this blog as a way to give back to the developers’ community what I’ve picked up myself in the first place – from Forums, chats, etc. Alas, we live in a (very) material and a (very) demanding world: projects like this one are fundamental for me to sustain my business and my family – and as a consequence maintain and update this blog too.\n\nThank you in advance for your support! 🙏🏻 It is really appreciated.\n",
      tags: ["Automation","Installer","Native Installer"],
      id: 16
    });
    

    index.add({
      title: "HTML Panel Tips #25: CC 2018 Survival Guide",
      category: ["CEP"],
      content: "That time of the year has come, and Photoshop CC 2018 is here. Read along to find out Everything You Always Wanted to Know About CEP 8 (But Were Afraid to Ask). A brief history of Photoshop’s HTML Panels support for your pleasure:\n\n\n  \n    \n      CC Version\n      PS Internal Version\n       \n      CEP Version\n    \n  \n  \n    \n      Photoshop CC\n      14.x\n       \n      4.0\n    \n    \n      Photoshop CC 2014\n      15.x\n       \n      5.0\n    \n    \n      Photoshop CC 2015\n      16.x\n       \n      6.0\n    \n    \n      Photoshop CC 2015.5\n      17.x\n       \n      7.0\n    \n    \n      Photoshop CC 2017\n      18.x\n       \n      7.0\n    \n    \n      Photoshop CC 2018\n      19.x\n       \n      8.0\n    \n  \n\n\nSet aside the weird correspondence in the above version numbers, you see that we’ve jumped to CEP 8, and Photoshop internal version is now 19.0.0.\n\nVersions\n\nCEF has been bumped to 3.2987, Chromium is 57.0.2987.74 (leap of faith because for some mysterious reason window.location = \"chrome://version\" doesn’t work anymore) and Node.js is 7.7.4, whereas Generator’s Node.js is 4.8.4. Enough with digits, let’s dig into actual stuff. You’d be tempted to use Version=\"8.0\" in manifest.xml, like:\n\n&lt;?xml version=\"1.0\"?&gt;\n&lt;ExtensionManifest xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  ExtensionBundleId=\"com.example.helloWorld\"\n  ExtensionBundleVersion=\"1.0.0\" Version=\"8.0\"&gt;\n  &lt;ExtensionList&gt;\n    &lt;Extension Id=\"com.example.helloWorld.panel\" Version=\"1.0.0\"/&gt;\n  &lt;/ExtensionList&gt;\n  &lt;ExecutionEnvironment&gt;\n    &lt;HostList&gt;\n      &lt;Host Name=\"PHXS\" Version=\"19.0\"/&gt;\n      &lt;Host Name=\"PHSP\" Version=\"19.0\"/&gt;\n    &lt;/HostList&gt;\n    &lt;LocaleList&gt;\n      &lt;Locale Code=\"All\"/&gt;\n    &lt;/LocaleList&gt;\n    &lt;RequiredRuntimeList&gt;\n      &lt;RequiredRuntime Name=\"CSXS\" Version=\"8.0\"/&gt;\n    &lt;/RequiredRuntimeList&gt;\n  &lt;/ExecutionEnvironment&gt;\n  &lt;!-- etc. --&gt;\n&lt;/ExtensionManifest&gt;\n\nTempting, but don’t do it. Version in &lt;ExtensionManifest&gt; and &lt;RequiredRuntime&gt; has always matched CEP version, but in this case setting the two to \"8.0\" will make your panel disappear from the Window &gt; Extensions menu. Why? We’re in the same boat, I’ve asked around but got no plausible answer yet. Stick to \"7.0\".\n\nCSInterface\n\nI’ve inspected the diffs between CSInterface.js (CEP 7 vs. CEP 8) and it’s just punctuation in comments. LOL.\n\nDebugging\n\nDebugging tips: Google Chrome 62.0.3202.62 (Official Build) (64-bit) at least on macOS Sierra 10.12.6 can’t connect anymore a debug session. It seems it can: you successfully load localhost:8088 (or whatever port you’re using) and the usual textual link appears. Click it, and you’re staring at a blank Chrome Developer Tools page. Out of curiosity I opened the Console and got a “Uncaught Error: No setting registered: showAdvancedHeapSnapshotProperties” referring to inspector.js.\n\n\n\nWorkaround: download Google Chromium. The theory is that you should test your panel with the exact Chromium version implemented in the host application: yet, finding it is a majestic pain in the butt if you try to follow the official instruction for old builds. The closest one I’ve been able to get is 57.0.2987.0 and you can get it too here. For debugging in a working DevTool, Chromium-latest should work too.\n\nNode.js\n\nThere must be some kind of bloody war going on at the Adobe headquarters around Node.js. Only between CC 2015.0 and CC 2015.1.2 its implementation changed thrice, and of course there’s something new and backward not compatible in CEP 8 as well. Anyway, the State of the Art is as follows. Node is still disabled by default. Very like with CEP 7, if you want to enable it, add these in the manifest.xml:\n\n&lt;Resources&gt;\n  &lt;CEFCommandLine&gt;\n    &lt;Parameter&gt;--enable-nodejs&lt;/Parameter&gt;\n    &lt;Parameter&gt;--mixed-context&lt;/Parameter&gt;\n  &lt;/CEFCommandLine&gt;\n\nMind you, --mixed-context is optional. So you basically have three possible scenarios:\n\n\n  Forget about Node entirely (no --enable-nodejs flag): it’s disabled.\n  Enable Node in a Separate Context (--enable-nodejs but no --mixed-context).\n  Enable Node in a Mixed Context (--enable-nodejs and --mixed-context).\n\n\nAs a quick reminder on Contexts, due to the io.js switch back in an earlier version, I’ve been told, Node.js and the Browser/Panel contexts don’t talk to each other anymore. What does this mean? If you have a Node.js module and try to access the jQuery $ object, or csInterface, or anything else defined in the Browser context (e.g. your main.js file), it won’t work. Workaround: use the global window object as a bridge. Or enable Mixed Context.\n\nThis hasn’t change from CEP7, so I’ll skip to the new stuff. Besides injecting in the global space Buffer, global, process, require and module, CEP 8 adds a new cep_node global, which has its own Buffer, global, process and require properties. These new props props are useful because now an &lt;iframe&gt; (which still needs to have its own enable-nodejs specified), is going access cep_node, that you can use as a bridge object between &lt;iframe&gt; elements when the Context is Separate. This marks a difference with CEP7, where Node globals were always available. Also, order of appearance is important to access data, see below:\n\n&lt;body&gt;\n  &lt;!-- Node globals: OK, cep_node:OK --&gt;\n  &lt;iframe src=\"iframe1.html\" enable-nodejs&gt;\n    &lt;!-- Node globals: NO, cep_node:OK --&gt;\n    &lt;!-- You can set node_cep props, and access them \\*later\\*, e.g.\n    &lt;script&gt;\n      cep_node.process.pippo = \"Goofy\";\n    &lt;/script&gt; --&gt;\n  &lt;/iframe&gt;\n  &lt;iframe src=\"iframe2.html\" enable-nodejs&gt;\n     &lt;!-- Node globals: NO, cep_node:OK --&gt;\n     &lt;!-- You can access node_cep props set \\*before\\*\n    &lt;script&gt;\n      console.log(cep_node.process.pippo); // \"Goofy\"\n    &lt;/script&gt; --&gt;\n  &lt;/iframe&gt;\n&lt;/body&gt;\n&lt;!-- &lt;script&gt; tags belongs to iframe1.html and iframe2.html --&gt;\n\nAs opposed to the previous case and still specific to CEP8, with Mixed Context the Node globals are always available, everywhere (&lt;iframe&gt; elements included), but this time cep_node is rebuilt from scratch in each &lt;iframe&gt;.\n\n&lt;body&gt;\n  &lt;!-- Node globals: OK, cep_node:OK --&gt;\n  &lt;iframe src=\"iframe1.html\" enable-nodejs&gt;\n    &lt;!-- Node globals: OK, cep_node:OK --&gt;\n    &lt;!-- set node_cep props only for this iframe only, e.g.\n    &lt;script&gt;\n      cep_node.process.pippo = \"Goofy\";\n    &lt;/script&gt; --&gt;\n  &lt;/iframe&gt;\n  &lt;iframe src=\"iframe2.html\" enable-nodejs&gt;\n     &lt;!-- Node globals: OK, cep_node:OK --&gt;\n     &lt;!-- cep_node props set in another iframe are not available\n    &lt;script&gt;\n      console.log(cep_node.process.pippo); // undefined\n    &lt;/script&gt; --&gt;\n  &lt;/iframe&gt;\n&lt;/body&gt;\n&lt;!-- &lt;script&gt; tags belongs to iframe1.html and iframe2.html --&gt;\n\nWhat else: module and exports globals may conflict with some JS libraries, e.g. jQuery, resulting in the library being loaded in the Node context rather than in the Browser’s. As a fix, try:\n\n&lt;!-- Insert above script imports --&gt;\n&lt;script&gt;\n  if (typeof module === 'object') {\n    window.module = module; module = undefined;\n  }\n  if (typeof exports === 'object') {\n  \twindow.exports = exports; exports = undefined;\n  }\n&lt;/script&gt;\n&lt;!-- JS imports --&gt;\n&lt;script src=\"scripts/jquery.js\"&gt;&lt;/script&gt;\n&lt;script src=\"scripts/csinterface.js\"&gt;&lt;/script&gt;\n&lt;!-- Insert after JS imports, IF you need module, exports --&gt;\n&lt;script&gt;\n  if (window.module) module = window.module;\n  if (window.exports) exports = window.exports;\n&lt;/script&gt;\n\nOh, now in CEP 8 require() paths need to be absolute and not relative, like:\n\n\t// Replace this:\n\trequire(\"./js/lib/javascriptobfuscator.js\");\n\n\t// With either this:\n\tvar dir = csInterface.getSystemPath(\"extension\")\n\trequire(dir + \"/js/lib/javascriptobfuscator.js\");\n\n\t// Or:\n\trequire(__dirname + \"/js/lib/javascriptobfuscator.js\");  \n\nAnd I’d say this sums up things for Node.js\n\nBugs\n\nI’m listing here the known CEP 8 bugs, even the one already mentioned in the post.\n\n\n  Version=\"8.0\" in the &lt;ExtensionManifest&gt; and &lt;RequiredRuntime&gt; tags in manifest.xml (to match CEP version) makes the Panel disappear from the Extensions list.\n  Panel Geometry bug: setting &lt;Size&gt;, &lt;MinSize&gt; and &lt;MaxSize&gt; with equal values doesn’t produce a fixed size (unresizable) panel anymore, as it used to do with any other pre-2018 version. In order to make a fixed sized panel in 2018, set only the &lt;Size&gt; tag. That is to say: panel is fixed size in 2018 but not in CC, 2014, 2015, 2015.5, 2017 or the other way around. Very funny. Moreover, the very fist time a panel is collapsed/reopened, it assumes forever some completely wrong width/height, and may hide relevant content.\n  Bug with Panel Refresh: (either via ⌘+R on Chromium Dev Tools, or window.location.reload(true) in the panel). You may run into problems with Node after the first refresh unless you have both --enable-nodejs and --mixed-context flags set.\n  As I mentioned before, setting the panel’s window.location = \"chrome://version\" doesn’t load the Chrome Version page in the Panel anymore – it used to work fine with CC 2017.\n  Not really a bug, but be aware that some videos that used to play well (e.g. from YouTube) in CEP 7, won’t play at all in CEP 8. This is due to a WebM VP9 codec that Chromium doesn’t support anymore (the codec License doesn’t get along well with the Chromium Open Source License, so they dropped the support).\n\n\nDocumentation\n\nYou can have a look at the official CEP 8 Cookbook here. While writing this post, the CEP Github repository still has no CEF Client (CEF Client has been added and is found here), no updated ZXPSignCmd. Admins of the Adobe-CEP GitHub repository accepts push requests for the Cookbook, so users are allowed / encouraged to make the documentation better!\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["CC 2018","HTML Panels Tips"],
      id: 17
    });
    

    index.add({
      title: "Photoshop Scripting Course Update (August 2017)",
      category: ["Scripting"],
      content: "In October 2016 I’ve announced to be working on a course about Photoshop Scripting. Is it ready for release?\n\nAccording to my original (publicly undisclosed) plans, I would have put it for sale around April 2017. But I haven’t – yet. Why? When? WTF? TL;DR It will take some more months – yes, the unit of measure is “Months”, plural, and perhaps counted on two hands. So don’t hold your breath. But it will be published, and it’ll be worth the wait.\n\nThe truth, the whole truth and nothing but the truth\n\nI started working on the previous course, on Photoshop HTML Panels Development (book, sample code, and video tutorials), around September 2015 – I put it for sale on March 24th, 2016. Back then, I had to start from scratch in many, if not all, senses. I had to deal with the practical aspects of creating content out of one’s brain juices and trying to turn it into a product that other people may be willing to spend their money on – as a beginner. From book self-publishing tools to e-commerce platforms, down to video production, audio equipment, VAT/MOSS and whatnot.\n\nEach one of them has had its own learning curve – how does Leanpub work? Gumroad integration? Newsletter service? Adobe Audition? Time well spent, but extra time nonetheless. That summed up with the creation of, well, the actual stuff: teaching HTML Panels Development as efficiently as possible. With that in mind, I tried to follow the same idea of my previous course.\n\nThanks to which – it’s no secret – I have, financially speaking, saved my butt in 2016: for I’m still a freelance with variable incomes, also based on hourly rate jobs, and 2016 has happened to be one bad year. So I started in September 2016 working on the PS Scripting course. Same same but different, could I possibly follow my HTML Panels course schedule this time too?\n\n✔︎ No need to spend time re-learning the platforms and tools, I can use my previous work as a template.\n\n✔︎ I am more experienced in authoring a course now, so perhaps the process will be faster.\n\n✔︎ I may need to save my butt one more time, and if this is not a powerful drive, I don’t know what could be.\n\n✘ Photoshop Scripting, compared to HTML Panels, is a vast topic, a universe of its own. Hence, if the Panels book is 300 pages, I would expect the Scripting book to largely exceed 400.\n\n✘ With one course under my belt, I’d like some aspects of the next one to be a bit fancier. In other words, as an author, I know where there’s room for improvement.\n\nSumming pluses and minuses, I’ve told myself that it could work! But I was wrong, so very wrong.\n\nA couple of positive events hit me – on one side, I’ve found a technical editor who is a smart, talented and experienced developer friend of mine. She gave me a truckload of excellent feedback, which will require for me to put my hands back on the +200 pages of the first part’s first draft.\n\nOn the other side – a personal side – after 2.5 years of waiting mainly but not only because of the Italian bureaucracy, my wife and I could finally start the restoring works of our house, a portion of a renewed yet old (let’s say “ancient”) farmhouse that we bought in late 2014. We’ve spent the last full year signing papers, looking at drawings, paying way too many taxes, discussing walls, roofs, plaster, getting mad at carpenters, plumbers, discovering all kind of big and small issues in places and materials we happily ignored the existence of until then. We’re still halfway through it, but with the exception of the sewage, the Big Works should be done – now it’s time to throw the money into the bottomless pit known as “the finishing touches”: insulation, tiles, bathrooms, painting, doors, kitchen and the like.\n\nBelieve me – you don’t need to believe me if you’ve ever undergone this kind of “situation”: you just know for the rest of your life – you can’t possibly concentrate having somebody switching on a cement mixer at 8 AM, or hammering something upstairs, or while waiting for the gutter squad. Moreover, while putting on standby this long-term project of mine, I had to work on more mid and short-terms stuff in order to keep sustaining my family and the vampire, aka the house restoration.\n\nI’ve released, among the rest, two major updates of my Photoshop best seller Panels ALCE 3 and Double USM 2. I have new and old ongoing collaborations on other Panels, alongside with my usual routine job on Retouching. So, I failed miserably in trying to follow my old schedule. It just didn’t work for a mix of the above reasons.\n\nWhat to expect?\n\nI’m still 100% committed to having the PS Scripting course done, but it’s surely going to take many more months. I’m not willing to quantify, mostly because I’m still stuck midstream in the house works, yet I’m slowly getting back to the right mindset – I have to.\n\nI apologize if my initial announce has raised expectations that I’ve not been able to meet yet. What I can tell you is that the content I’ve written so far (7 chapters, +200 pages) is really exciting. I should keep a low profile, but I’m especially proud of the ActionManager section: you won’t find anywhere else about 60 pages that logically cover this topic, from the historical perspective up to the details of Descriptor inspectors and AM Setters. You gonna love it.\n\nSo, if you’ll need to wait, it’s going to be worth it – or at least, I’m putting all my energy into making the course worth your time. Thanks for your patience, and keep an eye on this blog for future updates! Cheers from the building site once known as “our house”.\n",
      tags: ["Professional Photoshop Scripting","Personal"],
      id: 18
    });
    

    index.add({
      title: "Third-party Photoshop Panels: Configurator Reloaded",
      category: ["CEP"],
      content: "I’m not used to talking about commercial products I’ve not personally developed on my blog, but I’ve decided it might be interesting to cover the work of other developers here, from time to time or when something catches my eye.\n\nToday I’d like to tell you about Configurator Reloaded, a Photoshop panel by Thomas Zagler, who revives the idea of Adobe Configurator (the original one).\n\nFor those of you who are awfully too young to know, or just not familiar with it, Adobe Configurator (an Adobe Technology Preview – a free product which last update dates 2013) was an easy, visual tool to build Photoshop Panels using a series of predefined, simple graphic elements such as containers, buttons, Photoshop tools, widgets, etc.\n\n\n\nMostly used to create simple, customized interfaces of existing tools, some developers like Giuliana Abbiati did raise the bar being able to build complex, commercial products with it, like the well-known Channels Power Tool. If you think it’s not well-know, please check it out.\n\nWhile transitioning from Flash to HTML Panels, more or less around the announcement of the Creative Cloud, Adobe discontinued Configurator, which is now pretty much useless as a tool, unless you’re stuck with CS6 and/or CC (Photoshop internal versions 13 and 14).\n\nPanels 101\n\nThird party panels are supported in Photoshop from version CS4: they were Flash/Flex things, and lived a happy life until Photoshop CS6 included. Adobe then released Photoshop CC as the only bridge version supporting both Flash and HTML Panels. Flash has been dropped permanently since CC 2014, and we all hope that HTML will have a longer life than his old cousin. Mind you, Flash support varies among CC applications, e.g. InDesign has abandoned Flash much later.\n\nAccording to information found in someplace in forums, Adobe has no short-term plan (i.e. no plan at all) to port the original Configurator to the present days of HTML. Which brings us to Configurator Reloaded: a third-party, independent project by the German developer Thomas Zagler.\n\n\n\nFirst and foremost, it’s not entirely a Configurator clone, but it covers many of the average users’ needs. That is to say, creating GUIs (Graphic User Interfaces) that contain Buttons calling existing Actions, Commands, Scripts and Tools:  These elements can be grouped in containers and manually ordered:\n\n\n\n\n\nContainers can be colored and labeled, and you have some amount of freedom in the configuration of the overall aspect of your panel. As opposed to Configurator (original), you cannot export, thus distribute, your project as a separate Panel: the customized GUI that you’ve created is accessed only within your copy of Configurator Reloaded Panel as one of its “Workspaces.” Of course, you can have multiple Workspaces, even though you can’t display more than one at the same time.\n\n\n\nAlso, Configurator (original) had “Widgets” such as Search, Movie Player, Popup Panel, etc. as well as “On Panel Initialization” scripts and other useful properties for the exported panel – that Configurator Reloaded simply lacks. Many users that fall in the “Adobe Configurator Orphans” group tend to immediately notice these absences; having often been critic towards Adobe’s bugs / missing features in the original product, it’s easy to fall into the trap of expecting Configurator Reloaded to fill the gaps, and resuscitating an abandoned product exactly where it’s been left.\n\nIn my opinion, we should remember that this project is not sponsored by Adobe by any means: instead, we’re talking about the initiative of an independent developer, who’s worked on a Panel that (I imagine) must be awfully tedious to build. Years ago, I built myself a simpler panel called PS Tools, (grouping only, guess what, Photoshop Tools): I remember it as oh-so-boring to code. Then Adobe’s engineers caught up and released the Tools Palette customization as a new feature – I’ve been a precursor :-)\n\nTo sum up, one might find “Configurator Reloaded” a name that winks at the original product; it benefits (from the marketing point of view) from its fame, even if the feature set is only partially comparable. That said, it delivers what it advertises, it’s very well built, the learning curve is minimal, and being at version One point O it’s fair to expect new features down the line. The price tag is in the mid-high range for Photoshop Panels, $44.90 (regional taxes included – I’m in Italy so the international price may vary) and can be bought here.\n\nPlease note: when possible, buy from the original developers to support their work – marketplaces are nice, but they eat out a sometimes considerable amount from the street price. As a huge fan of automation, I believe that whatever carves out even seconds from your daily job pays itself rather quickly, so stop reading this and try the available fully-functional demo version yourself. Your thoughts are welcome in the comments section below.\n\nDisclaimer: this blogpost contains an affiliate link to the Configurator Reloaded panel – you’ll support my blog as well if you end up buying it following this link. So many years spent in this business expressing freely my point of view should prevent you from thinking about this post as a gimmick to make extra money – it’s just not the case.\n",
      tags: ["Panels"],
      id: 19
    });
    

    index.add({
      title: "HTML Panel Tips #24: Fixing ZXP Timestamping errors",
      category: ["CEP"],
      content: "Recently, running the ZXPSignCmd command line utility to sign and timestamp HTML Panels has proved to cause errors. The problem lies in the Timestamp Authority.\n\nAdobe Configurator 4, which relies on it behind the scenes to export the (Flash) panel as ZXP is also affected. Quick fix as follows: hammer it! And if it doesn’t work, change the timestamp authority. We’ve been using https://timestamp.geotrust.com/tsa for quite some time now. Is its recent failure linked to the SHA-1 deprecation? I can’t say, but you can try a different service from this list – that comes from the small but always great CEP (HTML) Panels developers community.\n\n\n  http://tsa.starfieldtech.com\n  http://sha256timestamp.ws.symantec.com/sha256/timestamp\n  http://timestamp.comodoca.com\n  http://timestamp.verisign.com/scripts/timstamp.dll\n  http://timestamp.digicert.com\n  http://time.certum.pl\n\n\nI assume you know how to use it. If this is not the case, check out my previous posts: here and also here. Few notes: first, it might be that in the future Geotrust will be working again (I’ll update the post, if/when).\n\nSecond, I’ve tried a mild Configurator 4 hack, but without any success – I guess the ZXPSignCmd call is somewhere out of my reach – if you find a way to change the timestamp authority there, or if you want to suggest other timestemp URLs, please let me know in the comments!\n\nThird, make sure to have the latest version of the ZXPSignCmd, which you can download here.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["timestamp","ZXPSignCmd","HTML Panels Tips"],
      id: 20
    });
    

    index.add({
      title: "Double USM v2 for Photoshop has been released!",
      category: ["Photoshop"],
      content: "A major reworking of my Sharpening extension for Photoshop has been released.\n\nI’m happy to introduce you to the new features. They say that a picture is worth a thousand words – so a 720p video is… oh so much better! Enjoy the following weird, fun six minutes, that are everything you need to know about Double USM 2.\n\n\n\nWhat not to like? You can go buy it here on my website (preferred option) or soon on Adobe Add-ons as well.\n\nIf you need to ponder a bit more, let me recap what is that all about – briefly because we all have no time to waste. Double USM is about sharpening, i.e. all the tricks we use to turn newborn digital images into razor sharp sources of gorgeous detail.\n\n\n\nWhatever fancy technique you happen to use for this purpose, the goal is to visually enhance the edges of your picture’s subjects. Chances are that you and your Sharpening filter are doing so injecting halos: dark and light ones.\n\n\n\nHalos are created setting three parameters: Amount (intensity), Radius (thickness), and Threshold (noise control).\n\n\n\nThe UnSharp Mask filter and its descendants cannot discriminate the control of Light and Dark halos. They’re created equal. Which is not so cool: super-experts like Dan Margulis, pooling a large number of professionals over the years, have found that Light Halos are the ones giving the feeling of oversharpening – the cardinal sin of the post-producer.\n\n\n\nDouble USM 2 is to the rescue: you’re allowed to selectively control Amount, Radius and Threshold for Dark and Light Halos separately. Better, sharper images! Plus, you have advanced preview options and Presets.\n\n\n\nWhat for? Tone down Light Halos on Traditional Sharpening.\n\n\n\nUse HiRaLoAm (High Radius Low Amount) to enhance the local contrast.\n\n\n\nOr perhaps mix them both for creative purposes.\n\n\n\nFor the technically inclined, Double USM is not anymore a script, but a Panel, that drives a C++ plugin, internally working with 32bits precision. It supports 8bit, 16bit and 32bit (HDR) images, on Photoshop from version CC up to CC 2017, both Mac and PC.\n\nYou can find some before/after on my website CC-Extensions, which contains a big red purchase button as well. Transactions are managed by Fastspring, the very same service Adobe uses for its marketplace – a direct link for buying Double USM here. Alternatively, it will be available for purchase soon on Adobe Add-ons as well.\n\nThank you!\n",
      tags: ["Double USM","sharpening"],
      id: 21
    });
    

    index.add({
      title: "HTML Panel Tips #23: JavascriptObfuscator API Gulp.js Plugin",
      category: ["CEP"],
      content: "When developing HTML Panels, I’m always quite fanatic about code privacy: for a variety of reasons, I don’t want users to peek into my files. Lately, I’ve found particularly effective the obfuscation provided by javascriptobfuscator.com – which, surprisingly, works equally fine with both JavaScript and ExtendScript.\n\nTheir paid tier offers the possibility to access it through HTTP – a good candidate for Gulp automation. Since there’s no ready-made plugin available on the internet, I’ve ventured into building one; which I’m going to share with you in this post. Before going into the technical details of Gulp plugins, let me recap very briefly my point of view on code privacy.\n\n\n  HTML Panels should protect both JS and JSX, and JSX is known to be particularly resistant to aggressive minification and/or obfuscation.\n  To my experience, free obfuscation services are not that much reliable: even the apparently ugliest result can be automatically beautified in such a way that at least part of the code logic is exposed. Moreover, some of them are known to inject malicious code in the process.\n   I’ve evaluated two or three different options, and I’ve finally chosen the one I’ll talk about. I’m not sponsored in any way – alas :-)\n  No, JSXBIN isn’t as secure as it was in the past.\n\n\nPlease note that I’ll assume Gulp knowledge – I’ve already written about it here, please check that if you find yourself lost. That said, let’s see the example code they provide on this page, that I’ll use as a starting point – it’s the Javascript Obfuscator API Example Project (HTTP + JSON API). The following is found in a script tag of an html page.\n\nvar proj = {};\n\nproj.APIKey = \"\"; // &lt;- your API Key\nproj.APIPwd = \"\"; // &lt;- your API Password\nproj.Name = \"Sample1\";\nproj.ReplaceNames = true;\n\n// for more parameters of the proj object , please reference\n// http://service.javascriptobfuscator.com/httpapi.asmx?WSDL\n\nvar item0 = new Object();\nitem0.FileName = \"test0.js\";\nitem0.FileCode = \"function hello(x,y){var z=11;return x+y+z;}\";\n\nvar item1 = new Object();\nitem1.FileName = \"test1.js\";\nitem1.FileCode = \"var strs=['aaaa','bbbb','cccc','dddd','eeee','abcdefghijklmnopqrstuvwxyz0123456789']\";\n\nproj.Items = [item0, item1];\n\nvar url = \"https://service.javascriptobfuscator.com/HttpApi.ashx\";\n\nvar xhr = new XMLHttpRequest();\nxhr.open(\"POST\", url, false);\nxhr.setRequestHeader(\"Content-Type\", \"text/json\");\nxhr.send(JSON.stringify(proj));\n\nif (xhr.status != 200) {\n  alert(\"Http Error :\" + xhr.status + \"\\r\\n\" + xhr.responseText);\n} else {\n  var result = JSON.parse(xhr.responseText);\n  if (result.Type != \"Succeed\") {\n    alert(\" ERROR : \" + result.Type + \":\" + result.ErrorCode + \":\" + result.Message);\n  } else {\n    for (var i = 0; i &lt; result.Items.length; i++) {\n      var div = document.createElement(\"DIV\");\n      div.style.cssText = \"margin:12px;border-bottom:solid 1px gray;\";\n      div.innerText = result.Items[i].FileCode;\n      document.body.appendChild(div);\n    }\n  }\n}\n\nLet’s inspect this. First, you need your API Key and API Password, that you can find in your JavascriptObfuscator dashboard, as soon as you’ve subscribed to one of the paid tiers. A proj object is created, and some properties assigned: your API data, the project name, and a series of parameters that will drive the obfuscation process. In this case, ReplaceName set to true (line 6). In the same call, you can send for processing more that one single file: they must be wrapped with an object (here item0 and item1), which actual code is found in the FileCode property (lines 11-17).\n\nThen they’re both stored into an array that is assigned to the proj.Items property. Next, a standard XMLHttpRequest of type POST is sent, passing the stringified proj object (line 26). The response is then showed in a couple of div elements in the page. Enough for this example. The kind of API that I’d like to consume in my real code is as follows – this comes directly, yet slightly simplified, from my gulpfile.js\n\nvar gulp = require('gulp'),\n    jso  = require('./jsobfuscator');\n\n// several previous tasks here...\n\ngulp.task('jso', function() {\n  return gulp.src([\n    'src/app/jsx/photoshop.jsx',\n    'src/app/app.js',\n  ], { base: './src/' })\n  .pipe(jso())\n  .pipe(gulp.dest(\"dist\"))\n});\n\nI’m passing two files in the source array – the ones I’m interested in deep obfuscating: the main application JS, and the only JSX of the Panel. The processed result is going to be saved in the dist folder, keeping the same directories structure. As you see, I’m requiring './jsobfuscator': this means that I’ve created in the root folder a jsobfuscator.js file, which contains my actual Gulp plugin. The entire code is as follows, read along for a walkthrough.\n\n400: Invalid request\n\n\nBefore going any further, check this Writing a Gulp Plugin documentation page as a way to orient yourself: I’ll point out the main problems I’ve run into as a total Gulp plugins beginner, that I’ve been able to solve after a good deal of head scratching time. First, you need to require some modules: the native fs, stream, and http, plus through2, gulp-util, and vinyl – install them:\n\nnpm install --save-dev through2 vinyl gulp-util\n\nI’m exporting a single function, that returns (line 84) the transform function passed through the through.obj() method: this is the way a Gulp plugin works.\n\n\n\t\n\n\nThe transform function (line 10) is exactly where everything happens. I’ve recreated there the proj object you’ve seen in the demo file, passing a lot more parameters. These ones (lines 21-29, like MoveStrings, DeepObfuscate, etc.) should mimic the setup illustrated in the screenshot at right, that comes from the JavascriptObfuscator Windows-only GUI.\n\nThat’s what you’re allowed to set with a Basic membership ($19 per month). I’m not 100% sure I’ve been able to replicate everything: if you have a better understanding of the parameters, please let me know in the comments. Next, I’ve created the wrapper object (appJS, line 31) that will contain the plain code to be obfuscated: please note that the FileCode property this time can’t be just equal to file, which is a peculiar object called Vinyl File – what your Plugin should receive and, eventually, return. Instead, you’re interested into the contents prop of the Vinyl File, which happens to be a Buffer: so you need to stringify it, passing the correct encoding ('utf8').\n\nAt line 35, appJS is inserted in the proj.Items array; I’m then creating a postData variable, that holds the String I’ll eventually need to POST to the  JavascriptObfuscator server – the stringified version of the entire proj object. Line 41-49, the options object will be needed in the request at line 80: I have to specify the host (no prepended https), path, method, and headers. Without 'Content-Type' and 'Content-Length' (see this StackOverflow thread for the Buffer.byteLength() method used to calculate the latter) the request will fail. Line 49, postCallback is called when the request is dispatched.\n\nAs soon as data is coming, i.e. response.on('data'), a string is formed. When the response is terminated, i.e. response.on('end'), I’m parsing the received JSON string into an object (line 62). If you want to log it – or log anything to debug your code – use the gutil.log() function, that you can find in the code comments here and there. The horribly, properly obfuscated string is found in the resObj.Items[0].FileCode prop, hurray! The only problem is, as I’ve mentioned earlier, that your Gulp plugin needs to return a Vinyl File. Hence, a brand new Vinyl File called jsFile is created (line 66) using current working directory, base, and path from the original file. What about the contents? You cannot stick in there the resObj.Items[0].FileCode String, because contents must be a Stream. Thanks to the holy StackOverflow, I’ve learned that a Readable (line 64) can turn String into Stream, and I’m a happy camper now.\n\nLast but not least, this Vinyl file must be pushed down the line, for the Gulp Plugin to work. Was I not into a callback, nested in a function, which in turn is nested into the main transform function, I would have used this.push(jsFile). But I happen to be at a location where this is not what it used to be: that’s the why I’ve stored it (line 15) into the that variable. So that.push(jsFile), line 75. At the very end, you need to run the transform’s callback (line 76), and you can call quit.\n\n\n\nThe above took me more than a full day of work to figure out… It’s been a self-taught crash course on Gulp Plugins – worth sharing to you and my future self when I’ll have forgotten every juicy detail of it; in about… a couple of months maybe? There is plenty of room for improvement – I would have liked to merge multiple source files not into a single file, but into a single http request: splitting the various input files into several elements of the proj.Items array. Yet, I have no idea how to do so (suggestions in the comments are welcome!). Error checking is completely missing, and so on and so forth. Rough as it is, it works; and every second saved from manual labor is worth the effort, isn’t it?\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["Gulp","obfuscation","HTML Panels Tips"],
      id: 22
    });
    

    index.add({
      title: "Vue.js – Nonlinear Sliders with Computed Properties",
      category: ["CEP"],
      content: "\n  term\n  definition\n  another definition\n\n\nWhile working on the forthcoming version of my DoubleUSM script, which I’m porting to HTML Panels, I’ve run into the following problem: how do you fit a large range – say, (1..500) with floating point precision – into a Slider which has, at best, less than 200 possible, real slots? Nonlinear sliders and Vue.js Computed Properties are the answer, read along.\n\n\n\t\n\n\nIf you play with the Photoshop UnSharp Mask dialog, which looks like what you see here, you’ll notice that the three sliders work in a very different fashion.\n\nAnd I’m not only talking about the sheer range: Amount is 1..500 (integer), Radius 0.1..1000 (floating point, 10.000 possible values!), Threshold 1..255 (integer).\n\nPerhaps more importantly, the sweetest range of each parameter – i.e. the most probable, or used – is different. As a consequence Adobe engineers have expanded it, to the detriment of other, less probable values. An example: the Radius, in a so-called traditional USM, rarely goes beyond 3px, depending on the image size. Let’s say 5px, or even 10px to be on the ultra-safe side. When it comes to inverse USM (or as my color correction maestro Dan Margulis would call it, HiRaLoAm: High Radius, Low Amount) Radius can go up to 50px, or even more. Not more than 100px, I’d bet, in 98.98% of every HiRaLoAm run in the civilized world. Let’s forget Adobe’s crazy ceiling of 1000px: I’ll stick to a maximum value of 500px. You’re facing a couple of issues:\n\n\n  Fitting 5000 values into a Slider that, at best, is 200px wide.\n  Expanding and compressing parts of the [0.1..500] slider span so that the more used/probable ranges have more coverage.\n\n\nI’m going to use the following very simple setup as a starting point:\n\n\n\t\n  Click the image to open the JSBin\n\n\nAs you see, I’ve bound the slider (aka input of type range, width of 175 pixels) and the number box (input of type number: I’ve hidden the spin boxes because I think they’re ugly) to the same radius property using v-model. It kinda works, but functionally, as a Radius slider, it’s unusable. I can’t precisely target any small value (say 0.4), because the whole slider is just too tiny to keep track of them: the first available value is 2.3! How did Adobe solve that issue? The only way to find it is experimentally: I’ve grabbed screenshots of the USM dialog at different Radius values, and measured the pixel span of the slider thumb aka handler. Annoying, and perhaps not ideal, but it worked. I’ve used a spreadsheet to graph the range pixels span (first column), against the radius value (last column)\n\n\n\nAs you see, the original Adobe’s slider spans about 500 actual pixels to cover 500 integer values: problem is that the Radius is expressed as floating point, and the behaviour is highly nonlinear, as the graph shows. Move the handler about halfway through (242px) and the Radius is only 20px (and not 250px), a twenty-fifth of the entire range has been very much expanded. In the spreadsheet I’ve added two column, with normalized values (0..100) of the measured pixels, then mapped to my original slider actual values of the Slider, that will be of some use later on.\n\n\n  You’ll forgive me if I’m a bit imprecise with these calculations. Fact is that I have no idea whether a 175px width slider has room, for its thumb/handler, for 175 actual different positions when you drag it. I’ve set in the HTML its min=0 and max=175, roughly matching its current width in pixels. BTW 0..175 is a range of 176 values.\n\n\nLooking at the graph, you can see four separate, linear segments: [0..10], [10..50], [50..200], [200..500], which is bearable: this is what they do, and I could perhaps replicate them. I’m afraid it doesn’t suit my own taste so I’ll change the mapping to fit my own idea of the proper slider behaviour for USM Radius. And the way to do that is via VueJS Computed Properties. The idea is that we’ll bind (via v-model) the spinner (number field) to the same radius property of the initial example, while the slider to another prop called  sliderRadius, calculated on the fly from radius. This goes in the computed section of the Vue instance, see below (please note that I’m returning the same, unprocessed value for now).\n\n\n\t\n  Click the image to open the JSBin\n\n\nThe getter here is the function that computes sliderRadius from radius, while the setter does the reverse (sets the radius from sliderRadius). In practice, the getter is involved when you type number values in the spinner; the setter when you drag the slider. With this in place, it’s time to compute the two functions. I’ve decided that four linear ranges like [0..5], [5..30], [30..100], [100..500] are a better fit for me – see the following graph.\n\n\n\nIn case you’re dubious, I’m plotting now Radius in the  $ x $, Slider in the  $ y, $  whereas the in previous graph I did the reverse (Slider  $ x $, Radius  $ y $). I’ve set the points so that there is a (theoretical) match for the important ranges. For instance, the Radius range [0..5] is mapped to [0..50] in the slider, so that (theoretically) each point value (0, 0.1, 0.2, 0.3…) has a perfect match with the slider (0, 1, 2, 3, and so on). At least this is what I hope is the case, because – as I’ve written – there’s no guarantee that a 175px slider has 175 available positions for the thumb. Now it’s only a matter of finding the $ a $ and $ b $ coefficient for a linear equation $ y = ax + b $. Solving that for $ a, b $ is easy: $ a $ is rise over run e.g. for the getter in the second range [5..30]:\n\n\n\nWe know all the Radius/Slider pairs, so b is got substituting the x,y and recently found a values in the linear equation, eg.\n\n\n\nAs a result, the linear equation for the second range [5..30] getter is:\n\n\n\nThat’s for the getter. You have to invert Slider and Radius to obtain setter coefficients, so for instance (setter, second range):\n\n\n\n\n\n\n\nWhich translates into the following code:\n\nvar vm = new Vue({\n  el: '#app',\n  data: {\n    radius: 100\n  },\n  computed: {\n    sliderRadius: {\n      get: function() {\n        var val = this.radius;\n        if (val &lt;= 5) {\n          return val * 10;\n        } else if (val &lt;= 30) {\n          return val * 2 + 40;\n        } else if (val &lt;= 100) {\n          return val * 0.714285714285714 + 78.5714285714286;\n        } else {\n          return val * 0.0625 + 143.75;\n        }\n      },\n      set: function(val) {\n        val = +val;\n        if (val &lt;= 50) {\n          this.radius = Math.round10(val * 0.1, -1);\n        } else if (val &lt;= 100) {\n          this.radius = Math.round10(val * 0.5 - 20, -1);\n        } else if (val &lt;= 150) {\n          this.radius = Math.round10(val * 1.4 - 110, -1);\n        } else {\n          this.radius = Math.round10(val * 16 - 2300, -1);\n        }\n      }\n    }\n  }\n});\n\nI’ve added a Math.round10 function borrowed from MDN, which allows me to send to the spinner properly formatted numbers. Also note that I’ve changed accordingly the HTML, limiting the slider to max=175 and, importantly, setting the step to 0.1: otherwise, when you change numbers in the steppers, the slider would jump. The final, working JSBin is this one:\n\n\n\t\n  Click the image to open the JSBin\n\n\nAs you see, it maps correctly small floating point values, then ramps up nicely and compresses high, less used Radii. Which doesn’t mean that you can’t set them, because you can always type, say, 489.3px in the spinner, and bind that value to your actual Photoshop JSX function call. It’s not rocket science at all, but it took me some time to put this together for my own panels – that’s why I’m sharing here, both for you and my future, senior self 😉\n\nIf you’re into Vue.js, you might want to check also: Vue.js – Binding a Component in a v-for loop to the Parent model.\n",
      tags: ["Vue.js"],
      id: 23
    });
    

    index.add({
      title: "I am authoring a new Course! And it will be on Photoshop Scripting",
      category: ["Scripting"],
      content: "You might have noticed that updates on my blog are getting a bit sparse: no worries, I’m up and running as usual. Actually I’m busy as hell – which is quite ironic, since I’d aim for a simple(r) life. But anyway: one year ago I started writing my Photoshop HTML Panels Development course, which I’ve successfully published in late March 2016.\n\nI’m now back at my desk, working on a very similar project, and targeting no less than… Photoshop Scripting. Yup. First things first, here’s the way for you to both receive news on the project status as months go by – the release date is Q2/2017, so don’t hold your breath – and an exclusive, substantial discount coupon few days before the course will be publicly available. The mailing list will have ultra-low traffic, don’t expect to receive more than two or three emails during these months.\n\n\n\nPlease subscribe, it’s also useful for me to assess what the interest upon this course is like; besides, it’ll save you some good money.\n\nTell me more!\n\nI’ve been working for something less than a couple of months, so it’s really too early to describe a course that is in its early development phase. I can tell you that what has driven me undergoing such a crazy task is the lack of existing resources, describing a linear learning path to get to Scripting enlightenment. Forums are full of helpful people, there are blogposts, articles, references – yet, the available documentation (in the term’s broadest sense possible) is scattered to say the least. Which, alas, is very Adobe-ish.\n\nThe Scripting community itself is composed of very few, very talented and skilled developers; while the average coder is either self-taught (and that’s a tough business – been there myself), or with a patchy knowledge. Web and JS developers entering the Photoshop world may have hard time too – ExtendScript has its own quirks and peculiarities indeed. What about other CC apps? I’ve always had the impression that the InDesign Scripting community – on average – is better equipped; and please note that I’m not interested in the developers’ elite, but the community’s at large. It might be that InDesign has a better, cleaner ExtendScript implementation (yes, it’s different); I’ve had a look at their official documentation, and I’m biased to think that, as always, education plays a role.\n\nThis course aims to be my own take on providing one, among the many possible others, learning path. It’s a pretty darn minefield, though. The topic’s so vast that it’s impossible for me to cover it all – so most of my sweating goes into finding the right way, to cover the right topics, in the right order (being “right” what makes sense to me). The course target is impossibly crazy both newcomers and  experienced developers. I’ve started softly, emphasizing not only the language, but the programmer’s mindset too.\n\nAnd yes, I will devote an entire fat chapter on ActionManager, aka Scripting Voodoo. I’m eager to get there, but it’ll take time :) I can’t provide any sample content yet, except few screenshots: compared to the HTML Panels course, this one looks better to my eyes – I’ve refined my style, also thanks to customers’ feedback.\n\n\n\t\n\tRandom pages from the first draft\n\n\nI plan to blog about this course three-four times in total: the early announcement (this very post), when I’ll be midway through it, and finally to announce the actual release date, with a final reminder. You won’t hear me talking too much about this :) A dedicated website will follow, very much like the HTML Panels course, and bundled videotutorials are planned as well. To save your fingers a little scrolling work, here is the newsletter form again – please subscribe if you’re interested. Thank you! 🙏🏻\n\n\n",
      tags: ["Professional Photoshop Scripting"],
      id: 24
    });
    

    index.add({
      title: "Vue.js – Binding a Component in a v-for loop to the Parent model",
      category: ["CEP"],
      content: "Learning Vue.js is fun – if I run into a problem that has taken me some head scratching time to solve and/or and no easy Stack Overflow answer, why not writing a blog post for you and my future self? 🙂 Today’s stumbling block is bi-directionally binding of a Component (v-model), to the root data object – being the Components generated in a v-for loop.\n\nSounds unclear? Think about a lot of instances of a Component containing, say, checkboxes or radiobuttons, automatically generated from an array. It’s a quite frequent scenario, at least in my projects, so let’s have a look.\nUPDATE [October 2017]: The article refers to Vue 1.x – with version 2, things have slightly changed – I’ve updated the Checkbox section with code working with Vue 2.5.2.\n\nCheckboxes\n\n[See the Vue 2.5.2 update at the end of this section] I’m going to show you a couple of different setups and solutions. First initial arrangement is as follows:\n\n&lt; body id=\"main-container\"&gt;\n\n  &lt; input type=\"checkbox\" value=\"0\"\n          id=\"boxes[0].name\"\n          v-model=\"boxes[0].status\"\n          /&gt; zero\n\n  &lt; input type=\"checkbox\" value=\"1\"\n          id=\"boxes[1].name\"\n          v-model=\"boxes[1].status\"\n          /&gt; one\n\n  &lt; input type=\"checkbox\" value=\"boxes[2].name\"\n          id=\"two\"\n          v-model=\"boxes[2].status\"\n          /&gt; two\n  &lt;!-- etc --&gt;\n&lt;/body&gt;\n\nWhile the Javascript and the Vue code is:\n\nvar vm = new Vue({\n  el: '#main-container',\n  data: {\n    boxes: [\n      {'name' : 'zero',  'status': false},\n      {'name' : 'one',   'status': false},\n      {'name' : 'two',   'status': false},\n      {'name' : 'three', 'status': false},\n      {'name' : 'four',  'status': true },\n      {'name' : 'five',  'status': false},\n      {'name' : 'six',   'status': false},\n      {'name' : 'seven', 'status': false},\n      {'name' : 'eight', 'status': false},\n      {'name' : 'nine',  'status': false},\n    ]\n  }\n});\n\nIn this scenario it is particularly important for me that the v-model of each checkbox is bound to the boxes array of objects, so that other functions are able to refer to them (say, passing their values down to the Photoshop JSX layer). But clearly the checkboxes cry to be implemented as Vue Components! So let’s do that – first in the HTML as a template – that, by now, is mostly an empty placeholder:\n\n&lt;template id=\"boxes-template\"&gt;\n  &lt;div&gt;\n  &lt;input type=\"checkbox\"\n         value=\"\"\n         id=\"\"\n  /&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n\nWe need to register the Component in the JS, so:\n\nvar BoxesChecks = Vue.extend({\n  props: ['boxIndex', 'boxName', 'boxStatus'],\n  template: '#boxes-template'\n});\n\nVue.component('boxes-checks', BoxesChecks);\n\nvar vm = new Vue({\n// etc. etc.\n});\n\nPlease note that I’m using both camelCase and kebab-case, as the Vue.js doc suggests. I’ve declared three props:\n\n\n  boxIndex: will be 0, 1, 2, 3… 9.\n  boxName: the checkbox string label, like \"one\", \"two\", etc.\n  boxStatus: will be the Checkbox checked status (either true or false), and will be bound to the data.boxes Object.\n\n\nOK, how do we implement the v-for loop, so that all the ten Components instances are rendered on the page? A good starting point is:\n\n&lt;boxes-checks v-for=\"box in boxes\"&gt;\n&lt;/boxes-checks&gt;\n\nSo far so good, nothing that you can’t figure by yourself. Here comes the tricky part, have a look.\n\n&lt;template id=\"boxes-template\"&gt;\n  &lt;div&gt;\n    &lt;input type=\"checkbox\"\n           value=\"{ { boxIndex} }\"\n           id=\"{ { boxName } }\"\n           checked=\"{ { boxStatus } }\"\n    /&gt;\n    [{ { boxIndex } }] { { boxName } } is: { { boxStatus }}\n  &lt;/div&gt;\n&lt;/template&gt;\n\n&lt;boxes-checks\n    v-for=\"box in boxes\"\n    :box-index=\"$index\"\n    :box-name=\"box.name\"\n    :box-status=\"box.status\"\n    &gt;\n&lt;/boxes-checks&gt;\n\n{ { boxes | json } }\n\nThe checkbox value is { { boxIndex } }, that is passed as a prop using the shorthand syntax :box-index (which stands for v-bind:box-index) and assigned to the special property provided by the v-for loop $index. Note here camelCase and kebab-case. Similarly, { {boxName} } and { {boxStatus} } are passed in the loop using :box-name and :box-status, and the latter is used to fill the checked property of the input tag.\n\nThis way, the component “knows” whether to be initialized as checked / unchecked depending on the status property of the respective object in the boxes array. I’ve also added as a label for each box component a string containing them all, and at the bottom the JSON version of the boxes object to check if binding works properly.\n\n\n\t\n\n\nWe’re slowly getting there: each checkbox shows its index, name and status, and the one labelled “four” correctly initializes itself as “checked”. But if you try clicking them, neither their status, (true/false in the label) nor the root boxes object updates. This binding requires a couple of new lines of code, one in the Component’s template – i.e. a click handler:\n\n&lt;input type=\"checkbox\"\n       value=\"{ { boxIndex } }\"\n       id=\"{ { boxName } }\"\n       checked=\"{ { boxStatus } }\"\n       @click=\"changeStatus\"/&gt;\n\nand the corresponding function in the Components declaration in the JS:\n\nvar BoxesChecks = Vue.extend({\n  props: ['boxIndex', 'boxName', 'boxStatus'],\n  template: '#boxes-template',\n  methods: {\n    changeStatus: function() {\n      this.boxStatus = !this.boxStatus;\n    }\n  }\n});\n\n\n\t\n\n\nThis triggers a change in the Component’s boxStatus property, so that when you click each checkbox, you see that its own status (logged in the label) changes accordingly. Which is cool, but there’s just one piece of the puzzle missing – can you find it? The Component’s boxStatus is updated, but the Component (or better: each Component) has an isolated scope of its own! In fact, the boxes object, logged as JSON is not changed (every status is false but one). In order to make a two way binding, you need to add the binding type modifier .sync to the prop, like:\n\n&lt;boxes-checks\n  v-for=\"box in boxes\"\n  :box-index=\"$index\"\n  :box-name=\"box.name\"\n  :box-status.sync=\"box.status\"&gt;\n&lt;/boxes-checks&gt;\n\nThis correctly set everything: each Component is initialized with the boxes object, and two-ways bound to it – the same way it was just before refactoring with Components. See the full code in this JSBin here\n\n\n\nVue 2.x October 2017 UPDATE With the new version, .sync has been first deprecated, then re-introduced (see here). Yet, letting a child component modify parent data is considered an anti pattern in Vue: in this case, you should emit events and use a new Vue instance as a hub. See the following JSBin as an example.\n\n\n\nRadioButtons\n\nFollowing the same idea, let me show a different solution for RadioButtons, using events. The component looks like that:\n\n&lt;template id=\"channel-radio\"&gt;\n  &lt;label class=\"topcoat-radio-button\" style=\"display:block\"&gt;\n    &lt;input type=\"radio\" name=\"topcoat\"\n               checked={ { channelChecked } }\n               @click=\"changeRadio\"&gt;\n      &lt;div class=\"topcoat-radio-button__checkmark\" style=\"margin-bottom:4px\"&gt;&lt;/div&gt;\n      { {channelName} } [{ { channelIndex } }]\n    &lt;/label&gt;\n&lt;/template&gt;\n\n&lt;channel-radio\n  v-for=\"channel in channels\"\n  :channel-name=\"channel.name\"\n  :channel-index=\"channel.index\"\n  :channel-checked=\"channel.checked\"\n  &gt;\n&lt;/channel-radio&gt;\n\nYou see that there are channelName, channelIndex and channelChecked props, with a similar changeRadio function for the click handler. The loop is based upon a channels array, see the JS side:\n\nvar ChannelRadio = Vue.extend({\n  props: ['channelName', 'channelIndex', 'channelChecked'],\n  template: '#channel-radio',\n  methods: {\n    changeRadio: function() {\n      this.$dispatch('radioClick', this.channelName);\n    }\n  }\n});\n\nVue.component('channel-radio', ChannelRadio);\n\nvar vm = new Vue({\n  el: '#main-container',\n  data: {\n    channels: [{\n        'name': 'Luminosity',\n        'index': 0,\n        'checked': false\n      },\n      {\n        'name': 'Red',\n        'index': 1,\n        'checked': true\n      },\n      {\n        'name': 'Green',\n        'index': 2,\n        'checked': false\n      },\n      {\n        'name': 'Blue',\n        'index': 3,\n        'checked': false\n      }\n    ]\n  },\n  events: {\n    radioClick: 'handleRadioClick'\n  },\n  methods: {\n    handleRadioClick: function(radioName) {\n      for (var i = 0; i &lt; this.channels.length; i++) {\n        if (this.channels[i].name === radioName) {\n          this.channels[i].checked = true;\n        } else {\n          this.channels[i].checked = false;\n        }\n      }\n    }\n  }\n});\n\nClicking on each radiobutton component now dispatches a 'radioClick' message (defined in the component’s methods object, and carrying the channelName as the payload), handled by the root Vue instance (see both events and methods objects).\n\nThe handleRadioClick then adjust the checked properties on each object within the channels array, that the radiobuttons are bound to – please note that because of this, there’s no need to add .sync in the template now.\n\nSee it in action in this JSBin here.\n\nThat’s it! It took me a while to figure this out. Possibly the second example – where the parent (root, here) Vue instance is in charge of updating the data object, to which the Component is bound – is “more proper” than the first one – where the Component did it by itself.\n\nHope this helps! 👐🏻\n",
      tags: ["Vue.js"],
      id: 25
    });
    

    index.add({
      title: "HTML Panel Tips #22: Protecting your Code",
      category: ["CEP"],
      content: "About one year ago I had a so-called aha moment and decided to write a book. I had two or three subjects in mind, first choice was HTML Panels’ Licensing Solutions – i.e. how to build trial versions, anti-piracy systems, and the like. Luckily, and I really mean: luckily, I changed my mind and tackled a topic appealing to a slightly broader audience: the HTML Panels Development course was born.\n\nStill, licensing systems in the context of HTML Panels are a soft spot of mine (see my old post about partial serial number verification), and I wish I had time to write that book! I did build, from my biased point of view, very good prototypes back then: for instance implementing RSA encryption, or server-side automatic licensing files delivery to be used in conjunction with e-commerce providers. Whatever you choose to do, you’re protection system must rely on secured code that nobody can look at – which is what this article is all about.\n\n\n\nSealed enough\n\nAs a due preamble, there’s no such a thing as unbreakable software. Let’s admit however that you and I, we’re possibly not building stuff that has very much appeal to the international crackers community, no? For whatever reason we like to keep our code hidden (privacy, licensing systems as a form of respect to paid users…), chances are that the malice level of our wannabe pirates is not worldwide top-notch. Neither we want to die in the process of keeping our code protect, and spend too many sleepless nights over that.\n\nAs an engineer I collaborate with uses to say, “I would rather spend time building and earning money on new products”. So, a fair compromise in my opinion is to do the best that we can, being aware of both the context our products belong to (their price, spread, number of users, etc.) and the resources needed to protect them. So my own goal is to make it “sealed enough”. Where to put the bar, it’s up to you.\n\nThe tale of JsxBin\n\nOne year ago, I based all my researches and demos around licensing systems upon a very crucial feature of Panels: JsxBin code. HTML and Javascript are commonly considered inherently insecure, due to the discoverability / readability of the code. Not the case of JsxBin, a blob of gibberish nobody’s able to understand. So I carefully diverted sensible operations like http calls, cryptography etc. to the ExtendScript layer; a bit of a hassle for several reasons (main one: ExtendScript is stuck, and will probably be ‘till the end of the times, to ECMAScript 3 compatibility), but worth the effort. JsxBin was the closest thing to binary code we had.\n\nTo make things clear, it’s not binary at all but more like an advanced form of cyphertext – a sealed box we could trust. In fact we trusted it so much that (despite being the task utterly boring, and illegal too) one would have tried to reverse-engineer it, either (a) because of a very twisted idea of fun (b) to enjoy his/her own failures as the JsxBin safeness confirmation. I know you know JsxBin if you’re reading this, but here it is what it looks like:\n\n@JSXBIN@ES@2.0@MyBbyBn0ACJAnASzCjNjFByBWzGiPjCjKjFjDjUCEzEjOjBjNjFDFePiEjBjWjJjE\njFhAiCjBjSjSjBjOjDjBzDjBjHjFEFdhIzHjDjPjVjOjUjSjZFFeFiJjUjBjMjZzFjHjSjFjFjUGNyBn\nAMEbyBn0ABJEnAEXzHjXjSjJjUjFjMjOHfjzBhEIfRBFeFiDjJjBjPhBff0DzAJCEnftJGnAEXGfVBfy\nBnfABB40BiAABAJByB\n\nTheory says that, like with any other cyphertext, given an infinite number of plain text and cypher pairs (say, Jsx and corresponding JsxBin), you can find the key to decode it. Which is what ExtendScript Toolkit (aka ESTK) provides you: just start with var a; and inspect the resulting JsxBin code, then keep adding complexity and write down your findings. On and on and on… until you’re bored to death (unless you’re a cryptography enthusiast).\n\nYou will understand me if I’m vague here – we’re at the dangerous intersection of legal and privacy issues – but lately JsxBin proved to be… not as sealed as we liked to think. I’m not saying that everybody can flip and turn it back into readable code in a snap (can you? can somebody you know?), but the probability that somebody can decypher your JsxBin isn’t zero anymore. I’d say quite small, but not zero.\n\nObfuscation\n\nSo what? Well, if your paranoid side isn’t zero either, you must think about some other (I’d say “extra”) form of protection, namely: obfuscation. Let me address this once and for all: javascript obfuscation is getting better and better and, with the balance between efforts, results and context we operate within – that I talked about before – I find it a very good fit. I want to make a couple of important points, though.\n\nFirst, obfuscation is not going to substitute JsxBin: think about obfuscation strategies as layers, that you can overlay one on top of the other. E.g. you can obfuscate your Jsx, then JsxBin it (it can get more complex than that, but it gives you the idea). Second, beware of free obfuscation services. It turns out that “Javascript obfuscation” is such a popular query on Google that people are starting to take advantage of it. You can find articles online about strange calls made by websites who use JS code that has been obfuscated for free: guess what, malicious code has been injected during the minification/optimization/obfuscation process (go figure where) and this opens a horrid can of worms in terms of security that you don’t want to mess with, period.\n\nBut again, while Javascript in HTML Panels relies on CEF (the Chromium Embedded Framework) engine to be parsed – and that’s the modern Google V8 engine we all love – ExtendScript is an entirely different beast. I did test minification systems in the past, and my heart broke seeing how almost all the efforts to compress code resulted in no-longer-working code. That is, possibly due to the fact it doesn’t know about modern JS syntax, ExtendScript is remarkably sensitive when it comes to being massaged.\n\nMinifiy, Optimize, Obfuscate?\n\nI’ve been using these terms loosely, but they express different ideas.\n\nMinification is the process of compressing your code, usually to be delivered faster over the internet, consuming less bandwidth; depending on the minifier you pick up, variable names can be changed as well, so that:\n\nfunction f1(arg1, arg2, arg3) {}\nfunction f2() {\n   var var1, var2, var3;\n}\n\n// becomes:\n\nfunction f1(a,b,c){}function f2(){var a,b,c;}\n\nOptimization, instead, means that code analysis is performed, and various techniques are applied – such as, and I’m quoting from a paid compression service: “Dictionary compression, a lossless data compression algorithm that uses as dictionary your own JavaScript source code to replace duplicates by a reference to the existing match thus reducing even more its raw byte size.” Depending on the service you use, code can be not even shorter, but run faster too.\n\nObfuscation finally is the process to transform the source code to make it harder to understand. As a funny example, here it’s what alert(\"Hello, JavaScript\") looks like when you express it using [Japanese textual emoticons](http://utf-8.jp/public/aaencode.html?src=alert(%22Hello%2C%20JavaScript%22)\n\nﾟωﾟﾉ= /｀ｍ´）ﾉ ~┻━┻ //*´∇｀*/ \\['_'\\]; o=(ﾟｰﾟ) =_=3; c=(ﾟΘﾟ) =(ﾟｰﾟ)-(ﾟｰﾟ); (ﾟДﾟ) =(ﾟΘﾟ)= (o^_^o)/ (o^_^o);(ﾟДﾟ)={ﾟΘﾟ: '_' ,ﾟωﾟﾉ : ((ﾟωﾟﾉ==3) +'_') \\[ﾟΘﾟ\\] ,ﾟｰﾟﾉ :(ﾟωﾟﾉ+ '_')\\[o^_^o -(ﾟΘﾟ)\\] ,ﾟДﾟﾉ:((ﾟｰﾟ==3) +'_')\\[ﾟｰﾟ\\] }; (ﾟДﾟ) \\[ﾟΘﾟ\\] =((ﾟωﾟﾉ==3) +'_') \\[c^_^o\\];(ﾟДﾟ) \\['c'\\] = ((ﾟДﾟ)+'_') \\[ (ﾟｰﾟ)+(ﾟｰﾟ)-(ﾟΘﾟ) \\];(ﾟДﾟ) \\['o'\\] = ((ﾟДﾟ)+'_') \\[ﾟΘﾟ\\];(ﾟoﾟ)=(ﾟДﾟ) \\['c'\\]+(ﾟДﾟ) \\['o'\\]+(ﾟωﾟﾉ +'_')\\[ﾟΘﾟ\\]+ ((ﾟωﾟﾉ==3) +'_') \\[ﾟｰﾟ\\] + ((ﾟДﾟ) +'_') \\[(ﾟｰﾟ)+(ﾟｰﾟ)\\]+ ((ﾟｰﾟ==3) +'_') \\[ﾟΘﾟ\\]+((ﾟｰﾟ==3) +'_') \\[(ﾟｰﾟ) - (ﾟΘﾟ)\\]+(ﾟДﾟ) \\['c'\\]+((ﾟДﾟ)+'_') \\[(ﾟｰﾟ)+(ﾟｰﾟ)\\]+ (ﾟДﾟ) \\['o'\\]+((ﾟｰﾟ==3) +'_') \\[ﾟΘﾟ\\];(ﾟДﾟ) \\['_'\\] =(o^_^o) \\[ﾟoﾟ\\] \\[ﾟoﾟ\\];(ﾟεﾟ)=((ﾟｰﾟ==3) +'_') \\[ﾟΘﾟ\\]+ (ﾟДﾟ) .ﾟДﾟﾉ+((ﾟДﾟ)+'_') \\[(ﾟｰﾟ) + (ﾟｰﾟ)\\]+((ﾟｰﾟ==3) +'_') \\[o^_^o -ﾟΘﾟ\\]+((ﾟｰﾟ==3) +'_') \\[ﾟΘﾟ\\]+ (ﾟωﾟﾉ +'_') \\[ﾟΘﾟ\\]; (ﾟｰﾟ)+=(ﾟΘﾟ); (ﾟДﾟ)\\[ﾟεﾟ\\]='\\\\\\'; (ﾟДﾟ).ﾟΘﾟﾉ=(ﾟДﾟ+ ﾟｰﾟ)\\[o^_^o -(ﾟΘﾟ)\\];(oﾟｰﾟo)=(ﾟωﾟﾉ +'_')\\[c^_^o\\];(ﾟДﾟ) \\[ﾟoﾟ\\]='\\\\\"';(ﾟДﾟ) \\['_'\\] ( (ﾟДﾟ) \\['_'\\] (ﾟεﾟ+(ﾟДﾟ)\\[ﾟoﾟ\\]+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟｰﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ ((o^_^o) - (ﾟΘﾟ))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (c^_^o)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟｰﾟ)+ ((o^_^o) - (ﾟΘﾟ))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟΘﾟ)+ (c^_^o)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟｰﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟｰﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (o^_^o))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟｰﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟｰﾟ)+ (c^_^o)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟΘﾟ)+ ((o^_^o) - (ﾟΘﾟ))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (ﾟΘﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((o^_^o) - (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ (ﾟｰﾟ)+ (o^_^o)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ ((o^_^o) - (ﾟΘﾟ))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟΘﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (c^_^o)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟΘﾟ)+ ((o^_^o) +(o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)\\[ﾟεﾟ\\]+(ﾟｰﾟ)+ ((o^_^o) - (ﾟΘﾟ))+ (ﾟДﾟ)\\[ﾟεﾟ\\]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟΘﾟ)+ (ﾟДﾟ)\\[ﾟoﾟ\\]) (ﾟΘﾟ)) ('_');\n\nDon’t ask me how it works, but it does – if you don’t believe me, copy &amp; paste that in ESTK and run it. Now imagine the face of somebody opening your JSX and finding there a bunch of Jap emoticons ^_^\n\nServices evaluation\n\nFirst, when testing a service, try to beautify _the _uglified code. You may find out that apparently excellent services such as this one, which compresses some code from a blogpost of mine this way:\n\neval(function(p,a,c,k,e,d){e=function(c){return(c&lt;a?'':e(parseInt(c/a)))+((c=c%a)&gt;35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--){d[e(c)]=k[c]||e(c)}k=[function(e){return d[e]}];e=function(){return'\\\\w+'};c=1};while(c--){if(k[c]){p=p.replace(new RegExp('\\\\b'+e(c)+'\\\\b','g'),k[c])}}return p}('1 a,N,J,g,b;N=6(){m h.O=a.X};J=6(){1 j,4;4=1w;h.j=6(1d){5.1e.1y(\"z A Q\",\"5.1e.1A.1C(1d.O)\");a.r=18;m 18};j=h.j;h.Y=6(){1 M;M=\"1t {      p: 1z {       1W: \\'1Q\\',        1P: [\\'1S\\', \\'1T\\'],       n: \\'K\\',       E: 1N {},       w: 1M {         1i: 19 {n: \\'P\\', u: {1p: \\'P\\'}},          1j: 19 {n: \\'1F\\', u: {1p: \\'1I\\'}}       }     }   }\";4=7 1L(M);4.n=a.U;4.p.E.1K=11;4.p.w.1i.1k=6(){m 4.12()};4.p.w.1j.1k=6(){g.O=1R(4.p.E.n);j(g);4.12()}};h.Z=6(){5.1H();4.1E();4.1G()};h.14=6(){1 B;B=!!5.x.V;9(B){C(g,5.x,a.F)}};h.10=6(W){1 I;I=D(W,a.F);5.x=I}};a={1U:\"1V-1D-1O-1s-1u\",X:1B,1v:1x(\"$$$/1J/26/2r/K=K\"),U:\"z A Q\",F:\"z A Q 2s 2t\",r:11};g=7 N();b=7 J();9(5.2q===2p.2l){b.Y();b.Z()}15{b.14();b.j(g)}9(!a.r){b.10(g)}r?\\'P\\':R;6 D(o,s,f){9(R!=f){o=f(o)}1 d=7 2m;1 l=o.y.u.2n;d.13(5.e(\\'S\\'),s);16(1 i=0;i&lt;l;i++){1 k=o.y.u[i].1X();9(k==\"2o\"||k==\"2u\"||k==\"2v\"||k==\"y\")2C;1 v=o[k];k=5.2D(k);1n(T(v)){2\"2E\":d.2B(k,v);c;2\"2A\":d.13(k,v);c;2\"2w\":d.2x(k,v);c;1b:{9(v 2y 1r){1 8=7 17;8[\"H\"]=e(\"#1f\");8[\"%\"]=e(\"#1c\");d.2z(k,8[v.G],v.2k)}15{1a(7 1g(\"1h G 1q D \"+T(v)))}}}}m d}6 C(o,d,s,f){1 l=d.V;9(l){1 L=5.e(\\'S\\');9(d.24(L)&amp;&amp;(s!=d.1o(L)))m}16(1 i=0;i&lt;l;i++){1 k=d.23(i);1 t=d.1Y(k);q=5.1Z(k);1n(t){2 3.20:o[q]=d.21(k);c;2 3.28:o[q]=d.1o(k);c;2 3.29:o[q]=d.2g(k);c;2 3.2h:{1 8=7 17;8[e(\"#2i\")]=\"H\";8[e(\"#1c\")]=\"%\";8[e(\"#1f\")]=\"H\";1 1l=d.2f(k);1 1m=d.2a(k);o[q]=7 1r(1m,8[1l])}c;2 3.2c:2 3.2d:2 3.2j:2 3.2b:2 3.2e:2 3.22:2 3.27:2 3.25:1b:1a(7 1g(\"1h G 1q C\n// Etc. etc.\n\n…the very same website provides a twin service able to perfectly restore / beautify its own obfuscated code, making the whole obfuscation page… quite pointless – no? Second, always do test your code thoroughly. ExtendScript may seem to work, but in fact it may not.\n\nJavascript2img\n\nA website giving terrific output – but that you cannot really use with ExtendScript, alas – is Javascript2img. Their idea is to transform your code into something that is also a valid PNG image, so a simple alert() gets an extreme makeover:\n\nv58d1dde603be30842b27fd17614124a0=\\[ function(vb4604041b2c258763d25f88597bc71e0){return 'e1c1bfebab6bf67d6a890159995b9edf156ac725d1b4829149132045b0d6b1e6f9c02b68';}, function(vb4604041b2c258763d25f88597bc71e0){return v18db97631c1cc5e4fee9f5e108b11b86.createElement(vb4604041b2c258763d25f88597bc71e0);}, function(vb4604041b2c258763d25f88597bc71e0){return vb4604041b2c258763d25f88597bc71e0\\[0\\].getContext(vb4604041b2c258763d25f88597bc71e0\\[1\\]);}, function(vb4604041b2c258763d25f88597bc71e0){return vb4604041b2c258763d25f88597bc71e0\\[0\\].text=vb4604041b2c258763d25f88597bc71e0\\[1\\];}, function(vb4604041b2c258763d25f88597bc71e0){return null;}, function(vb4604041b2c258763d25f88597bc71e0){'d06b6c54863ac33d12419dd04f7acb85c696f722acdf7b0f35bb848a9f48c54d61c3515d';}, function(vb4604041b2c258763d25f88597bc71e0){return '8d163d76c569f509002af1a489436be60492eb8ef2be9d748f8769c7663a95c6f8055c6d';}, function(vb4604041b2c258763d25f88597bc71e0){vb4604041b2c258763d25f88597bc71e0.style.display='none';return vb4604041b2c258763d25f88597bc71e0;}, function(vb4604041b2c258763d25f88597bc71e0){v9caf9b339af1b9f364bcb2c035c44bbd.onload=vb4604041b2c258763d25f88597bc71e0}, function(vb4604041b2c258763d25f88597bc71e0){v9caf9b339af1b9f364bcb2c035c44bbd.src=vb4604041b2c258763d25f88597bc71e0;}, new Function(\"vb4604041b2c258763d25f88597bc71e0\",\"return unescape(decodeURIComponent(window.atob(vb4604041b2c258763d25f88597bc71e0)))\"), function(vb4604041b2c258763d25f88597bc71e0){vd6f4885c38c918428c6ea3b8b8a687bf=new Function('vb4604041b2c258763d25f88597bc71e0',v58d1dde603be30842b27fd17614124a0\\[10\\](v0a63761d20b234e464ed87e282c4eec3\\[vb4604041b2c258763d25f88597bc71e0\\]));return vd6f4885c38c918428c6ea3b8b8a687bf;}\\]; vce94aec448332eef9b14d81fb54c7458=\\[0,255,0\\]; v0a63761d20b234e464ed87e282c4eec3=\\[ 'cmV0dXJuJTIwJ2NhbnZhcyclM0I=', 'cmV0dXJuJTIwJ25vbmUnJTNC', 'cmV0dXJuJTIwJzJkJyUzQg==', 'cmV0dXJuJTIwJ3NjcmlwdCclM0I=', '', 've64b33e0a0c89a624e9ea3b58c2594de', 'v78bf4435ddbf6b19616fb171ba4230bd', 'cmV0dXJuJTIwJ2RhdGElM0FpbWFnZSUyRnBuZyUzQmJhc2U2NCUyQyclM0I=', '', 'iVBORw0KGgoAAAANSUhEUgAAAAcAAAAHCAIAAABLMMCEAAAAd0lEQVQImQXBvQpBcQCH4dffSb1JJyWTDM5mk1s69+gWbJRJicEmm/iRjzxPp21bYQ9L2cafWYRSywF/uoIpKXiD6h4b8ohdGWpNdlCO8sSRTPCeFJxp6YYe+UIFVzwlg6Q08IJ3uJAx6ctGqz6p5JTMcS0fOCd/vQ8w1VAqjQYAAAAASUVORK5CYII=', 'cmV0dXJuJTIwdjE4ZGI5NzYzMWMxY2M1ZTRmZWU5ZjVlMTA4YjExYjg2LmdldEVsZW1lbnRCeUlkKHZiNDYwNDA0MWIyYzI1ODc2M2QyNWY4ODU5N2JjNzFlMCklM0I=', 'cmV0dXJuJTIwZG9jdW1lbnQ=',\n// Etc. etc.\n\nAnd this is the resulting PNG: \n\n\n\nThe ExtendScript engine can’t really figure the above code out and breaks – I did try also prepending it with all ES5 / ES6 shims (extendscriptr included), so if you want to try yourself to make it work, let me know in the comments if/how you succeed! Thanks. Nothing prevents you, although, to use javascript2img to obfuscate any other JS code within your panel (in case, be aware that some frameworks such as Angular may require some extra care).\n\nJavascript Obfuscator\n\nThis is a paid service, with a free tier – I’ll let you decide whether based on your standards it is cheap or expensive: Javascript Obfuscator goes from $19 up to $79 per month. It has a web interface, but extra options (also from the free tier) are accessible only using their Windows only Gui:\n\n\n\nAbove, all the settings that I’ve been able to use in the free tier. Good news: at least with them, ExtendScript code is perfectly fine and happy, so this is definitely an option. According to them, that security level is low – they provide more advanced features – yet again, it depends on who you think you’re dealing with when protecting your code.\n\nJs Scrambler\n\nThis one offers lot of interesting features, such as Domain Lock and Expiration Date, but it’s definitely pricey. They offer a time limited demo mode, and plans goes from $35 up to $255 per month (minimum payments accepted: three months). The code works in ExtendScript as well, so it’s a good option for your JSX – I’m not in the position to tell you how it compares to Javascript Obfuscator, but to my purposes it ranks OK.\n\nJsxBlind\n\nMade by the very talented InDesign developer Marc Autret, JsxBlind tackles the issue of code protection from a different perspective. It presupposes JsxBin as the source and outputs JsxBin as well, so you need to:\n\n\n  Export to binary the original, plain Jsx using ESTK. You get a JsxBin file (nothing different from what we’re used to).\n  Process the above JsxBin through JsxBlind. You get another JsxBin file.\n\n\nAccording to Marc, if somebody decoded the JsxBlind-ed JsxBin, it would get scrambled variables and function names:\n\n\n\t\n\tImage (c) Marc Autret\n\n\nJsxBlind is provided for free – refer to the original blogpost for all the details. Please note that, depending on your coding style, function names obfuscation (which is optional) may or may not work. Moreover, ExtendScript support varies among Creative Cloud applications: Marc is an InDesign developer so that is his platform of choice, Photoshop seems to work as well, but do test your obfuscated code before shipping.\n\nProtection strategies\n\nI’ve shown here 4 valid alternatives – let me sum up here my own operative and strategic suggestions.\n\n\n  Keep in mind the reason why you’re protecting your code, and balance the resources you’re putting into this process, versus the results you’re getting and your actual needs.\n  JsxBin isn’t dead. The large majority of those who might be involved into the evil business of spying your code don’t have any key to open that lock.\n  Obfuscation methods and providers can be layered: you could use Javascript Obfuscator, then export as JsxBin, then use JsxBlind.\n  Do always test your obfuscated code. ESTK non outputting parsing errors doesn’t mean that your scripts will work as expected. For instance don’t hardwire variable params inside strings, always use String interpolation (I faced this very issue myself calling suspendHistory())\n\n\nPlease let me know your thoughts in the comments below!\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["obfuscation","HTML Panels Tips"],
      id: 26
    });
    

    index.add({
      title: "HTML Panel Tips #21: Photoshop CC 2015.5 survival guide",
      category: ["CEP"],
      content: "Today Photoshop CC 2015.5 has been released even if everybody wanted CC 2016. This will cause you some headaches: let me hand you a painkiller. First thing to know: CC 2015.5 is, for some inscrutable reason, a major version.\n\n\n  CC internal version was 14\n  CC 2014 was 15\n  CC 2015 was 16\n  CC2015.5 is 17.\n\n\nThis means that if you’ve set an upper version in your manifest.xml like:\n\n&lt;HostList&gt;\n  &lt;Host Name=\"PHXS\" Version=\"[14.0, 16.9]\" /&gt;\n  &lt;Host Name=\"PHSP\" Version=\"[14.0, 16.9]\" /&gt;\n&lt;/HostList&gt;\n\nYour panel will just stop working. Myself, I prefer not to set any range, like:\n\n&lt;HostList&gt;\n  &lt;Host Name=\"PHXS\" Version=\"14.0\" /&gt;\n  &lt;Host Name=\"PHSP\" Version=\"14.0\" /&gt;\n&lt;/HostList&gt;\n\nWhich means: from 14.0 onwards. Mind you, the installation paths for extensions are thankfully always the same:\n\n(Win User): C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\Adobe\\CEP\\extensions\\\n(Win System): C:\\Program Files (x86)\\Common Files\\Adobe\\CEP\\extensions\\\n(Mac User): ∼/Library/Application Support/Adobe/CEP/extensions\n(Mac System): /Library/Application Support/Adobe/CEP/extensions/\n\nSecond thing to know: in Photoshop at least, CEP jumped to (major) version 7. Documentation and new features should be available soon. Please note that CEP7 means that for debugging purposes you need to set a new debug flag! On Mac, open the Terminal and enter:\n\ndefaults write com.adobe.CSXS.7 PlayerDebugMode 1\n\nOn Windows run Regedit, browse to: HKEY\\_CURRENT\\_USER/Software/Adobe/CSXS.7 then add a new PlayerDebugMode key of type String with value 1.\n\nThird thing to know: according to this Adobe’s official blogpost, there is now a suggested path for shared, third-party plugins (in case your Panel uses them). What happens when your users migrate to the new CC 2015.5 major version is that, by default and unless they know how to avoid it, old CC 2015 is going to be removed.\n\nSo all your Scripts belonging to /Presets/Scripts and your plugins in /Plug-ins are gone too. The paths for versions persistent plugins (from CC up to current, and hopefully newer versions too) according to Adobe is:\n\n(Win): C:\\Program Files\\Common Files\\Adobe\\Plug-Ins\\CC\n(Mac): /Library/Application Support/Adobe/Plug-Ins/CC\n\nFourth thing to know (hey, I can count!): if you rely on .zxp for installation, you need to download the new version of ExManCmd command-line tool from this link. If/when something new emerges, I will update this post.\n\n\n\nX-Files corner: Photoshop went from version 5 to 5.5, then from CS5 to CS5.5 (actually 5.1 but within a Creative Suite version 5.5), and now from CC 2015 to CC 2015.5. There must be something really weird going on in Adobe with the number five – conspiracy theorists, let me know! 😁\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["CC 2015.5","HTML Panels Tips"],
      id: 27
    });
    

    index.add({
      title: "HTML Panel Tips #20: Javascript Frameworks",
      category: ["CEP"],
      content: "Javascript definitely has a problem: too many frameworks! But as my Color Correction maestro Dan Margulis would put it: “the opposite problem might be much worse”. I’ve tackled the issue of JS frameworks in a dedicated chapter of my Photoshop HTML Panels Development course, but I’d like to add some new thoughts here as well.\n\nThis is an atypical Tip: the topic needs more than a quick coverage so bear with me and read along. Also, note that I’m not, strictly speaking, a front-end JS expert: I know how to put together some stuff, but I’m not really into Webpack, Babel, EcmaScript 2015, etc. so I’ve a pragmatic approach. Coming from Flash Panels and the Olav Kvern school of blogposts, I’ve been using MVC patterns in ActionScript; like many others, I’ve found myself stuck in a dead end when wind changed and turned to HTML. As Matthias Petter Johansson predicts in an entertaining video appropriately called Too many tools and frameworks I chose my framework based on the combination of Popularity (how much it is widespread and adopted) and Authority (who’s build it) – you can’t fail this way, can you? So I went with Google AngularJS.\n\nAngularJS\n\nBeing a self-taught JS developer, the framework learning curve was quite steep: so I bought courses, books etc. and started climbing mount Angular. I’ve used it in some of my commercial products and, as I’ve mentioned, I’ve also built a dedicated demo panel in AngularJS for my Photoshop HTML Panels Development course. Per se, that Panel is not rocket science: it scans your PS open document and seeks for Text layers, retrieving the text content and letting you update it. It features three tabs (just a pretext to use routing) and looks like that:\n\n\n\nYet I’ve built it in AngularJS trying to use all the best practices I knew: separation of concerns, modularity, you name it. I ended up with a code by all means exaggerated for that little panel, but which architecture can scale and support much more complex and elaborated projects. You can have a look at the dependencies schema used in this image.\n\n\n\nIf you’re familiar with AngularJS you can understand what’s in there – I won’t get through it in this very post: the full Panel’s code, with detailed explanations, is available for download alongside in a 28 custom made panels bundle with the Panels course – you can borrow it and use it in your own projects if you want.\n\nNow, after having written that chapter, I ended up using the very same architecture in an actual, commercial product: a contract job for a Panel that looks apparently simple (but it’s not 😶). Compared to other jobs I did in the past, this one features a great amount of modularity: possibly it wasn’t totally needed when I build it, but being a contract job I’ve preferred to overdo when coding it, and then ensure hopefully easier updates in the future. As a compulsive new technologies addicted, I have now to face the advent of AngularJS v2, which scrambles all the rules of v1 and is an entirely different beast. Mind you, I went with Angular from the beginning mostly because I like learning new stuff, and for knowing a framework such as AngularJS might be useful for my career apart from HTML Panels (wishful thinking).\n\nMind you, I can stay with v1 for a long long time! Stuff evolves so rapidly in JS land that by the time Angular v1 is ultimately unsupported, new players will have hit the field: so basically… why bother now (besides, Adobe isn’t the fastest company in updating SDK). Also, I’m not a full-time developer and another steep learning curve will eat time that (a) I don’t have, and (b) that I can spend more profitably, i.e. writing another book. Angular v2 is better experienced using TypeScript – I can barely stand CoffeeScript as a language to compile to JS, I’m sorry but another strongly typed language is not in my plans. So… I’ve taken the chance to get accustomed to a valid alternative (at least for HTML Panels) that I’d like to suggest you.\n\nVueJS\n\nLet’s start with downsides: it isn’t as widespread as AngularJS (note by my future self from 2019: it really is now!!) or the other framework that everybody’s excited about (React – which frankly I don’t know, but the article “Your Timeline for Learning React“ is enough to make me skip it altogether – if you really think I should get into React instead, please let me know in the comments the reason), nonetheless Vue.js has some respectable 20K stars on Github. Another concern may be that Vue.js is a personal project: yes, it’s mainly built by Evan You, not an enterprise backed developers team. If you can live with that… Vue.js has:\n\n\n  an amazing community: apparently it’s very popular among Laravel developers\n  very good documentation: IMHO uber-important factor when choosing a framework\n  reactive data binding: rendered templates are always up-to-date\n  components\n  routing\n\n\nand much more, that I won’t cover here ’cause frankly I can’t understand it yet – I’ve just started approaching the framework… How sad is truth, isn’t it?! Vue.js implements a MVVM (Model - View - ViewModel) pattern, which is considered a good choice for user interfaces (like… Panels!).\n\nOne of the most understandable articles on MVVM is by Addy Osmani, and I don’t know why, this doesn’t surprise me. Let’s have a look at a dead simple example: In this case, the View is the templating in the HTML, the Model is the model object (a regular, plain JS object) and the ViewModel is the Vue instance that binds the View (here the \"app\" element – think about it as the Vue’s version of Angular ng-app), and binds the Model.\n\nReactivity means that there’s no need to update the view if/when the model changes – as soon as model is updated, the view will reflect the new value. This leads to two-way data binding, such as: Here you see v-model (equivalent to Angular ng-model) and the use of a JSON filter with $data. Refactoring the same example with modular ViewModel (usually called just vm) you can see that it’s really the data that drives everything, so if you need two Vue instances – and their parent – to share the same model, just share it by identity: In the above example, the globalModel var contains reactive data that is used by both vmRadius and vmOpacity. Extending further the idea, each vm can bind model data that is both shared and private, like: If you need it, you can always use a PubSub pattern with $on, $emit and $broadcast, similarly to Angular.\n\nVue.js also has all the directive that you’re used to in Angular, so v-if, v-show, v-for, v-on, etc. I haven’t talked about methods (i.e. functions that you define in the vm), but I’m not really writing a Vue tutorial here – the official documentation is remarkably good and there’s also this videoseries (mostly free) by the always great Jeffrey Way – let me just briefly mention components: Everything I’ve shown here is really basic, there’s so much more to discover in Vue.js! Hopefully I’ve raised to your attention what seems to me a valuable alternative to more known and opinionated frameworks that, in the context of HTML Panels, for some among us are either overkill and/or difficult to master and maintain.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","Angular","Vue.js"],
      id: 28
    });
    

    index.add({
      title: "The Adobe Photoshop HTML Panels Development Course is available!",
      category: ["CEP"],
      content: "I’m super excited to officially announce that the Adobe Photoshop HTML Panels Development course – Build and Market Adobe Creative Cloud extensions – is finally available! Visit the project website for all the information and the special launch price offer, or read along for a brief summary.\n\nGet to the Photoshop HTML Panels Development course website!\n\nIt took me seven months but I eventually made it! Here’s what you’ll find.\n\nThe Book\n\n285 pages 300 pages PDF, 15 Chapters of solutions to real-world development problems. It starts easy (I mean, from the very basics!) then gradually gets to advanced topics such as Node.js modules, REST services integration, Adobe Generator servers, WebSockets, etc. Mobi and ePub formats are available as well.\n\n\n\n28 Panels\n\nEach book’s Chapter covers a topic, then 28 custom-made Panels (with fully commented code) dig deeper, implementing the basic concepts shown in a more advanced fashion.\n\n\n\nThe Video series\n\nI’ve recorded three hours of HD screencasts dealing with some of the aspects covered in the book, adding extra information and new sections on specific subjects (such as ActionManager code - aka Scripting Voodoo! - that the book just briefly mentions).\n\n\n\nThere’s much more to know about it, so please head to the Course website and you’ll be able to get some sample content (an 40 pages excerpt from the book and half of an hour of HD videos… for free!).\n\nGet to the Photoshop HTML Panels Development course website!\n\nMind you, I’m running a special offer through April 2016 so if you want to take advantage of the discounted launch price hurry up!\n",
      tags: ["Adobe Photoshop HTML Panels Development"],
      id: 29
    });
    

    index.add({
      title: "HTML Panels book: Website launched!",
      category: ["CEP"],
      content: "Panels people! From now you can subscribe to http://htmlpanelsbook.com and book (pun intended!) your discount code for the “Adobe Photoshop HTML Panels” thing, which is going to be released by the end of March.\n\nBecause we all love to publicly disclose deadlines, don’t we? Subscribing to the newsletter is a way for you to be updated on the book’s state, get some preview pages, and receive a substantial discount few days in advance of the official announcement.\n\n\n\nSo, what am I up at right now? The book itself is more or less done, I’ve to polish here and there, add selective code highlighting (Leanpub has released this new feature few days ago and I’m eager to try it) and similar minutiæ.\n\nThe next big thing I’m tackling is… videotutorials! Which I plan to optionally bundle alongside with the ebook: it’s going to be a bloodbath. But worth the effort! Please subscribe, so that I can get in touch with you more easily (no spam, no tricks). Mind you, it seems the newsletter has troubles with @mac.com, @icloud.com and @me.com domains (i.e. all Apple) - preferably  use a different email account of yours.\n\nThank you and have some nice time!\n",
      tags: ["Adobe Photoshop HTML Panels Development"],
      id: 30
    });
    

    index.add({
      title: "HTML Panels book Update (Dec 2015)",
      category: ["CEP"],
      content: "Hello readers, I’m happy to inform you that the “Adobe Photoshop HTML Panels Development” book is in good shape! I’ve kept writing, even if my life by the end of the year has got pretty intense.\n\nThese days, lot of paper work for the annual taxes, lunches and dinners with family and friends, besides my postproduction and coding jobs that will run at full speed until mid-January, when I’ll hopefully have some extra spare time to breathe and write at a more constant pace.\n\nAnyway, I’d like to share with you the book’s Index as it is now; most of the content is already in place, even if stuff is not yet final. I’ve built several demo panels (about 17 of them, and counting), which code will be available alongside with the book. TODO sections have not been written yet; I’m currently finishing chapter 13 - the book is about 190 pages so far, with code excerpts and some images.\n\nBook Index\n\n\n  Introduction Why I wrote this book Audience and Assumptions What this book is What this book is not What you need to get started Version History Feedback Support Errata Piracy\n  Photoshop Extensibility Overview Plug-ins (PhotoshopSDK) Scripting Flash Panels HTML Panels\n  The HTML Panel Stack CEP: Common Extensibility Platform CEF: Chromium Embedded Framework HTML and CSS Javascript Extendscript Layers communication\n  Setting up the environment Code editors Debug Flag Installation folders Download the Libraries Storing projects\n  Building up “HelloWorld!” Take1: Manifest.xml Take2: ExtendScript Take3: Debugging Hello World wrap-up\n  CSInterface JS interface CEP Versions evalScript() getSystemPath() and including multiple JSX\n  Exchanging data between Panel and Host Application Passing primitive values from JS to JSX Passing Objects from JS to JSX Passing Objects from JSX to JS Demo Panel: Actions Demo Panel: Data exchange\n  Events No shortage of Events in town Host Application Events - ExtendScript Events in Photoshop Host Application Events - CEP Application Events Custom ExtendScript Events Custom CEP Events CEP Panel’s Events Demo Panel: Photoshop ExtendScript Events Demo Panel: Photoshop CEP Events (LoseFocus) Demo Panel: Photoshop Custom ExtendScript Events Demo Panel: CEP Custom Events\n  Styling Matching the Host Application UI look Synch with Photoshop Theme changes Flyout and Contextual menus Icons, Size and Retina Displays Demo Panel: Flyout and Contextual menus Demo Panel: High-PPI display\n  Node.js Two of a kind Importing modules Issues and Workarounds Demo Panel: connecting to a REST service Demo Panel: modularize JSX Events management\n  Store and retrieve data locally Persistence Using the Filesystem Web Storage Indexed DB What should I use? Demo Panel: Persistence Demo Panel: Node fs and nconf modules Demo Panel: Node.js presets module\n  Javascript Frameworks What is the best JS framework for HTML Panels? Newbies corner Demo Panel: AngularJS\n  Communicate with the WWW (work in progress) Connect with the internet Download and open a file Upload a file Download and Upload with Node.js Local Node.js servers Web Sockets (TODO) Generator integration (TODO) Demo Panel: Flickr panel Demo Panel: Upload panel (XMLHttpRequest) Demo Panel: Master/Slave Demo Panel: Web Sockets (TODO) Demo Panel: Generator (TODO)\n  Miscellanea (TODO)\n  Packaging and Deploying(TODO)\n  Extensions Marketing (TODO)\n  Appendices (TODO)\n  Acknowledgements (TODO)\n  Copyright (TODO)\n\n\nOriginally, I wanted to record a lengthy videotutorial, to be bundled with the book (as an optional purchase). I’m afraid it doesn’t fit in my time schedule now, so I’ll be publishing the book first, then produce the video series later in 2016 (which will be sold separately, at a discounted price for the ebook owners).\n\nI take the chance to thank each and everyone among friends and developers (most of the time the two overlap) who’s been helping me so far - I wish all the best to you and your families, see you in 2016. Ciao!\n",
      tags: ["Adobe Photoshop HTML Panels Development"],
      id: 31
    });
    

    index.add({
      title: "HTML Panel Tips #19: CC2015.1 (CEP6.1) Node.js Fixes",
      category: ["CEP"],
      content: "The latest release of Photoshop introduces, among the rest, a new version of the Common Extensibility Platform, CEP6.1. In turn, CEP6.1 is a major break in backward compatibility due to the way it manages Node.js. If you have an extension using Node.js, chances are that it’s now broken in Photoshop CC2015.1. Read along to know why, and how to fix it.\n\n[UPDATE] With the release of Photoshop CC2015.1.2, the unified context has been brought back! Read the relevant section below to find out how to enable it.\n\nNode.js is now io.js\n\nAdobe has switched to the io.js branch. If you didn’t know, some Node maintainers forked the mainline and started working on io.js - this happened quite a while ago: in fact now the branches have merged back, and the versioning has changed too: Node is not anymore 0.12 or the like - we count like 5.1 (current version when I write this). Why did Adobe picked the older io.js is a question that yours truly cannot address.\n\nNode.js is disabled by default\n\nDue to security concerns, Node.js was by default enabled in CEP6.0 and prior versions - it’s now disabled, unless you add in the manifest.xml, in the &lt;resources&gt; tag the following:\n\n&lt;CEFCommandLine&gt;\n  &lt;Parameter&gt;--enable-nodejs&lt;/Parameter&gt;\n&lt;/CEFCommandLine&gt;\n\nAlso, you need to explicitly enable Node in iframes too:\n\n&lt;iframe id=\"someID\" class=\"someClass\" enable-nodejs&gt;\n\nNode.js and Browser contexts are detached\n\n[UPDATE] The unified context is back in the PS CC2015.1.2 update, but it’s disabled by default. In order to enable it, put this one in the manifest.xml:\n\n&lt;CEFCommandLine&gt;\n  &lt;Parameter&gt;--enable-nodejs&lt;/Parameter&gt;\n  &lt;Parameter&gt;--mixed-context&lt;/Parameter&gt;\n&lt;/CEFCommandLine&gt;\n\nWhat follows below still applies for previous version of Photoshop.\n\nDue to the io.js move, Node.js and the Browser context don’t talk to each other anymore. This is a big pain in the ass, and, according to Adobe’s engineers, a unified context will be back in CEP7 (that should be released, if they keep the same pace, alongside with Photoshop CC2016, so many months from now). What does the above mean? If you have a Node.js module and try to access jQuery $ object, or csInterface, or anything else defined in the Browser context (e.g. your main.js file), it will fail! So much fun. Workaround: use the global window object as a bridge:\n\n// ===========\n// in main.js\n// ===========\nvar csInterface = new CSInterface();\nwindow.csInterface = csInterface;      \nwindow.$ = $;\n// etc.\n\n// =====================\n// in your node.js files\n// =====================\nvar $ = window.$;\nvar document = window.document;\nvar csInterface = window.csInterface;\n// etc.\n\nChrome only for debugging\n\nNo way to use Safari anymore, only the Chrome Developer Tools.\n\nThat’s all folks\n\nWhen/if I find something else pertinent to the CEP6.1 switch, I’ll update this post. If you do, please add in the comments below! Thanks.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","Node.js"],
      id: 32
    });
    

    index.add({
      title: "Im writing the book 'Photoshop HTML Panels development'!",
      category: ["CEP"],
      content: "Hello readers, I’m happy to publicly announce that I’m writing a book (temporarily) titled:\n\nPhotoshop HTML Panels developmentBuild and Market Adobe Creative Cloud extensions \n\nI’ve let the news leak here and there as a way to strengthen my commitment in the project, and I’m ready now to challenge bad luck and tell the world about this ambitious venture. My goal now is to give you some details about the book, so read along.\n\nBook Content\n\nI’ve been blog posting about Panels for some years now, and following their evolution ever since Photoshop introduced them, back in the Flash era - exchanging information with other users, reporting bugs to Adobe’s engineers, posting to Forums, building commercial products and open source stuff. I’ve gathered all my code and notes and tried to give that a proper shape - different from, dare I to say, everything that’s been published so far about the subject.\n\n\n\nThe book is structured so that both beginners and advanced users can profit from it - building skills from the ground up, yet exploring annoyingly entangled topics in detail and providing code examples and several demo panels. It’s not going to be “HTML Extensions Cookbook: 42 ready-made Panels to borrow code from”, nor “Learn Photoshop Panels in 12 days and 4 full projects” though!\n\nEach chapter deals and tries to solve a particular problem that you, as a Panel developer, are likely to face sooner or later - from Node.js to Events, Styling, interaction with the WWW, etc. etc. - including some advanced topics such as Generator or Socket.io. I know that stuff I’ve been writing about is “a problem that you’re likely to have” because many people who find themselves stuck with a particular coding issue related to Panels (a frequent fact of life that everybody who’s ever been exposed to Adobe APIs has a good knowledge of), to my great surprise have been starting to reach out for my help.\n\nAnd to a possibly even greater surprise of mine, it seems like I’ve been (mostly) successful in providing them with guidance, reference code, or simply my unopinionated point of view. Mind you: even if there is some overlap with what I’ve published in this blog over time, I’d say that mostly the book contains either brand new or heavily reshaped content and code - even the sheer amount of information overwhelms what blogposts can show. \n\nPlus, I’ll be adding sections about the extensions marketing - with insider advices (based on my personal experience as a vendor) for those willing to run a successful and self-sustaining business on top of panels. The book mentions Photoshop because this is the software I’ve been able to feed my family thanks to, in the last 16 years - but I’d say about 80% of its content (with possibly the exception of some specific JSX Events) applies to all the Creative Cloud Host Applications that support HTML Panels (InDesign, Illustrator, etc.) If the above sounds ambitious and possibly (just a tad) loud, well… I admit it might be - but rest assured that I’ve poured into this book lots of sweating, sharing everything I know and especially what I didn’t know (but I’ve learned in the process of writing!), without the fear to admit when I’ve not been able to sort stuff. I’d like to thank also to each and every one who’s been so kind to help me with suggestions, code chunks and support so far.\n\nRelease date\n\nToo early to disclose, I’ll keep you posted about the book status here: anyway, I’d say not before Q1/2016. I’m at page 113, which happens to be the Italian equivalent of 911, whatever this may ever mean, and I still have some pretty dense stuff to deal with. Also, I have a special idea that… :-)\n\nPublisher\n\nThe book is going to be released by Leanpub, a Canadian company, as an ebook. I’ve had an aha moment reading Azat Mardan ProgWriter, where the author (a “standard” software engineer) tells about how he’s become a technical writer (that is: a ProgWriter). I’ve thought that HTML Panels are such a small niche that nobody in the traditional publishing would ever be interested in accepting a similar book project - nor the timing would have been appropriate, for such a fast pace evolving environment - so I recklessly went on with self publishing. Finger crossed!\n\nCool fact about Leanpub is that you write in their own Markdown flavour and they compile PDF + Mobi + ePub (I guess using a LaTex intermediate for PDF). In other words, you can focus on content only (which is well enough) and not worry about InDesign or the like - in my opinion the compromise between customization and ease of use is perfectly OK.\n\nPlease comment!\n\nI’d like to hear about you - please let me know in the comments your needs, opinion and suggestions, or whatever is Panels related. Back to the desk now, ciao!\n",
      tags: ["Adobe Photoshop HTML Panels Development"],
      id: 33
    });
    

    index.add({
      title: "De-uglify ExtendScript Toolkit on Retina Displays",
      category: ["Scripting"],
      content: "I’m no big fan of ESTK for a variety of reasons; when I finally replaced my old MacBookPro with a newer model with Retina display I got even more disappointed: pixels everywhere! There’s a quick fix/hack that I’d like to show you (also as a reminder for my future self, just in case ESTK is still going to be around in 2021 when I’ll get a newer Mac). You may want to right-click the app, select Info and uncheck the “Open in Low Resolution” checkbox, alas it is grayed out. Don’t worry.\n\n\n  Grab the ExtendScript Toolkit.app, right click it and select Show Package content\n  Go to Content &gt; Info.plist and open it with the text editor of your choice.\n  Go to the line before the closing &lt;/dict&gt; tag and add the following two highlighted lines:\n\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n&lt;dict&gt;\n    &lt;!\\-\\- LOT OF STUFF HERE... --&gt;\n    &lt;key&gt;NSHighResolutionCapable&lt;/key&gt;\n    &lt;true/&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\n\n\n  Save and close the Info.plist file, then open the Terminal and:\n\n\ntouch /Applications/Adobe\\ ExtendScript\\ Toolkit\\ CC/ExtendScript\\ Toolkit.app\n\n(hint: type touch  and then drag the ExtendScript ToolKit.app in the Terminal window to save you typing the rest - we’re all born lazy). Now voilà, you open ESTK: text is crisp and clear and confetti fall from above. Cheers!\n",
      tags: ["ESTK"],
      id: 34
    });
    

    index.add({
      title: "HTML Panel Tips #18: Photoshop JSON Callback",
      category: ["CEP"],
      content: "CC 2015 previews a new Photoshop Event listening system and deprecates the \"com.adobe.PhotoshopCallback\", due to a bug that makes all the extension receiving the event - that is: if two panels are registered for the event “make”, then each panel sees the other panel’s “make” event, and must ignore it. The solution implemented marks a break in retro-compatibility and is going to be the only one accepted from Photoshop CC2016 (version 17.x) onwards. Let’s have a look at what is that all about.\n\nPlease refer to HTML Panels Tips: #7 Photoshop Events, Take 1 to review how PS events listening works. Now, there are three different changes you must be aware of:\n\n\n  \"com.adobe.PhotoshopJSONCallback\" is the replacement event - no more \"com.adobe.PhotoshopCallback\".\n  The ExtensionID must be appended as a suffix.\n\n\nSo what used to be:\n\nvar csInterface = new CSInterface();\ncsInterface.addEventListener(\"PhotoshopCallback\", PSCallback);\n\nnow becomes:\n\nvar csInterface = new CSInterface();\nvar extensionId =  csInterface.getExtensionID();\ncsInterface.addEventListener(\"com.adobe.PhotoshopJSONCallback\" + extensionId, PhotoshopCallbackUnique);\n\n\n  The data property of the CSEvent that the PhotoshopCallbackUnique receives is a JSON string hidden inside a string.\n\n\nfunction PhotoshopCallbackUnique(csEvent) {\n  console.log(csEvent);\n}\n// ver1, {\n//   \"eventID\": 1298866208,\n//   \"eventData\": {\n//     \"documentID\": 1566,\n//     \"new\": {\n//       \"_obj\": \"document\",\n//       \"depth\": 8,\n//       \"fill\": {\n//         {\n//           \"_enum\": \"fill\",\n//           \"_value\": \"white\"\n//         },\n//         \"height\": {\n//           \"_unit\": \"distanceUnit\",\n//           \"_value\": 360\n//         },\n//         \"mode\": {\n//           \"_class\": \"RGBColorMode\"\n//         },\n//         \"pixelScaleFactor\": 1,\n//         \"profile\": \"sRGB IEC61966-2.1\",\n//         \"resolution\": {\n//           \"_unit\": \"densityUnit\",\n//           \"_value\": 300\n//         },\n//         \"width\": {\n//           \"_unit\": \"distanceUnit\",\n//           \"_value\": 504\n//         }\n//       }\n//     }\n//   }\n\nAs you see, there’s a ver1, that needs to be stripped. Also, as a bonus, extra information is provided as JSON (here a New Document’s been created). I’ve uploaded a demo extension on my GitHub repo, which looks like:\n\n\n\t\n\n\nWhich basically let you switch on/off listeners to the corresponding events (\"make\", \"duplicate\", etc are the stringID). You can find the full code on GitHub, yet the relevant files are:\n\nHTML\n\nI’ve used Topcoat CSS (apparently not anymore maintained, but I love them), nothing too fancy.\n\n400: Invalid request\n\n\nJS\n\nThe init function starts the themeManager and then activate persistence (if you need a refresh on the topic, have a look at this older HTML Panel Tip). Be careful with persistence, since if it’s active it will constantly cache the extension in the Chrome Dev Tools (i.e. you change code, reboot the panel and changes aren’t loaded - you need to reboot Photoshop. This happens on OSX at least; knowing it will save some head scratching time). Then you add a listener for the PhotoshopJSONCallback plus extensionID (line 24).\n\nOn line 26 PhotoshopCallbackUnique strips the \"ver1\" string in the event.data, then parse the JSON and puts it back as an Object. Line 44, toggleEventRegistering: this is the unique switches onChange callback, which duty is to dispatch the com.adobe.PhotoshopRegisterEvent or com.adobe.PhotoshopUnRegisterEvent, corresponding to the on-the-fly-calculated typeID of the provided stringID.\n\n400: Invalid request\n\n\nI hope this help clarifying how things work now. I repeat, both PhotoshopCallback and PhotoshopJSONCallback are usable in Photoshop CC2015 (the first is “just” deprecated), while from CC2016 onwards only the latter will work. I’d like to thanks Tom Ruark who pointed me to the Adobe’s original code, that I’ve refactored a bit for this blogpost. Also, I take the chance to tell you I’m working at a more comprehensive learning resource about Photoshop HTML Panels (ebook + video series), so stay tuned here.\nCiao!\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","PhotoshopCallback","PhotoshopJSONCallback"],
      id: 35
    });
    

    index.add({
      title: "In memory of Mike Hale and PS-Scripts.com",
      category: ["Scripting"],
      content: "As you might have noticed, PS-Scripts – the independent forum devoted to Photoshop scripting - is offline since May 2015 due to the passing of the domain owner and maintainer, the very missed Mike Hale.\n\nPeople (like me) who have been spending countless hours there looking for, and finding, valuable information, posting and chatting with other experienced developers, have been deeply touched by the loss of Mike. And, I’d say, by the sudden disappearance of a decade old forum. Bits of information can be found in this Adobe Photoshop Scripting official forum thread, but I’ve always liked to tell the whole story (as I know it) here. It’s not a timely post, but somehow I feel I had to write it anyway.\n\nAs a regular visitor of PS-Scripts, I noticed in 2014 a lack of posts from Mike: I checked his user information and saw that he kept logging, but his contributions were just fading off. At first I thought that he could have just lost interest: in the recent past, other great developers who wrote fundamental chapters of the history of Scripting such as Paul Riggott and xbytor announced to be definitely fed up with Adobe’s lack of commitment in fixing bugs and evolving the platform, and quit. Logs from Mike definitely stopped in late 2014, and around May 2015 a placeholder appeared instead of the Forum’s landing page. Slightly nervous, I asked for information in the official forum and started digging the internet looking for Mike’s traces: I’ve been in touch with him few times over the years, but couldn’t find his email anywhere - most of our chats happened as private messages on PS-Scripts. Apparently he took privacy in high regard - very little can be found about him online - and as a introvert I’m not the kind of person who emails from time to time just to keep in touch.\n\nGoogling for him, one night I eventually run into his obituary, which left no doubts since he’s portrayed this way:\n\n\n  […] he then went on to pursue a career in the photography lab industry. He was a contributor to ADOBE Photoshop and led an international photo scripting forum. His passion in life was restoring old damaged photos and spoiling his nieces, nephews, grandnieces and nephews. Michael L. Hale was a kind, compassionate, caring gentleman who is admired and respected by many and will be greatly missed by all.\n\n\nSo Mike passed away in September 2014 and the Forum’s placeholder page was just GoDaddy’s way to tell the world the domain was about to be vacant. Saddened by the news I got in touch with some of the people I know in the business, and we started discussing a viable strategy to get back online the invaluable 10 years worth PS-Scripts archive.\n\nAmong those who kindly offered their help, either logistic and/or financial, xbytor, Chuck Uebele, Jeffrey Tranberry, Tom Ruark, Pete Green, Sandra Voelker and others. xbytor found the PS-Script snapshot on archive.org (sort of the internet time capsule), which is available for consultation yet not searchable (as far as I know) and offered to reach out for Mike’s relatives in order to retrieve his own backups. xbytor was one of the PS-Scripts administrators, and has given a substantial contribution over the years to the forum. He also tried to talk with GoDaddy support, as a plan B. Both plans haven’t been as successful as we hoped: no backup from the Hales, and lot of bureaucracy just to take over the domain.\n\nI volunteered to try again with GoDaddy, and found additional information: while the domain was somehow pre-paid and still reserved to Mike Hale until mid-2016, the hosting expired and - given a grace period of few months - GoDaddy eventually deleted all the servers content by the end of May 2015 - we arrived too late. Being PS-Scripts based on phpBB (a very common PHP forum platform), it’s not possible as far as I’ve been told by PHP programmers, to restore it unless you have a proper backup (that is, some sort of mySQL database + phpBB data), such as the one from Mike - who by the way would have been dated September 2014 at best. Conversely, Archive.org snapshot (dated May 2015, so 8 months more up to date), is just a plain, flat version: unsuitable for posting, logging, searching, etc.\n\nSo, paradoxical as it seems in the era of digital permanence, 10 years of shared knowledge, efforts and care have more or less vanished in a snap. All those who’ve been in touch with Mike describe him as a helpful and kind person, and I could not agree more. He was not only knowledgeable, but also willing to share, and as a maintainer of the forum he has demonstrated with facts his own commitment in a field in which frustrations, dare I to say, are far more common than satisfactions. Trying to preserve his, and others’ work, proved to be hard too.\n\nDownloading Archive.org entire websites is explicitely forbidden by their policies, yet somebody I’m, ehm, close to paid a small sum to a company which does retrieve sites for offline viewing - it’s not a complete dump (about 12K files, lot of which are useless duplicates of the login page and the like), to view it you need to set up a local server on your machine (with MAMP or Ampps – copy also the .htaccess file), otherwise just search for a topic on Finder and use that folder as a source.  You can also try yourself with apps such as Site Sucker, but the output might be very large - I’ve not been able to restrict to the latest Archive.org snapshot (the one from May 2014), if you know how to do it write in the comments below.\n\nChuck Uebele, who is an Adobe Community Manager, volunteered to set up a dedicated section in the Adobe Photoshop Scripting forum, to collect relevant threads. Sandra Voelker offered the photoshop-scripting.com domain she owns. In order to avoid the issue of linking too tightly a website to somebody, I suggested GitHub (which by the way can host for free static websites) and started this experimental, early alpha PS-Scripting Cookbook – which is based on GitHub Pages (plain text Markdown files, “compiled” on the fly with Jekyll, a Liquid Templates based Generator), linking Gists (version managed snippets).\n\n\n\nMike Hale\n\n\nA mixed approach might be the right one, in which code is available, version managed, and discussed too - the greater plus that PS-Scripts offered was the interaction between contributors, which usually led to new and interesting solutions. Needless to say, contributions are needed and more than welcome so please join in - this community needs you too. I’d like to thank everybody who’s spent his own time trying to build first, then recover the PS-Scripts community: a place I personally and professionaly owe a lot to.\n\nSo long Mike, and thanks for all the fish.\n",
      tags: ["Mike Hale","ps-scripts.com"],
      id: 36
    });
    

    index.add({
      title: "Partial Serial Number Verification System in Javascript",
      category: ["Scripting"],
      content: "Back in 2007, developer Brandon Staggs wrote a brilliant article about software licensing, showing how to implement what he calls a Partial Serial Number Verification System using the Delphi language. Apparently the remarkable technique first appeared around 2003 in a set of slides by Chris Thornton, developer of the ClipMate software. Years have passed, cryptography is more affordable, yet I would say that this approach to the Software Licensing problem is still valid in some businesses - besides the fact it’s fascinatingly clever. In a nutshell, Staggs shows how to build a serial number (seed, keys, checksum), which keys verification in the final product is partial - so that a cracker would hardly be able to build a long lasting keygen. I’ve ported the code to Javascript (originally written in Delphi). Two caveats:\n\n\n  My understanding of Delphi is very limited: in fact I’ve never seen any Delphi code before, the port is entirely based on searching the Language Reference and a bit of common sense.\n  As a translation to a different language, mine is a pretty literal one. I’ve also avoided ES5 / ES6 features on purpose: I wanted the code to run on the Photoshop ExtendScript interpreter (which is very unfortunately stuck to ES3).\n\n\nAs follows, Brandon Staggs introduction quoted with his permission from the original article - which I highly recommend you to read since he discusses the concept and details his code. Then you can find my Javascript version (frankly, I thought it would have been slightly easier to write). Eventually, a link section with some reference URLs that I visited while studying the code and that might help you as well.\n\n\n  Brandon Staggs original Introduction\n\n  Most micro-ISVs [Independent Software Vendors] use a serial number/registration code system to allow end users to unlock or activate their purchase.  The problem most of us have run into is that a few days or weeks after our software is released, someone has developed a keygen, a crack, or has leaked a serial number across the internet. There are several possible solutions to this problem. You could license a system like Armadillo/Software Passport or ASProtect, or you could distribute a separate full version as a download for your paying customers. Each option has advantages and disadvantages. What I am going to show you is a way to keep “rolling your own” license key system while making working cracks harder for crackers to produce, and working keygens a thing of the past. Aside: If you think it’s crazy to post this publicly where crackers can see it, don’t worry about that. I’m not posting anything they haven’t seen before. The entire point of partial key verification is that your code never includes enough information to reverse engineer a key generation algorithm. Also, I offer no warranty of any kind — this is for your information only! Now, on with things. Our license key system must meet some basic requirements.\n\n  \n    License keys must be easy enough to type in.\n    We must be able to blacklist (revoke) a license key in the case of chargebacks or purchases with stolen credit cards.\n    No “phoning home” to test keys.  Although this practice is becoming more and more prevalent, I still do not appreciate it as a user, so will not ask my users to put up with it.\n    It should not be possible for a cracker to disassemble our released application and produce a working “keygen” from it. This means that our application will not fully test a key for verification. Only some of the key is to be tested. Further, each release of the application should test a different portion of the key, so that a phony key based on an earlier release will not work on a later release of our software.\n    Important: it should not be possible for a legitimate user to accidentally type in an invalid key that will appear to work but fail on a future version due to a typographical error.\n  \n\n  The solution is called a Partial Key Verification System because your software never tests the full key. Since your application does not include the code to test every portion of the key, it is impossible for a cracker to build a working valid key generator just by disassembling your executable code. This system is not a way to prevent cracks entirely. It will still be possible for a cracker to edit your executable to jump over verification code. But such cracks only work on one specific release, and I’ll suggest a couple of tricks to make their job harder to complete successfully. […] [Quoted with permission]\n\n\nJavascript port of the original Delphi code\n\n400: Invalid request\n\n\nReference Links\n\n\n  Brandon Staggs: Implementing Partial Serial Number Verification System.\n  Patrick McKenzie: Everything You Need To Know About Registration Systems.\n  Allan Odgaard [TextMate developer]: OpenSSL for License Keys.\n  Chris Thornton: Keygens, Protection, Encryption Panel Software Protection Methods slides.\n  The Business of Software community.\n  GitHub’s JS encryption-related repos: crypto-js, forge, jsencrypt (RSA), base32-js.\n  Youtube: RSA Encryption Algorithm.\n  Dan Vanderkam: Arbitrary precision Hex &lt;-&gt; Dec converter.\n  MDN: Bitwise Operators.\n  Delphi Basics Reference Language.\n  JS Minifier (use “Conservative” then strip newlines - aggressive minifiers will break ExtendScript code, see this article)\n\n\nHope this helps! Thanks for reading and if you feel dandy there’s always that yellow “Donate” button in the top-left corner :-)\n\nCredits\n\nI would like to thank Brandon Staggs for the kind permission of quoting his original article - pay a visit to his blog and the StudyLamp Software LLC website.\n",
      tags: ["Licensing","Partial Serial Number Verification"],
      id: 37
    });
    

    index.add({
      title: "HTML Panel Tips #17: CC2015 Survival Guide",
      category: ["CEP"],
      content: "Superstitious people in Italy think that seventeen is a bad luck number - yet I’m not gullible and I think it must be by chance that this Tip #17 is about the apparently, ehm, troublesome update to Creative Cloud 2015. As follows is a checklist of common problems (both from the developer’s and user’s point of view - I’ve been in touch with quite a number of them both) and my own suggestions to stop worrying and love the bomb.\n(UPDATED Jul 5th, 2015)\n\n1. My [insert name here] Product doesn’t work anymore!\n\nWhy should it? I tell you why, it’s a matter of mismatching expectations. The Creative Cloud app tells you it will port your Preferences: you would expect that this means Scripts-Plugins too but alas it’s not the case. Moreover, by default (what a tumble!) the CC app removes old versions of Photoshop CC, so that you can’t just copy Scripts and Plugins from CC2014 on CC2015.\n\nDeveloper: advise your customers to look in the Advanced properties of the CC update application and uncheck “Remove previous versions” so that they have access to stuff installed within the Photoshop CC2014 folder. User: look in the Photoshop CC2014/Presets/Scripts/ for Script products, and/or Photoshop CC2014/Plug-ins/ and manually copy sensible stuff from there into the corresponding Photoshop CC2015/ paths - ask your developer for help.\n\n2. Adobe Extension Manager says the product is installed but it isn’t!\n\nYou might have not heard about this, but Adobe Extension Manager (AEM) has reached his EOL (End Of Life) - that is, it’s not going to be supported by Adobe anymore. AEM doesn’t know about CC2015, it’s of no use to install CC2015 products - more about it below.\n\nDeveloper: it’s somehow by chance (or by design, depending on your POV) that your Panel shows up in CC2015, because the path for CC2014 and CC2015 is shared:\n\nMac: ~/Library/Application Support/Adobe/CEP/extensions\nWin: C:\\&lt;username&gt;\\AppData\\Roaming\\Adobe\\CEP\\extensions\n\nMind you: if your panel is self-contained (doesn’t require files such as scripts installed within the Photoshop folder, like JSXs in Presets/Scripts/ or plugins in Plug-ins/) it will work on CC2015. Otherwise, you need to move the required files manually in order for the panel to be functional. The same applies for Plug-ins and Scripts.\n\nUser: ask your developer if you think the Panel is not working properly even if it shows up, and ask for CC2015 installers (yes, you need to install them on the new version.\n\n3. The [insert name here] Panel says it’s not properly signed and/or timestamped!\n\nMy friend, have you signed and timestamped your panel? Chances are you’ve not - or possibly you are used to keep the debug flag on (CC2014) on your machine and your users don’t - and you don’t have it either on CC2015. CEP panels need to be signed/timestamped - I personally do this via ZXPSignCmd commandline tool, you’ll find all the excruciating details here).\n\nDeveloper: Build a signed/timestamped ZXP, rename it as ZIP, unZip it (OSX won’t let you do this because his inner soul is mean, download any third party free app such as The Unarchiver) and now distribute the folder for manual installation in the usual paths (see #2 above). You’ll notice that a signed/timestamped panel contains a META-INF folder and a mimetype file: take care of these guys. Please notice that:\n\n\n  You modify a single file in a signed/timestamped panel (such as the manifest.xml to add compatibility to CC2015) and the signing/timestamping is screwed, error will follow and the panel won’t show up anymore.\n  You let the uncompressed panel lie in a Google Drive folder shared with somebody else, and the signing/timestamping is screwed as well (true! It took me forever to get this one, still I don’t know why it happens).\n  Perhaps if you stare at a panel long enough, the signing and timestamping will break into tears.\n\n\nUser: ask the developer for a manual installation Panel (see #2 for the installation paths).\n\n4. Do I have to modify the Panel’s manifest.xml?\n\nIt depends. I never explicitly set an upper limit, so my panels will hopefully load in CC2015 as well:\n\n&lt;HostList&gt;\n    &lt;Host Name=\"PHXS\" Version=\"14.0\" /&gt;\n    &lt;Host Name=\"PHSP\" Version=\"14.0\" /&gt;\n&lt;/HostList&gt;\n\nThe above means: from 14 upwards. Same story if you use a MXI file for hybrid panels:\n\n&lt;products&gt;\n    &lt;product familyname=\"Photoshop\" version=\"14.0\"/&gt;\n&lt;/products&gt;\n\n5. How do I debug on CC2015?\n\nSet the PlayerDebugMode as a String with value “1” in:\n\nWin: regedit &gt; HKEY_CURRENT_USER/Software/Adobe/CSXS.6\nMac: /Users/&lt;username&gt;/Library/Preferences/com.adobe.CSXS.6.plist\n\nLet me read your mind: com.adobe.CSXS.6 doesn’t exist. Brave developer, clone it from CSXS.5 and rename it, you’ll be forgiven. Mind you: Starting with Mac 10.9, Apple introduced a caching mechanism for plist files. Your modifications to plist files do not take effect until the cache gets updated (on a periodic basis, you cannot know exactly when the update will happen). To make sure your modifications take effect, kill all cfprefsd processes. They will restart automatically.\n\n6. How do I let users install [insert name here] if Adobe Extension Manager is gone?\n\nI’ve had this thought myself too, right after the eleventh glass of wine at the AEM Funeral Party. The developer community is facing this problem and to date several possibilities are on the table:\n\n\n  Manual installation: you let users move files around - it’s not needed that I list pros and cons.\n  Native installers: you’re braver than me. If you have reliable info on this process, please let me know in the comments\n  Scripted installers: I’ve built one (free and opensource, please find it here). The idea is simple: ExtendScript knows File Management, so Photoshop itself can deploy the needed files. You let users download a ZIP, which contains an Assets folder and an Installer.jsx file - they drag and drop it into Photoshop and it’ll do the magic.\n  Extension Manager CommandLine utility: not ready for public download when I’ writing this but it will in a short while. It’s basically the operational core of AEM in all the beauty and appeal of a CLI. You’re allowed to send users a ZIP packed with your usual product’s ZXP, the Adobe CLI plus a Installer.sh (OSX) or a Installer.bat (Windows) which runs the CLI with the right params.\n  Other opensource projects still under development, like this one.\n\n\nMy own preference goes (for obvious reasons) to Scripted installer, then Manual installation, then AEM CLI, then horse shoeing.\n\n7. How do I sell through Adobe Add-ons if Extension Manager is gone?\n\nAdobe Add-ons relies on the Creative Cloud app to deploy files, via the File Synch feature (let users check it’s enabled) - they’ve somehow transplanted the AEM core into the CC app. So keep building your ZXP files and life will be good.\n\n8. My [insert name here] Script behaves badly\n\nFrom Photoshop CC2015 onwards, Adobe’s engineers have introduced the new Mondo rendering engine for ScriptUI dialogs. Previously it was Flash, and… you know. Mondo is the same framework used in Photoshop actual Filters and Plug-ins, so we’re allowed to guess that engineers will take care of it. I’ve to tell you that, while theoretically Mondo implements the same set of features that the one before it, actually it isn’t completely so (I’ll be blogposting in the near future about this). So:\n\n\n  The appearance of your Scripted Dialogs will be different\n  The functionality of your Scripted Dialogs might be different\n\n\nAnd by functionality I mean: ScriptUI can’t load JPG images now, only PNGs - so if your dialog relies on JPGs, it will fail on CC2015. (This is going to be fixed in the future).\n\n9. Some of my users say that their Adobe Extension Manager lists CC 2015 as well!\n\nCool down the enthusiasm, it’s a false positive - Adobe said that:\n\n\n  The fact that it still seems to work for CC 2015 in some cases but not others, is because the Exchange Plugin, the component in the Creative Cloud desktop app that contains the ”brains” of Extension Manager and allows extensions to be automatically installed via Add-ons, automatically updates the Extension Manager Database. In some cases this ”just works” as if Extension Manager fully supported CC 2015. It should work more often on Windows than Mac. Having said that, it’s not a supported workflow.\n\n\n10. Timestamp failure when building the ZXP\n\nYou run into ”Error - the timestamp returned from the chosen TSA could not be verified, so the ZXP created is likely to be rejected by other tools. Please create your ZXP with a different trusted TSA.”, right? Please make sure you’re using CC2015 version of the ZXPSignCmd, grab the updated version from the CEP Github Repo (MAC and PC).\n\n11. Extension Manager Opensource replacement\n\nDeveloper Max Penson has come up with the free ZXP Installer, a native app (Mac/PC) which runs under the hood the new ExManCmd commandline utility. Drag and Drop your ZXP and voilà! It still under development, but it works and you can point your customers to it. (if, for some reason out of your control, you happen to have problems with your Extension Manager *and* Creative Cloud database files, it’s going to be bad times. Been there, seen things, wasted an awful amount of time).\n\n12. After installing CC2015, my [insert name here] doesn’t work anymore on CC2014\n\nI don’t know how on earth this can happen, but I’ve seen it myself (in my own case, an entire folder in the Presets/Script disappeared causing an HTML Panel failure).\n\n13. More to come\n\nI’m adding stuff to the list (already 4 extra topic since the original draft). Take care and have fun keeping up to date :-)\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","CC 2015"],
      id: 38
    });
    

    index.add({
      title: "HTML Panels Tips #16 AngularJS Binding bug patch",
      category: ["CEP"],
      content: "CEP 5.2 has a bug (affecting Macs with non US keyboards) that prevents AngularJS to realize binding in HTML Panels when input fields are involved. New Zealand developer Kris Coppieters has been able to find a successful way to patch Angular and make it work - with his kind permission I’m going to share the instruction.\n\nThe input bug\n\nFirst, what am I talking about? See it yourself with this code:\n\n\n\nIn the browser, the text input and the h2 are bound to the same ng-model, so what you type in the input area is immediately reflected in the h2, as you’d expect. First thing that you learn in Angular tutorials: binding.\n\nNow, it happens that in Mac OSX with non-US keyboards (you can change that in the Preferences and see it yourself) the keypress / input event is fired only when a backspace is typed, so that:\n\n\n  You type “Ciao” here and nothing happens there.\n  You hit Backspace, so: “Cia” - this triggers the binding and the text appears in the h2.\n\n\nThe above is utterly annoying imho, and verified in several Adobe application (PS included). Kris Coppieters has been able to patch the angular.js file so that binding works again - please visit his Rorohiko Workflow Resources website as a way to say thanks (he’s an InDesign automation developer).\n\nI’ll be quoting Kris extensively [my comments are inline in square brackets].\n\nAngular Patch\n\n[Basically it’s a matter of modifying two files in the AngularJS source, then compile the patched build] Check out the desired Angular source code. It can be found at: https://github.com/angular/angular.js Use the repository URL: https://github.com/angular/angular.js.git.\n\nIn SourceTree or whatever your favorite git client is, clone the repo to your local hard disk. Find the desired version - e.g. for version 1.2.15, the commit is a9b5a10, dated Mar 22, 2014. I use the ‘Jump to’ functionality [top right corned in the interface] in SourceTree to jump to the 1.2.15 label. Create a local branch from that label and name your local branch to something descriptive. The patched version of Angular will detect:\n\n\n  That it is running on CEP5 in a Creative Cloud product.\n  What platform (Mac vs. non-Mac) it is running on.\n\n\nIf it is running on CEP5, and it is on a Mac, then the $SnifferProvider in the Angular source code is set to avoid using the ‘input’ event. The input event is currently (December, 2014) broken, and only fires when the backspace key is hit; it does not fire for other events. Oddly enough, this seems like a reversal of the broken behavior of IE9 (which does not emit the ‘input’ event when backspace is used, whereas CEP seems to suppress the ‘input’ event for everything except backspace).\n\nThe standard $SnifferProvider avoids using ‘input’ when running on IE9. The code I’ve added also avoids using ‘input’ when running on Macintosh CEP5/CC. This cures the ‘keyboard language dependency’ issue with panels ignoring the field contents on Macs with non-US keyboards. I changed two files:\n\n\n  src/Angular.js\n  src/ng/sniffer.js\n\n\nThe patch consists of:\n\nStep 1\n\nNear the very top of the src/Angular.js file, find the jsLint info for msie. Add two new variables to the list: isCreativeCloudCEP and isMacintosh. Mimic the syntax that is used for msie for these two variables: this is where version Angular 1.2.x and 1.3.x look somewhat different. Angular 1.2.x\n\n'use strict';\n/* We need to tell jshint what variables are being exported */\n/* global\n    -angular,\n    -msie,\n    -isCreativeCloudCEP,\n    -isMacintosh,\n    -jqLite,\n...\n*/\n\nAngular 1.3.x\n\n'use strict';\n/* We need to tell jshint what variables are being exported */\n/* global angular: true,\n  msie: true,\n  isCreativeCloudCEP: true,\n  isMacintosh: true,\n  jqLite: true,\n...\n*/\n\nStep 2\n\n2) Find msie in the list of variables being defined, and again add similar declarations for variables isCreativeCloudCEP and isMacintosh right behind it. For Angular 1.2.x (around line 136):\n\nvar /** holds major version number for IE or NaN for real browsers */\n    msie,\n    isCreativeCloudCEP,\n    isMacintosh,\n    jqLite,           // delay binding since jQuery could be loaded after us.\n\nFor Angular 1.3.x (around line 167):\n\nvar\n    msie,             \n    // holds major version number for IE, or NaN if UA is not IE.\n    isCreativeCloudCEP,\n    isMacintosh,\n    jqLite,           \n    // delay binding since jQuery could be loaded after us.\n\nStep 3\n\nA little bit further find the lines where msie is assigned and add the following statements for isCreativeCloudCEP and isMacintosh. For Angular 1.2.x (around line 155):\n\n/**\n * IE 11 changed the format of the UserAgent string.\n * See https://msdn.microsoft.com/en-us/library/ms537503.aspx\n */\nmsie = int((/msie (\\d+)/.exec(lowercase(navigator.userAgent)) || [])[1]);\nif (isNaN(msie)) {\n  msie = int((/trident\\/.*; rv:(\\d+)/.exec(lowercase(navigator.userAgent)) || [])[1]);\n}\n\n/* CEP Patch */\nisCreativeCloudCEP = \"object\" == typeof window.__adobe_cep__;\nisMacintosh = navigator.userAgent.indexOf(\"Macintosh\") &gt;= 0;\n\nFor Angular 1.3.x (around line 184):\n\n/**\n * documentMode is an IE-only property\n * https://msdn.microsoft.com/en-us/library/ie/cc196988(v=vs.85).aspx\n */\nmsie = document.documentMode;\n\n/* CEP Patch */\nisCreativeCloudCEP = \"object\" == typeof window.__adobe_cep__;\nisMacintosh = navigator.userAgent.indexOf(\"Macintosh\") &gt;= 0;\n\nStep 4\n\nIn the file src/ng/sniffer.js find these lines and add right behind it a conditional. For Angular 1.2.x (around line 71):\n\n// IE9 implements 'input' event it's so fubared that we rather pretend that it doesn't have\n// it. In particular the event is not fired when backspace or delete key are pressed or\n// when cut operation is performed.\nif (event == 'input' &amp;&amp; msie == 9) return false;\n\n// CEP Patch\nif (event == 'input' &amp;&amp; isMacintosh &amp;&amp; isCreativeCloudCEP) return false;\n\nFor Angular 1.3.x (around line 67):\n\n// IE9 implements 'input' event it's so fubared that we rather pretend that it doesn't have\n// it. In particular the event is not fired when backspace or delete key are pressed or\n// when cut operation is performed.\nif (event == 'input' &amp;&amp; msie == 9) return false;\n\n// CEP Patch\nif (event == 'input' &amp;&amp; isMacintosh &amp;&amp; isCreativeCloudCEP) return false;\n\nStep 5\n\nA little bit further, find the last returned objects and add isMacintosh and isCreativeCloudCEP. For Angular 1.2.x (around line 89):\n\nmsie : msie,\n\n// CEP Patch\nisMacintosh: isMacintosh,\nisCreativeCloudCEP: isCreativeCloudCEP\n\nFor Angular 1.3.x (around line 82):\n\ncsp: csp(),\nvendorPrefix: vendorPrefix,\ntransitions: transitions,\nanimations: animations,\nandroid: android,\n\n// CEP Patch\nisMacintosh: isMacintosh,\nisCreativeCloudCEP: isCreativeCloudCEP\n\nStep 6\n\nCompile a new version of Angular: you will need to install node.js, grunt, bower… I won’t explain that here - it’s a matter of visiting Angular Contribute page and the node.js web site and following instructions. Build angular as documented on the Angular ‘contribute’ page, then go into the build folder, and extract angular.min.js from the final .zip file. This is your patched version - it’s a drop-in replacement for the standard angular.min.js version that you branched off from.\n\nFollow-up - building Angular\n\nLet’s thank again Kris Coppieters for the instruction so far - I will add a brief section about building Angular, which hasn’t been as straightforward as I assumed first.\n\nInstalling Dependencies\n\nQuoting from Angular website:\n\n\n  Before you can build AngularJS, you must install and configure the following dependencies on your machine: Git: The Github Guide to Installing Git is a good source of information. Node.js: We use Node to generate the documentation, run a development web server, run tests, and generate distributable files. Depending on your system, you can install Node either from source or as a pre-packaged bundle. Java: We minify JavaScript using our Closure Tools jar. Make sure you have Java (version 7 or higher) installed and included in your PATH variable. Grunt: We use Grunt as our build system. Install the grunt command-line tool globally with: npm install -g grunt-cli Bower: We use Bower to manage client-side packages for the docs. Install the bower command-line tool globally with: npm install -g bower Note: You may need to use sudo (for OSX, *nix, BSD etc) or run your command shell as Administrator (for Windows) to install Grunt &amp; Bower globally.\n\n\nThis has been easy, no problem to me - by the way I already had everything needed, chances are you too already rely on those tools. Things get slightly worse when:\n\n# Install node.js dependencies:\nnpm install\n\n# Install bower components:\nbower install\n\n# Build AngularJS:\ngrunt package\n\nFirst npm: I’ve had lots of unmet dependencies with npm (Mac users should **sudo** npm install):\n\n\n\nWhich I ignored. No apparent problem with bower, but when it came to grunt package the command stopped because of some missing module, like:\n\n\n\nIn this case as you can see in the first Error line, the ‘clone-stats’ package couldn’t be found, so went on with npm install clone-stats, then grunt package again. I run into several missing modules, so I kept  npm install this-and-that and then grunt package on and on. Until the missing package appeared to be a local module such as:\n\n\n\nThen I gave up - by the way, this happened while running the “docs” task (which I’m not interested into), the compilation of both regular and minified version of angular.js was already done so I could grab the file and use it. And, good news, it works :-)\n\nTL;DR\n\nAre you having problems with AngularJS binding the model to an input field? Chances are you’re on a Mac with non-US keyboard (and if you’re not, your users can be). This is a known issue of CEP5.2. Kris Coppieters came up with an Angular patch that you can apply - if you’re lazy, just grab the patched version of Angular-1.2.9 and Angular-1.3.5.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","AngularJS"],
      id: 39
    });
    

    index.add({
      title: "HTML Panels Tips #15 Asynchronous vs. Synchronous",
      category: ["CEP"],
      content: "Today’s tip focuses on the Asynchronous nature of CSInterface.evalScript() calls, and on ways to make it work in a synchronous fashion.\n\nSynchronous vs. Asynchronous\n\nQuoting StackOverflow:\n\n\n  When you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes.\n\n\nOr visually:\n\n\n\n\n\nBack in Flash days evalScript was synchronous, now in HTML land is asynchronous:\n\nCSXSInterface.instance.evalScript('jsxFunction'); // Sync\ncsInterface().evalScript('jsxFunction'); // Async\n\nAsync example\n\nRemember that evalScript function takes two params:\n\n\n  The ExtendScript code to run.\n  A callback function, which in turn takes as a param the returned value from the ExtendScript call.\n\n\nThe default behavior is as follows.\n\nvar csInterface = new CSInterface();\nvar numberOfDocuments = undefined;\n\ncsInterface.evalScript('app.documents.length', function(result) {\n  numberOfDocuments = result;\n});\n\nalert(numberOfDocuments); // undefined\n\nIn the above example the app.documents.length statement is executed and directly returned - the number of currently opened documents in Photoshop is passed to the callback, which assigns it to the numberOfDocuments variable. The alert says “undefined” because of the asynchronous nature of evalScript: line #4 executes, then the interpreter goes ahead to line #8 not waiting for the ExtendScript to return (in this example, a single line of code, but in real world a long chain of functions that trigger usually time consuming Photoshop operations) so when alert(numberOfDocuments) is called, numberOfDocuments is still undefined. So to speak, it is like asking your partner to do the shopping then, as soon as (s)he goes out, open the fridge and say “rats, it’s empty!”\n\nSync workarounds\n\nThere are few different ways to mimic a synchronous behavior\n\n1. Set a Timeout\n\nThe easiest way is to just wait for your partner to get back from the store:\n\nvar csInterface = new CSInterface();\nvar numberOfDocuments = undefined;\n\ncsInterface.evalScript('app.documents.length', function (result) {\n  numberOfDocuments = result;\n});\nsetTimeout( alert(numberOfDocuments), 500);\n\nAlthough, it’s not ideal. You might not know in advance how long to wait - it depends of the kind of ExtendScript task you’re executing.\n\n2. Nest code in Callbacks\n\nIf the alert is moved within the callback it works properly:\n\nvar csInterface = new CSInterface();\nvar numberOfDocuments = undefined;\n\ncsInterface.evalScript('app.documents.length', function (result) {\n  numberOfDocuments = result;\n  alert(numberOfDocuments);\n});\n\nOf course this can lead to the so-called callback hell – nesting evalScript calls one inside another, like:\n\ncsInterface.evalScript('/* some code */', function(result) {\n  csInterface.evalScript('/* some other code */', function (result) {\n    // etc...\n  })\n});\n\nYet if you’ve not fancy needs, it works.\n\n3. Event driven callbacks\n\nThis method is supported only from CEP 5.2 onwards. You can implement custom Events via PlugPlugExternalObject, that are dispatched by ExtendScript and listened from JS. I’ve put the ExtendScript code in its own JSX file:\n\nfunction getNumberOfDocuments () {\n  try {\n    var xLib = new ExternalObject(\"lib:\\\\PlugPlugExternalObject\");\n  } catch(e) {\n  alert(e.message);\n  return false;\n  }\n\n  var eventObj = new CSXSEvent();\n  eventObj.type = \"alertDocumentsNumber\";\n\n  // data must be a string otherwise PS (at least CC 2014.1.2) crashes\n  eventObj.data = app.documents.length.toString();\n\n  eventObj.dispatch();\n  xLib.unload();\n  return true;\n}\n\nThen in the JS you set up a listener, which is the one who fires the alert();\n\nvar csInterface = new CSInterface();\n\ncsInterface.addEventListener(\"alertDocumentsNumber\", function(event) {\n  alert(\"The number of open documents is: \" + event.data);\n});\n\ncsInterface.evalScript('getNumberOfDocuments()', function(result) {\n  if (result == false) { alert (\"There's been a problem.\"); }\n});\n\nMind you, as far as other devs such as Kris Coppieters have supposed, “even if you manage to make multiple JSX calls from different parts of your JS, probably you can’t run multiple ExtendScripts concurrently”.\n\nAcknowledgements\n\nI’d like to thank Zihong Chen and Ten from Adobe and the others who’ve helped shedding some light upon this topic in the forums.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","evalScript","asynchronous"],
      id: 40
    });
    

    index.add({
      title: "HTML Panels Tips: #14 Flyout Menu",
      category: ["CEP","HTML Panels"],
      content: "With Photoshop CC 2014.2 (implementing CEP 5.2) it’s finally possible to have flyout menus in HTML Panels too - as we had back in Flash land. This Tip shows you how to set them and deal with their click handler.\n\n\n\nThe full code of this demo extension is available on my GitHub page in the PS Panels Boilerplate project. Things are pretty simple, though: you first set an XML String which describes the menu:\n\nvar flyoutXML = '\\\n&lt;Menu&gt; \\\n\t&lt;MenuItem Id=\"enabledMenuItem\" Label=\"Enabled Menu Item\" Enabled=\"true\" Checked=\"false\"/&gt; \\\n\t&lt;MenuItem Id=\"disabledMenuItem\" Label=\"Disabled Menu Item\" Enabled=\"false\" Checked=\"false\"/&gt; \\\n\t\\\n\t&lt;MenuItem Label=\"---\" /&gt; \\\n\t\\\n\t&lt;MenuItem Id=\"checkableMenuItem\" Label=\"Checkable Menu Item\" Enabled=\"true\" Checked=\"true\"/&gt; \\\n\t\\\n\t&lt;MenuItem Label=\"---\" /&gt; \\\n\t\\\n\t&lt;MenuItem Id=\"actionMenuItem\" Label=\"Click me to enable/disable the Target Menu!\" Enabled=\"true\" Checked=\"false\"/&gt; \\\n\t&lt;MenuItem Id=\"targetMenuItem\" Label=\"Target Menu Item\" Enabled=\"true\" Checked=\"false\"/&gt; \\\n\t\\\n\t&lt;MenuItem Label=\"---\" /&gt; \\\n\t\\\n\t&lt;MenuItem Label=\"Parent Menu (wont work on PS CC 2014.2.0)\"&gt; \\\n\t\t&lt;MenuItem Label=\"Child Menu 1\"/&gt; \\\n\t\t&lt;MenuItem Label=\"Child Menu 2\"/&gt; \\\n\t&lt;/MenuItem&gt; \\\n&lt;/Menu&gt;';\n\nRemember the \\ at the lines’ end to escape the carriage return. Nothing fancy, just MenuItem tags to define the structure. Mind you, nested menus don’t work right now (in Photoshop CC 2014.2.0 - although After Effects and Premiere are OK) but the bug is going to be fixed. You actually build the menu feeding setPanelFlyoutMenu with the XML string:\n\n// Uses the XML string to build the menu\ncsInterface.setPanelFlyoutMenu(flyoutXML);\n\nAn event listener has been added to respond to clicks (the event is \"com.adobe.csxs.events.flyoutMenuClicked\"):\n\ncsInterface.addEventListener(\"com.adobe.csxs.events.flyoutMenuClicked\", flyoutMenuClickedHandler);\n\nWhen a click occurs, an event is passed to the callback. It’s data attribute is an object containing both menuId and menuName . The callback in my case is as follows:\n\n// Ugly workaround to keep track of \"checked\" and \"enabled\" statuses\nvar checkableMenuItem_isChecked = true;\nvar targetMenuItem_isEnabled = true;\n// Flyout Menu Click Callback\nfunction flyoutMenuClickedHandler(event) {\n\n  // the event's \"data\" attribute is an object, which contains \"menuId\" and \"menuName\"\n  console.dir(event);\n  switch (event.data.menuId) {\n\n    case \"checkableMenuItem\":\n      checkableMenuItem_isChecked = !checkableMenuItem_isChecked;\n      csInterface.updatePanelMenuItem(\"Checkable Menu Item\", true, checkableMenuItem_isChecked);\n      break;\n\n    case \"actionMenuItem\":\n      targetMenuItem_isEnabled = !targetMenuItem_isEnabled;\n      csInterface.updatePanelMenuItem(\"Target Menu Item\", targetMenuItem_isEnabled, false);\n      break;\n\n    default:\n      console.log(event.data.menuName + \" clicked!\");\n  }\n\n  csInterface.evalScript(\"alert('Clicked!\\\\n \\\"\" + event.data.menuName + \"\\\"');\");\n}\n\nThe code above:\n\n\n  fires a simple alert with the Label when a menu item is clicked;\n  lets you check/uncheck a menu item;\n  toggles the enabled/disabled status of a menu item when you click on a different item.\n\n\nI haven’t been able to retrieve the enabled/disabled and checked/unchecked menu items status dynamically, so I’ve worked around storing them in vars. Not ideal maybe, but it works As you see, I’ve used the updatePanelMenuItem function, which is used this way (straight from CSInterface.js sourcecode):\n\n/**\n * Updates a menu item in the extension window's flyout menu, by setting the enabled\n * and selection status.\n *  \n * Since 5.2.0\n *\n * @param menuItemLabel The menu item label.\n * @param enabled       True to enable the item, false to disable it (gray it out).\n * @param checked       True to select the item, false to deselect it.\n *\n * @return false when the host application does not support this functionality (HostCapabilities.EXTENDED_PANEL_MENU is false).\n *         Fails silently if menu label is invalid.\n *\n * @see HostCapabilities.EXTENDED_PANEL_MENU\n */\nCSInterface.prototype.updatePanelMenuItem = function(menuItemLabel, enabled, checked) {\n  var ret = false;\n  if (this.getHostCapabilities().EXTENDED_PANEL_MENU) {\n    var itemStatus = new MenuItemStatus(menuItemLabel, enabled, checked);\n    ret = window.__adobe_cep__.invokeSync(\"updatePanelMenuItem\", JSON.stringify(itemStatus));\n  }\n  return ret;\n};\n\nThat’s it, hope this helps! The extension is available on GitHub in my PS Panels Boilerplate project.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","flyout"],
      id: 41
    });
    

    index.add({
      title: "HTML Panels Tips: #13 Automate ZXP Packaging with Gulp.js",
      category: ["CEP"],
      content: "As a HTML Panels Tips: #10 Packaging / ZXP Installers follow-up, in this tip I will show you how to automate (i.e. make less error prone) the utterly annoying job of building ZXP installers: i.e. with the help of Gulp.js, a task runner based on Node.js.\n\nGulp.js\n\nIf you come from Photoshop scripting like me, you might be pretty green when it comes to the plethora of tools that web developers are used to. Since they often need to perform repetitive tasks (such as compile LESS/SASS stylesheets, minify CSS, lint, uglify and concatenate JS, rename and move files, etc.) some tools known as build tools or task runners appeared - a couple of remarkable ones being Grunt and Gulp.\n\nThey do basically the same thing: Grunt is based on configuration, Gulp is (newer, and) code based. If the previous sentence makes no sense to you it doesn’t matter too much - if you want to learn more about task runners I strongly suggest you to watch this screencast made by the great and prolific @sayanee_ from the Build Podcast series (60 free videos about all kind of tech tools). Amazing resource, really.\n\nI won’t dig deeper on Gulp here (also because I’m not a task runner expert myself), I will just share the particular code I’m using and that proved to work, explaining what each part does and trying to show a glimpse of the big picture. Hopefully, my workflow is simple enough for you to tweak it to fit your own needs.\n\nRequisites\n\nGulp - like tons of useful stuff - is based on Node.js (a platform based on Google Chrome runtimes), so you’d better install it first downloading from the Node website if you haven’t already done that. Then, you need to know that there are about 88 thousands packages extending Node - which are managed by npm, the Node Package Manager. Npm is automatically installed alongside Node.js. You must have Gulp installed globally (i.e. not only in your project directory but available everywhere), so fire the Terminal and:\n\nsudo npm install -g gulp\n\nEnter your user password when requested (sudo is a mandatory step on OSX as far as I know - I can’t say if it’s needed on Windows shell too).\n\nFolders’ structure\n\n\n\nI will package a hybrid extension - the very same I showed in HTML Panels Tips: #10 Packaging / ZXP Installers. That is, an installer that deploys an HTML Panel, a Mac and Windows specific plugin files and a shared Photoshop script. Please refer to the original post for information about the manual process - here I’ll just automate it. The folder hierarchy is as follows:\n\n\n  src/ contains all the source code; build/ is empty and will contain just the final installer.\n  The source for the HTML panel is the src/com.example.myExtension folder.\n  The src/MXI folder gathers all the assets needed to pack the hybrid extension:\n    \n      src/MXI/com.example.myExtension.mxi is the configuration file (info here).\n      src/MXI/HTML is empty but will contain the ZXP of the HTML panel.\n      src/MXI/MAC, src/MXI/WIN and src/MXI/SCRIPT are the plugins and scripts needed by the extension.\n    \n  \n  Finally, ucf.jar, ZXPSignCmd and myCertificate.p12 are the command line tools and the signing certificate that you should well know about.\n\n\nI’ve left alone the package.json and gulpfile.js files, which I will disclose in a moment.\n\nSetting up the project\n\nYou can either create yourself the package.json file with the following content:\n\n{\n  \"name\": \"Gulp-package-demo\",\n  \"version\": \"0.1.0\",\n  \"description\": \"CC Extension packaging with Gulp - DEMO\",\n  \"author\": \"Davide Barranca\",\n  \"license\": \"MIT\"\n}\n\nOr let npm do it for you: cd into the project directory (the root), and\n\nnpm init\n\nThe Node Package Manager will initialise a project asking you few questions; as follows my answers (when there’s none, I just hit enter to confirm the suggested defaults):\n\nThis utility will walk you through creating a package.json file.\nIt only covers the most common items, and tries to guess sane defaults.\n\nSee `npm help json` for definitive documentation on these fields\nand exactly what they do.\n\nUse `npm install &lt;pkg&gt; --save` afterwards to install a package and\nsave it as a dependency in the package.json file.\n\nPress ^C at any time to quit.\nname: (Gulp) Gulp-package-demo\nversion: (0.0.0) 0.1.0\ndescription: CC Extension packaging with Gulp - DEMO\nentry point: (index.js)\ntest command:\ngit repository:\nkeywords:\nauthor: Davide Barranca\nlicense: (BSD-2-Clause) MIT\nAbout to write to /Users/Davide/Google Drive/PACK/Gulp/package.json:\n\n{\n  \"name\": \"Gulp-package-demo\",\n  \"version\": \"0.1.0\",\n  \"description\": \"CC Extension packaging with Gulp - DEMO\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\"\n  },\n  \"author\": \"Davide Barranca\",\n  \"license\": \"MIT\"\n}\n\n\nIs this ok? (yes)\n\nOne way or the other, a package.json file should be there. Now let’s install (locally – i.e. in this very folder) gulp:\n\nnpm install --save-dev gulp\n\nWe need two gulp plugins: gulp-clean is for deleting files:\n\nnpm install --save-dev gulp-clean\n\nwhile gulp-shell is a command line interface:\n\nnpm install --save-dev gulp-shell\n\nThe --save-dev flag instructs npm to add these ones as project’s dependencies, automatically modifying the json file for you:\n\n{\n  \"name\": \"Gulp-package-demo\",\n  \"version\": \"0.1.0\",\n  \"description\": \"CC Extension packaging with Gulp - DEMO\",\n  \"author\": \"Davide Barranca\",\n  \"license\": \"MIT\",\n  \"devDependencies\": {\n    \"gulp\": \"~3.8.7\",\n    \"gulp-shell\": \"~0.2.9\",\n    \"gulp-clean\": \"~0.3.1\"\n  }\n}\n\nYou’ll see that npm has installed these dependencies in a node_modules folder.\n\ngulpfile.js\n\nHere comes the fun: the gulpfile.js is where you start defining tasks, that you’ll be calling from the Terminal. Before digging into the actual, entire workflow, let’s familiarize with tasks - here’s a simple one:\n\nvar gulp = require('gulp');\nvar clean = require('gulp-clean');\n\n// clean all ZXP files found everywhere\ngulp.task('clean-all', function () {\n\treturn gulp.src('./**/*.zxp', { read: false })\n\t.pipe(clean());\n});\n\nYou first require gulp, then call the gulp.task function. The first parameter is the task name as a string (here 'clean-all'), the second is the function that will return a Stream (if you feel inclined, find here info about Streams). Don’t worry too much about that: what you need to know is just that you usually define a gulp source (here every zxp file from any subdirectory) and you pipe that to the clean function - which deletes files. The { read: false } is a configuration object that instructs gulp not to read them, to speed up the process. How do you call that task? In the Terminal (root folder):\n\ngulp clean-all\n\nAnd presto! all the ZXPs that might have been around are gone. Here is another useful task - it uses the gulp-shell plugin to call the ZXPSignCmd command line and actually pack files:\n\nvar shell = require('gulp-shell');\n\n// pack the HTML panel using ZXPSignCmd\ngulp.task('pack-html',  \n\tshell.task('./ZXPSignCmd -sign src/com.example.myExtension src/com.example.myExtension.zxp myCertificate.p12 myPassword -tsa https://timestamp.geotrust.com/tsa') // don't put the semicolon here or it will break!\n);\n\nIf you run in the Terminal:\n\ngulp pack-html\n\nit is like if you had typed the whole, long ZXPSignCmd string yourself. Hopefully you start appreciating the power of task runners now.\n\nCombining tasks…\n\nA task can rely on other tasks - dependencies are defined into an array of tasks:\n\ngulp.task('pack-html', \\['clean-all'\\]. function () {\n\tshell.task('./ZXPSignCmd -sign src/com.example.myExtension src/com.example.myExtension.zxp myCertificate.p12 myPassword -tsa https://timestamp.geotrust.com/tsa')\n}\n\nIn the above example, the 'clean-all' task is performed first, then the 'pack-html' task runs. If you have multiple dependencies (that is: your task needs to run after several other tasks) you can define them this way:\n\ngulp.task('test-task', ['first-dep', 'second-dep'], function () {\n    //...\n}\n\nMind you: while the 'test-task' actually runs after 'first-dep' and 'second-dep', you cannot be sure that 'first-dep' and 'second-dep' are executed in this very order! Dependencies are processed asynchronously: 'second-dep' may finish before 'first-dep'.\n\n… synchronously!\n\nBut in the ZXP automation we need to be sure that a task is done before the next one starts - i.e. the whole thing must be synchronous. There are few ways to trick gulp into this behaviour: returning a stream, defining a callback, using promises and - the easiest one and therefore my preferred - chaining dependencies:\n\n\n  Task #4 depends on Task #3\n  Task #3 depends on Task #2\n  Task #2 depends on Task #1\n\n\nYou run Task #4 and in fact the whole sequence is #1 &gt; #2 &gt; #3 &gt; #4. There might be more elegant ways to get there, but this approach has the advantage of being easily testable: if the result isn’t what you’d expect, just run Task #3 to test the gulpfile up to that point; if it’s still broken, try Task #2, and so on, until you find the bugged task.\n\nComplete gulpfile.js\n\nGiven the folder structure I’ve outlined, the entire gulpfile is as follows:\n\n400: Invalid request\n\n\nHopefully the code and the comments are self-explanatory. Few caveats that might save you some search time on Google:\n\n\n  './**/*.zxp' is a way to mean: every zxp file in every subfolder of the root (being the root './').\n  For a reason that I don’t know, a semicolon after the shell.task() breaks gulp.\n  { read: false } speeds up the process when you don’t need gulp to read files.\n  { base: './src/' } (as used in task #2) tells gulp to use ./src/ as a base for the relative paths. If you omit it or set it to the root './', you’ll end up with /src/MXI/HTML/src/com.example.myExtension.zxp (and extra /src/) and not /src/MXI/HTML/com.example.myExtension.zxp which is what you actually want.\n\n\nThis tip just scratches the surface of gulp.js as a task runner for CC Extensions - I hope to have time to dig deeper and share my findings again - feel free to add yours in the comments!\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","automation"],
      id: 42
    });
    

    index.add({
      title: "Introducing Fixel Detailizer 2 PS",
      category: ["Photoshop"],
      content: "I’m glad to announce the release on Adobe Add-ons of a powerful contrast booster for Photoshop: Fixel Detailizer 2 PS. Let’s see what is that all about!\n\nFixel Algorithms\n\nI’ve recently started a partnership with Fixel Algorithms, a company founded by engineers and programmers specialized in Digital Image Processing - amazingly smart people. Fixel Algorithms has been creating Filters for Adobe After Effects (video processing), and Detailizer is our first shared effort porting to Photoshop their excellent work.\n\nDetailizer at a glance\n\nDetailizer 2 PS decomposes your image into discrete Frequency Ranges and allows you to separately control the Contrast Boost of each of them. It’s like Frequency Separation on steroids!\n\n\n\t\n\tBefore\n\n\n\n\t\n\tAfter\n\n\nFrequency explained\n\nDo you need a primer on spacial frequency? Basically, it’s about the size of the detail in your pictures. For instance, say that you’ve shot a portrait - take a sample in:\n\n\n  the hair: there’s plenty of fine detail, pixel’s values (dark/bright) vary with a high frequency.\n  the lips: the variation is smoother compared to the hair, even if the transition between dark/bright values happens with a decent (say: middle-sized) frequency too.\n  the cheek: here the tonal variation is really smooth, and covers a larger area: in the sample things change on a lower rate (low frequency).\n\n\n\n\nIt might not be the most accurate description of what a spacial frequency is, but you’ve probably got the idea. In the real world the distinction is fuzzier: every area is made with all frequencies combined (within “low-freq” cheeks there is “high-freq” skin texture), the same way a complex signal such a sound or an electromagnetic wave is a combination of simpler signals:\n\n\n\nFeatures\n\nFixel Detailizer 2 PS peculiar frequency decomposition (the filter’s core) is performed with a fast, proprietary Wavelets algorithm developed by Fixel. The interface lets you boost separately 5 Frequency Ranges, in order to target precisely the detail level that you want.\n\n\n\nFixel Detailizer 2 PS works on 8bit, 16bit and 32bit HDR files too! All the calculations are internally performed at 32bits to ensure the maximum precision.\n\nHow to get it\n\nThis Photoshop Filter available on CC-Extensions for USD 29.99 and supports Photoshop CC and newer on both Mac and Windows (64bit).\n",
      tags: ["Fixel Algorithms","Fixel Detailizer"],
      id: 43
    });
    

    index.add({
      title: "HTML Panels Tips: #12 CEP Application Events",
      category: ["CEP"],
      content: "HTML Panels can listen to CEP Application Events - that is to say, Events dispatched by the Host (Photoshop, InDesign…) when something related to the app itself, or its documents, happens.\n\nList of Photoshop Events\n\nTo date (CC 2014, version 2014.0.0 Release, 20140508.r58) this is the list of the Events that Photoshop is able to dispatch. Hopefully the names are self-explanatory.\n\n\n  \"com.adobe.csxs.events.AppOffline\"\n  \"com.adobe.csxs.events.AppOnline\"\n  \"applicationActivate\"\n  \"applicationBeforeQuit\"\n  \"documentAfterActivate\"\n  \"documentAfterDeactivate\"\n  \"documentEdited\"\n\n\nMind you, according to the Extension SDK guide some of them aren’t supported in Photoshop yet; others aren’t listed there (but they come from the CEP HTML Test Panel from the Samples GitHub repo).\n\nImplementation\n\nAs easy as it gets - this goes in the JS:\n\nvar csInterface = new CSInterface();\ncsInterface.addEventListener(\"appOffline\", logEvent);\n// just a demo callback\nfunction logEvent(evt) { console.dir(evt); }\n\nEvent Parameters\n\nThe event object passed to the callback in Photoshop has several properties:\n\n\n  appId: \"PHSX\"\n  extensionId: \"\"\n  scope: \"APPLICATION\"\n  type\n  data\n\n\nAccording to each type, the data might come as an XML String, as follows:\n\ntype: \"com.adobe.csxs.events.AppOffline\"\ndata: \"\"\n\ntype: \"com.adobe.csxs.events.AppOnline\"\ndata: \"\"\n\ntype: \"applicationActivate\"\ndata: \"&lt;applicationActivate/&gt;\"\n\ntype: \"applicationBeforeQuit\"\ndata: \"&lt;applicationBeforeQuit/&gt;\"\n\ntype: \"documentAfterActivate\"\ndata: \"&lt;documentAfterActivate&gt;&lt;name&gt;icon&lt;/name&gt;&lt;url&gt;file:///Macintosh SSD/Users/Davide/Desktop/icon.tif&lt;/url&gt;&lt;/documentAfterActivate&gt;\"\n\ntype: \"documentAfterDeactivate\"\ndata: \"&lt;documentAfterDeactivate&gt;&lt;name&gt;icon&lt;/name&gt;&lt;url&gt;file:///Macintosh SSD/Users/Davide/Desktop/icon.tif&lt;/url&gt;&lt;/documentAfterDeactivate&gt;\"\n\ntype: \"documentEdited\"\ndata: \"&lt;documentEdited&gt;&lt;name&gt;icon&lt;/name&gt;&lt;url&gt;file:///Macintosh SSD/Users/Davide/Desktop/icon.tif&lt;/url&gt;&lt;/documentEdited&gt;\"\n\nMind you, I’ve been able to log the applicationBeforeQuit event only trying to quit Photoshop when an unsaved document was open - this gives the time for the console.dir to be logged, otherwise the Object is empty, after the app has quit. Also, I haven’t been able to successfully intercept the \"com.adobe.csxs.events.ExtensionUnloaded\" event, which would be of great use.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","Events"],
      id: 44
    });
    

    index.add({
      title: "HTML Panels Tips: #11 CEP Events (ExternalObject)",
      category: "– CEP",
      content: "The new CEP5 (from Photoshop CC 2014 onwards) has introduced the possibility to dispatch Custom Events from JSX and listen to them in the HTML Panel - this Tip shows you how to implement this communication.\n\nEvents et al.\n\nI’ve already covered Events in this series, so let me summarize what we can do with them and the different kinds of Events we might deal with.\n\n\n  CEP Host Application Events - i.e. Photoshop events related either to the app itself or the documents (but from the “host app point of view”, see this post) such as:\n    \n      appOffline\n      appOnline\n      applicationActivate\n      documentAfterActivate\n      documentAfterDeactivate\n    \n  \n  ExtendScript Events - i.e. Events related to actual Photoshop operations (dispatched when the user creates new Layer, selects/moves something, etc), see this post and a different approach.\n  Custom ExtendScript Events - i.e. Events you dispatch yourself in the JSX to communicate with, and pass data to, the HTML Panel (what this post is all about).\n\n\nExternalObject\n\nPhotoshop, Illustrator and Premiere Pro CC 2014 by the time of this writing (July 2014) support natively the dispatching of custom Events (using the PlugPlugExternalObject - which library is embedded in the app and we can happily forget about it), while InDesign must explicitly add and reference it (download it here and see the SDK Guide at pag 45). Basically you set an Event Listener in the JS for a custom event (named as you like), providing a callback. Then in the JSX you create and dispatch a CSXSEvent() at will.\n\nExample\n\nYou can download a full panel from my PS Panels Boilerplate repository on GitHub here. As follows some code highlights to demo the mechanism.\n\nJS\n\nIn the JS side there’s a simple EventListener with a callback as usual.\n\nvar csInterface = new CSInterface();\ncsInterface.addEventListener(\"My Custom Event\", function(evt) {\n    console.log('Data from the JSX payload: ' + evt.data);\n});\n\nJSX\n\nIn the JSX you create an ExternalObject instance: \"lib:\\PlugPlugExternalObject\" is the only parameter to pass. If there’s no error (the library exists and it’s available) you create a new CSXSEvent setting the type (which is referenced in the EventListener - the string must be the same, in the example “My Custom Event”) and a data property which is the payload that the HTML Panel is going to receive.\n\ntry {\n    var xLib = new ExternalObject(\"lib:\\\\PlugPlugExternalObject\");\n} catch (e) {\n    alert(e);\n}\n\nif (xLib) {\n    var eventObj = new CSXSEvent();\n    eventObj.type = \"My Custom Event\";\n    eventObj.data = \"some payload data...\";\n    eventObj.dispatch();\n}\n\n\n\nIn the GitHub panel things are slightly more elaborate: I’ve set a JSX function triggered by a button - which sends Custom Events spaced 1 second each. The Events’ payloads are then logged in a textarea.\n\nI’m facing two issues, though. First, I would expect that the log messages come time spaced (first log, 1” wait, second log, 1” wait, third log, 1” wait, etc), while the get to the textarea all together. Second, the [Return Message…] which comes as the result value passed to the evalScript() callback appears as the first, and not last, log. As far as I understand, the Events are kept on hold until the whole JSX routine is done and the evalScript() callback executed - whether this is a problem of Event dispatching (the Photoshop / ExtendScript side) or Event listening (the JS / HTML Panel side) I can’t really say.\n\nBonus\n\nA Twitter conversation with the digital artist Sergey Kritskiy led me to experiment whether the Panel is able to listen for CSXSEvents dispatched only from its own JSX files (the ones in its scope), or not. In other words, can an (open) HTML Panel fire a callback in response to a Custom Event of the right type coming from whatever Script might happen to run in Photoshop in that moment? The answer is yes: in fact if you keep the JSX to CEP Communication panel open and run in the ExtendScript ToolKit the same chunk of code provided in the JSX section above in this very post, the Panel shows “some payload data…” in its textarea.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips","Events"],
      id: 45
    });
    

    index.add({
      title: "Photoshop CC 2014 (v15.0) ready!",
      category: ["Photoshop"],
      content: "The support of Flash panels has been officially dropped with the release or Photoshop CC 2014 (internal version: 15.0) - meaning that in order to be compatible with it, Extensions must have been ported to HTML. Which I did for all my panels! They are now CC 2014 compatible - while scripts don’t need refactoring:\n\n\n  ALCE - Advanced Local Contrast Enhancer (HTML Panel)\n  Double USM (Script)\n  VitaminBW (Script)\n  PS-Projects (Script)\n  Floating Adjustments (HTML Panel)\n  Theme Switcher (HTML Panel)\n  Parametric Curves (Script)\n\n\nPlease find the updated installer (log in the Adobe Add-ons page if you’ve bought via Adobe Exchange, or in the Bigano e-store if you’ve bought from him). A couple of notes:\n\n\n  Adobe Configurator is now useless, since it was an AIR app outputting Flash panels. There’s no Adobe’s HTML replacement planned, nor a migration tool for existing panels (you can go banging fists on table in the forums, or embrace the new SDK if you want / feel inclined). I’ve been experimenting with templates targeted to non-coders, that would ideally need just some minor tweaking. I know there is also somebody else (also third party) fiddling with the idea of an HTML Configurator. Don’t hold your breath, but it might be that in the future something will appear.\n  CPT (Channel Power Tools) and False Profile - two extensions made by the great Giuliana Abbiati - have not been ported yet since she’s been busy with the HTML version of Dan Margulis PPW Panel. I’ll be happy to disclose information about the timeframe as soon as I have them.\n\n",
      tags: [],
      id: 46
    });
    

    index.add({
      title: "PS-Panels-Boilerplate on GitHub!",
      category: ["CEP"],
      content: "I’ve created a new project on my GitHub account where I will collect both Boilerplate code for HTML Panels and other Demo extensions - it’s called PS-Panels-Boilerplate. I’ve started with a revised version of my Topcoat CSS integration post. The code is now better because it doesn’t rely on the usual two version of topcoat:\n\n\n  topcoat-desktop-light.css\n  topcoat-desktop-dark.css\n\n\nInstead, I’ve tweaked the original .styl and the Grunt.js files - and following Garth Braithwaite’s tutorial I’ve eventually been able to output 4 CSS files that better match the Photoshop GUI, keeping the peculiar Topcoat personality:\n\n\n  topcoat-desktop-lightlight.css\n  topcoat-desktop-light.css\n  topcoat-desktop-dark.css\n  topcoat-desktop-darkdark.css\n\n\nThe themeManager.js code is refactored too, also (thanks to David Deraedt’s idea) injecting app’s Font information in the CSS.\n\nUPDATE - Photoshop Tools\n\nI’ve added this HTML Panel, which has been fun to build - go grab it on my GitHub page.\n\n\n\nIn the future - job, family and time permitting - I hope to be able to store in that repository the full code related to my entire HTML Panels tip series (and more! Since I’ve started approaching Angular, I plan to cover that one too).\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["GitHub"],
      id: 47
    });
    

    index.add({
      title: "HTML Panels Tips: #10 Packaging / ZXP Installers",
      category: ["CEP"],
      content: "While there is a more user friendly app such as Adobe Packager which is a great simple app for simple needs, I will show you how to custom build ZXP installers for:\n\n\n  HTML Extensions\n  Bundled Flash + HTML Extensions\n  Hybrid Extensions (Panel + Plug-ins / Assets, etc)\n\n\nusing the ZXPSignCmd, the ucf.jar command line tools, and custom MXI files. As a primer on Extension installers, you must know that:\n\n\n  in order to deploy your products (either Panels, Scripts but also assets such as Brushes, Textures, etc) you have to build a ZXP package - to be submitted to Adobe Add-ons (the Adobe Exchange web store) or distributed in other ways.\n  ZXP must be signed with either a Paid Certificate (see a list of supported providers) or a free, Self-Signed certificate.\n  ZXP must be timestamped. You can avoid it if you plan to sell your own stuff directly, but for some good reasons I strongly suggest you to timestamp. A table showing various scenarios is as follows:\n\n\n\n\nMore info and resources links below, so let’s start diving.\n\n1. Create a Certificate\n\nFirst, grab the CC Extensions Signing Toolkit, that is to say the ZXPSignCmd executable file. Myself, I will go with a free, self-signed certificate: Adobe Extension Manager will fire a couple of warning popups, but Adobe Add-ons is perfectly fine with it and won’t complain. So: fire the Terminal (or the Win Command line), get to the directory where you’ve moved the ZXPSignCmd file (if you don’t know how to do this, just type “cd “ - mind you there’s a space - in the terminal and drag and drop the folder, then hit Enter) and create a certificate using this pattern:\n\nZXPSignCmd -selfSignedCert &lt;countryCode&gt; &lt;stateOrProvince&gt; &lt;organization&gt; &lt;commonName&gt; &lt;password&gt; &lt;outputPath.p12&gt;\n\nAn actual example with fake data can be:\n\n./ZXPSignCmd -selfSignedCert IT BO DBCompany \"Davide Barranca\" OcaMorta selfDB.p12\n\nMind you, ./ at the beginning instructs the shell to look for the executable in the current directory. If everything went OK you should find a newly created selfDB.p12. That’s your self signed cert.\n\n2. Pack and Sign an HTML Extension\n\n\n\nLet’s assume the extension you’ve made has the ID com.example.helloworld and it’s contained within a directory named accordingly. Gather together the Extension with the ZXPSignCmd and the selfDB.p12 in one folder, then build the ZXP using this pattern:\n\nZXPSignCmd -sign &lt;inputDirectory&gt; &lt;outputZxp&gt; &lt;p12&gt; &lt;p12Password&gt; -tsa &lt;timestampURL&gt;\n\nWhich in our example (I’ve put a fake password too) translates in:\n\n./ZXPSignCmd -sign com.example.helloworld com.example.helloworld.zxp selfDB.p12 OcaMorta -tsa https://timestamp.geotrust.com/tsa\n\nIf everything’s OK you should find a newly created com.example.helloworld.zxp file. That’s your self signed, timestamped installer; submit it to the Add Ons website and/or privately and enjoy.\n\n3. Pack and Sign HTML + Flash Extensions\n\nHere comes the fun. I assume you have (for the same product) already coded and packed an HTML version, plus the Flash version, so you have:\n\n\n  com.example.helloworld.zxp (HTML version, packed and signed as shown above)\n  com.example.helloworld.zxp (Flash version, packed and signed with Flash Builder 4.5 as we used to do back in the Flex days)\n\n\nMind you, I use IDs for the ZXP names because in the past Adobe Exchange was a bit picky in that regard - so it’s not just hello.zxp. I don’t know whether things have changed now, try and let me know.\n\n\n\nPlease grab the ucf.jar command line tool and arrange things in the way shown in the screenshot at right. That is to say, put into a folder:\n\n\n  the selfDB.p12\n  the ucf.jar\n  a 46x46px icon.png (to be used as the icon in Extension Manager)\n  a new MXI folder. Inside the MXI there’s:\n    \n      an HTML folder containing the HTML panel ZXP installer,\n      a FLASH folder containing the Flash panel ZXP installer,\n      a MXI file, which I’m going to talk about right now.\n    \n  \n\n\nMXI file\n\nBasically it’s an XML file containing instruction for the files deployment and lots of metadata. Please refer to this Extension Manager CC Configuration File Reference for extra info and best practices. As follows the MXI needed to correctly deploy both Flash and HTML extensions. Mind you, PS CS6 (13.x) and current CC (14.x) are supporting Flash, while HTML is for Photoshop-next (which I assume that is going to be 15).\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;macromedia-extension\n  id=\"com.example.helloworld\"\n  icon=\"icon.png\"\n  name=\"Hello World\"\n  requires-restart=\"true\"\n  version=\"1.0.0\"&gt;\n\n  &lt;author name=\"Davide Barranca\"/&gt;\n\n  &lt;description href=\"http://www.davidebarranca.com\"&gt;\n    &lt;![CDATA[Here goes the extension description]]&gt;\n  &lt;/description&gt;\n\n   &lt;ui-access&gt;\n    &lt;![CDATA[Here you tell the user where she will find your product (in the Window - Extensions menu, or elsewhere)]]&gt;\n  &lt;/ui-access&gt;\n\n  &lt;license-agreement&gt;\n    &lt;![CDATA[Here goes the EULA]]&gt;\n  &lt;/license-agreement&gt;\n\n  &lt;products&gt;\n    &lt;product familyname=\"Photoshop\" version=\"13\"/&gt;\n  &lt;/products&gt;\n\n  &lt;files&gt;\n    &lt;!-- The HTML panel, for 15.0 onwards --&gt;\n    &lt;file source=\"HTML/com.example.helloworld.zxp\"\n        destination=\"\"\n        file-type=\"CSXS\"\n        products=\"Photoshop,Photoshop32,Photoshop64\"\n        minVersion=\"15.0\" /&gt;\n\n    &lt;!-- The FLASH panel, for 13.0 to 14.9 --&gt;\n    &lt;file source=\"FLASH/com.example.helloworld.zxp\"\n        destination=\"\"\n        file-type=\"CSXS\"\n        products=\"Photoshop,Photoshop32,Photoshop64\"\n        minVersion=\"13.0\" maxVersion=\"14.9\" /&gt;\n\n    &lt;!-- Extension Manager icon --&gt;\n    &lt;file source=\"icon.png\" destination=\"$ExtensionSpecificEMStore\" /&gt;\n  &lt;/files&gt;\n&lt;/macromedia-extension&gt;\n\nCouple of things to notice:\n\n\n  In the product tag, the version is actually the minimum version supported.\n  When the file-type=\"CSXS\" there’s no need to specify the destination.\n\n\nNow, fire the Terminal and:\n\njava -jar ucf.jar -package -storetype PKCS12 -keystore ./selfDB.p12 -tsa http://timestamp.entrust.net/TSS/JavaHttpTS com.sample.helloworld.zxp -C \"./MXI/\" .\n\nIt will ask you for the p12 password, and eventually the com.sample.helloworld.zxp file is built. Some other notes:\n\n\n  You might need to download Java from Oracle\n  I’ve changed the timestamp url (that’s another option)\n  There’s a final dot you have to keep!\n\n\nYou may ask why the ucf.jar and not ZXPSignCmd: the reason is that, in order to be CS6 compatible, ucf.jar is the tool of choice.\n\n4. Pack and Sign Hybrid Extensions\n\n\n\nHybrid extensions let you install extra content, which can be platform (Mac/Win) and/or version (CS6/CC) and/or bit version (32/64bit) targeted: either Plugins, PDFs, etc. In fact the Flash+HTML extension above qualifies as an Hybrid extension too. In this example I will bundle a lot of extra content - keep in mind that all the paths are relative to the MXI file location. For the completeness sake, I will then use the ZXPSignCmd since we will pretend that this one is a CC only Extension.\n\n&lt;files&gt;\n&lt;!-- The HTML panel, for 15.0 onwards --&gt;\n&lt;file source=\"HTML-cs-extensions/com.example.helloworld.zxp\"\n      destination=\"\"\n      file-type=\"CSXS\"\n      products=\"Photoshop,Photoshop32,Photoshop64\"\n      minVersion=\"15.0\" /&gt;\n\n&lt;!-- the entire DOCS/ folder content --&gt;\n&lt;file source=\"DOCS/\"\n      destination=\"$userhomefolder/Documents/HelloWorld_DOCS/\"\n      file-type=\"ordinary\"\n      products=\"Photoshop,Photoshop32,Photoshop64\" /&gt;\n\n&lt;!-- JSX file goes in the Photoshop/Presets/Scripts folder --&gt;\n&lt;file source=\"SCRIPTS/\"\n      destination=\"$scripts/HelloWorldFolder\"\n      file-type=\"ordinary\"\n      products=\"Photoshop,Photoshop32,Photoshop64\" /&gt;\n\n&lt;!-- MAC plugin --&gt;\n&lt;file source=\"MAC/\"\n      destination=\"$pluginsfolder/HelloWorldFolder\"\n      file-type=\"ordinary\"\n      products=\"Photoshop,Photoshop32,Photoshop64\"\n      platform=\"mac\"/&gt;\n\n&lt;!-- WIN plugin, 64bit --&gt;\n&lt;file source=\"WIN/\"\n      destination=\"$pluginsfolder/HelloWorldFolder\"\n      file-type=\"ordinary\"\n      products=\"Photoshop,Photoshop64\"\n      platform=\"win\"/&gt;\n\n&lt;!-- WIN plugin, 32bit --&gt;\n&lt;file source=\"WIN/\"\n      destination=\"$pluginsfolder/HelloWorldFolder\"\n      file-type=\"ordinary\"\n      products=\"Photoshop,Photoshop64\"\n      platform=\"win\"/&gt;\n\n&lt;!-- Extension Manager icon --&gt;\n&lt;file source=\"icon.png\"\n      destination=\"$ExtensionSpecificEMStore\" /&gt;\n&lt;/files&gt;\n\nThen you have to fire the Terminal and as usual:\n\n./ZXPSignCmd -sign MXI com.sample.helloworld.zxp selfDB.p12 OcaMorta -tsa https://timestamp.geotrust.com/tsa\n\nThat should hopefully create a ZXP, deploying:\n\n\n  The HTML Panel\n  General assets (in my case a PDF and an HTML file) in a User’s Home subfolder\n  C/C++ Plugins (targeting Mac/Win versions, 32 and 64 bits)\n  JSX Scripts in the Photoshop CC/Presets/Scripts custom named subfolder\n\n\nPlease refer to this page for a description of General and Product Specific Path Tokens.\n\nResources\n\nThis brief guide is far from being exhaustive - I’ve only described things that I know first hand. For a better understanding of the topic, please refer to the following official documentation:\n\n\n  ZXPSignCmd download\n  ZXPSignCmd - Instruction\n  ZXPSignCmd - Packaging and Signing Technical Note\n  ucf.jar download\n  ucf.jar - Packaging instructions\n  ucf.jar - Technical Note (see from pag. 23 onwards)\n  MXI Reference (with path tokens, etc)\n  EULA Template\n\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 48
    });
    

    index.add({
      title: "VitaminBW 1.0 &#8211; User Manual",
      category: ["Photoshop"],
      content: "This is the User Manual for VitaminBW 1.0, the Photoshop CC/CS6 script that spices up your black and white conversions! Vitamin BW is for sale on the CC-Extensions for about € 26.00 / $ 34.95 (fluctuations due to the currency change) and by now is supported on Photoshop CC, CS6 (either Perpetual or Subscription), on both Mac and Windows.\n\nInterface\n\nLet’s have a look at the important features of VitaminBW - it’s a matter of selecting few options and start the processing.\n\n\n\n\n\n1. Conversion\n\nThis is where VitaminBW applies its peculiar routine: you can select the Default conversion checkbox, or try the Boost option for a bit of extra punch.\n\n\n\n2. BW Filters\n\nSimulating actual colored filters in BW Photography, to tweak the tonal rendering. See here for a conversion table.\n\n\n\n3. Tint\n\nBesides the Neutral option, you can choose between Warm, Cool and Mixed (i.e. warm in the highlights and cool in the shadows).\n\n4. Preview\n\nVitaminBW provides you with two preview images (Harbor, Woman) you can test the different Conversion, BW Filters and Tint combinations against. The Preview checkbox toggles the original color version.\n\n5. Process - Single Tone\n\n\n\nThe Single Tone button starts the processing - that is, the conversion to black and white according to the VitaminBW peculiar routine, BW Filtering and toning. As a result you get a Group in the Layers palette, containing from top to bottom:\n\n\n  Tint layer\n  Conversion layer\n  Flat layer\n\n\nAll these layers are kept to give you some extra control over the result (nonetheless, if you’re OK with it just flatten the Group and call quit). Let’s review how you can tweak them! Mind you, the following steps are totally optional.\n\nTint layer\n\nThis is what affects the toning (Neutral, Cool, Warm, Mixed). The layer is in Color blending mode, 50% opacity so there’s room to either: saturate the result, raising the layer’s opacity; making it more subtle, lowering the layer’s opacity.\n\nConversion layer\n\nContains the peculiar VitaminBW conversion routine, and it’s provided with a blank layer mask. You can paint black in the layer mask to erase the contrast effect if needed - for instance in case the Boost option has introduced some unwanted halos.\n\nFlat layer\n\nThis is a dull conversion, part of which is revealed only if you paint black the Conversion layer mask. Most of the time you can safely ignore it.\n\n6. Triple Tone\n\nIf you’re inclined to experimentation try the Triple Tone, it’ll be fun! What this option does is to run three times the routine, applying the Conversion and Tint you’ve chosen, but selecting different BW Filters: one time Normal, one time Blue, one time Orange. As a result you end up with three VitaminBW layer groups - which different tonal rendering you can apply enabling their layer masks and painting in. The following illustration opens the first time you run Triple Tone.\n\n\n\nTrue, you can always perform the same three-times routine manually with custom filters, but usually the Normal/Orange/Blue combination is very effective and it was worth implementing as a single button.\n\nBuy here!\n\nVitamin BW is for sale on the CC-Extensions for about € 26.00 / $ 34.95 (fluctuations due to the currency change) and by now is supported on Photoshop CC, CS6 (either Perpetual or Subscription), on both Mac and Windows.\n",
      tags: ["VitaminBW"],
      id: 49
    });
    

    index.add({
      title: "Introducing Vitamin BW for Photoshop CC / CS6",
      category: ["Photoshop"],
      content: "Give a fresh breath to your Black &amp; White conversions with Vitamin BW, a script for Photoshop CC / CS6 featuring Smart Contrast Enhancement, Toning and BW Color Filters!\n\n\n\t\n\tTop left, clockwise: Color Original, default Photoshop conversion, Vitamin BW (Contrast: More; Toning: Cool; BW Filter: Orange). Vitamin BW (Contrast: More; Toning: Cool; BW Filter: Orange)\n\n\nAt a glance\n\n\n\nVitamin BW idea is to give you few but powerful options for Black &amp; White conversion in three main areas - Contrast, Toning and BW Color Filters.\n\nSmart Contrast Enhancement\n\nVitamin BW features a very effective routine to boost the 3D look of your images: you can choose from the Default parameter, or Boost (for a bit of extra punch).\n\n\n\nToning\n\nBesides the default Neutral option, you can have a Cool and Warm toning, or an extra Mixed version which uses the Cool tone in the Shadows and Warm in the Highlights. The toning is on a separate layer that you can always switch on/off, or lower/raise its opacity for a milder/stronger effect.\n\n\n\nBW Color Filters\n\nWay way back in time, thousands of years ago, people of my age used to shoot BW film putting some colored glass in front of the lens, like Yellow, Orange, Red, Green or Blue filters. We read in papyrus rolls that this way some light’s wavelengths were blocked: provided you knew your Egyptian, light theory and remember to bring filters with you in the field, you sometimes ended up with better pictures using the right filter on the right subject.\n\n[\n\nThe following chart comes from PhotographyMad (which features an introductory tutorial on Color Filters for Black &amp; White): \n\nHow much?\n\nVitaminBW 1.0 by Davide Barranca is for sale on CC-Extensions for about €26.00 / $34.95 (fluctuations due to the currency change) and by now is a script for Photoshop CC, CS6 (either Perpetual or Subscription), on both Mac and Windows.\n\nUser Manual\n\nSee this post for examples and more detailed descriptions!\n",
      tags: ["Vitamin BW"],
      id: 50
    });
    

    index.add({
      title: "Epson 11880 printer head deep cleaning",
      category: ["Photoshop"],
      content: "I’ve been printing large format with an Epson 11880 for some years now, and I know when it’s time to get tough. That’s my take on “Ghetto Printer Head Deep Cleaning”\n\nAbout in 2001 a friend of mine and I founded a company dedicated to fineart BW printing with Carbon based inks. We had first a Canon BJC-8500 with removable, disposable printheads (god bless Canon), then an Epson 7000 - both of them “converted” to BW inks (several shades of gray - stuff you don’t want to leave stains with on your hands and clothes). Fun times. I was in charge of all the technical stuff, from Photoshop to the printers.\n\nBack then I learned how to hack printers and replace components - carbon inks were kind of harmful for printheads and printhead replacement quite expensive if done by an Epson technician. So I did manage to find a service manual and learned how to do it by myself. That is to say that I’m used to think about printers in terms of what they actually are (a psychotic crooked mix of tubes, electronics, etc) rather than a black box.\n\nProblem\n\nI’ve been printing with an Epson 11880 for about 7 years now and lately I’ve got this kind of issues, appearing randomly on the edge of rolls paper:\n\n\n\nwhich is kind of annoying because that was a 64” x 84” exhibition print, and I hate acrylic retouching even though I’ve to do it (after all, chemical prints were retouched too). This problem got worse and we’ve had to trash few prints.\n\nDiagnosis\n\nI had to inspect the print head: the ghetto way is to perform some kind of action - like a nozzle check - and brutally unplug the power cord. The head is then free to move back and forth (and not locked in the parking position) so if you release the paper lever you can look from above in the slit where paper is usually fed and see whether the print head has some kind of hair or dust or whatever that might touch the surface while printing.\n\nThat was the case. Since I use Hahnemuhle rag based paper, the cutting process produces lots of powder, which accumulates in the printer and eventually can form some kind of mixture with ink and ambient dust.\n\nIn particular, the head had a couple of sort of hairs, which bended when I moved the print head. It’s like what happens when you’re in a car that accelerates: you lean back, then the car slows down and you lean forward. When the print head changed direction at the roll’s edge, the “hairs” moved too and hit the paper leaving stains - you nasty hairs.\n\nSolution\n\nHead cleaning the hard way!\n\nThe toolset includes a metal ruler, some thin tissue (the less hairy, the better) and a cleaning liquid - both tissue and spray I’ve used were originally for LCD screens.\n\n\n\nAs you see, it’s a matter of wrapping the ruler with the tissue (make it quite soft, three rounds would go) using some tape to fix it, and spray generously your new tool with the cleaning liquid.\n\nNext, while the printer is switched off and the head is free to move, insert the ruler as you would with a sheet of paper (see below) and move the print head over it one time - yes, I know, but it deserves it.\n\n\n\nRemove the ruler (pay attention not to rub the inked tissue against the plastic wheels below) and repeat two or three times with a new tissue.\n\n\n\nActually I’ve had to make two tools like this one to use them as sort of a pair of pliers - in order to pick out some dirt that seemed to stick on the head border - neat trick that worked ;-) Further inspection on the head surface revealed no more dirt. Since I was in a “desperate houseprinter” mood I decided it was time for some spring cleaning, because there still was some dirt accumulating in there.\n\nThe gray rolls that keep the paper pressed on the platen are a good candidate for some washing too. Here it is how I did with a vintage floral tissue (and the same liquid, generously poured on it), rub it up and down.\n\n\n\nNext up, the parking area - which is where the print head parks when you shut it down and where the pump sucks the ink to clean the nozzles. That’s the why is one of the dirtiest place in the printer - I’ve inspected it with a torch and found a nice mess.\n\nWith the usual clean tissue and liquid (and a pair of gloves) I’ve removed lots of rubbish - the kind of which you’ve see in the tweezers picture.\n\nThat’s basically it - of course when you’re done plug back the power cable and let the printer complain - the nozzle check will be a disaster but after a round or two of cleaning it’ll be ok again. I won’t recommend this kind of routine unless strictly necessary, but I’ve found that the Epson 11880 bears such a torture quite well and it’s a miracle cure when everything else fails!\n",
      tags: ["Epson"],
      id: 51
    });
    

    index.add({
      title: "HTML Panels Tips: #9 Persistence",
      category: ["CEP"],
      content: "How do you make the HTML Panel’s session persist even if the panel is closed in Photoshop? Each time an HTML Panel is launched, its GUI resets to the default values and initialization is performed. This happens not only in between Photoshop sessions, but also when the Panel is collapsed, iconized or closed. Today’s tip shows you how to make the Panel persistent, as we did back in Flash land with:\n\n// In Flash panels you would:\nCSXSInterface.instance.evalScript(\"PhotoshopPersistent\");\n\nThe mechanism now in HTML land requires Event dispatching - see Tip #7: Photoshop Events Take 1 for the details.\n\nCode on GitHub\n\nPlease find here the complete code for this example.\n\nExample\n\n\n\nThis panel has a switch that enables Panel’s persistence, and a bunch of other switches that have no effect other than show persistence in action. When the Persistent is ON, you can play with the other ones and collapse/close then reopen the panel: the secondary switches are shown in the state they were before. Conversely, if Persistent is OFF, each time the panel is opened all the switches are reset.\n\nHTML\n\nI make use of the Topcoat CSS Library (refer to Tip #6 for the implementation).\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n  &lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\" /&gt;\n  &lt;link id=\"theme\" rel=\"stylesheet\" href=\"css/light.css\" /&gt;\n  &lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n\n  &lt;div id=\"container\"&gt;\n    &lt;h3 class=\"center\"&gt;Panel Persistence&lt;/h3&gt;\n    &lt;label class=\"switch-label\"&gt;Persistent:&lt;/label&gt;\n    &lt;label class=\"topcoat-switch\"&gt;\n      &lt;input id=\"persistenceSwitch\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n      &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n    &lt;/label&gt;\n    &lt;hr&gt;\n    &lt;div class=\"flex-container\"&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch1\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch2\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch3\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch4\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch5\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch6\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch7\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch8\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n      &lt;label class=\"topcoat-switch switch\"&gt;\n        &lt;input id=\"testSwitch9\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n        &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n      &lt;/label&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n\nJavascript\n\nThe main.js file is where things happen. A Persistent() function is called every time the persistenceSwitch changes: it sets up and dispatches a CSEvent (see the highlighted lines), which is bound to the extension’s id (gExtensionId variable).\n\n(function() {\n  'use strict';\n\n  var csInterface = new CSInterface();\n  var gExtensionId = \"com.example.persistent\";\n\n  function Persistent(inOn) {\n\n    if (inOn) {\n      var event = new CSEvent(\"com.adobe.PhotoshopPersistent\", \"APPLICATION\");\n    } else {\n      var event = new CSEvent(\"com.adobe.PhotoshopUnPersistent\", \"APPLICATION\");\n    }\n    event.extensionId = gExtensionId;\n    csInterface.dispatchEvent(event);\n  }\n\n  function init() {\n\n    themeManager.init();\n\n    $('#persistenceSwitch').change(function() {\n      Persistent($(this).is(':checked'));\n    });\n  }\n\n  init();\n\n}());\n\nThis makes the Panel persistent or… “unpersistent”.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 52
    });
    

    index.add({
      title: "HTML Panels Tips: #8 Photoshop Events, Take 2",
      category: ["CEP"],
      content: "There’s a second way to listen to Photoshop Events from an HTML Panel - i.e. using the PSHostAdapters libraries - and today’s tip will show you how. In Tip #7 I’ve demoed how using PhotoshopRegisterEvent let you listen to events and register callbacks: but that is just one way out of the two available (as it was back in the Flash land) so let’s find the second one.\n\n\n\nExample\n\nI’ll be using as a starting point the very same Tip #7 code. This example panel has a switch that enables a listener on the cut, copy and paste events in Photoshop - the callback passes the corresponding StringID for displaying to the Text Area. Mind you, when an HTML Panel is open keyboard shortcuts might not work, so you’d be better off using the menus in order to trigger the events correctly - Adobe engineers are aware and will fix that in the future.\n\nLibraries\n\nYou need two PSHostAdapter libraries, that you can grab from the Extension Builder installation (in the Eclipse application): they are ps_host_adapter-2.0.js and (depending on your OS) PSHostAdapter.plugin or PSHostAdapter.plugin.8li. The following paths come from my OSX installation, for PC users it shouldn’t be any different - just use the Eclipse application as the root:\n\n/Applications/eclipse/plugins/com.adobe.cside.html.libsinstaller_1.0.0.201307260955/archive/jsar-1.0/release/ps_host_adapter-2.0.js\n\n/Applications/eclipse/plugins/com.adobe.cside.html.libsinstaller_1.0.0.201307260955/archive/csadapters-3.0/ps_host_adapter/\n\nYou should copy the ps_host_adapter-2.0.js somewhere in your project and include it in the index.html. Conversely, PSHostAdapter.plugin or PSHostAdapter.plugin.8li file must go in the Photoshop Plug-Ins folder (Plug-Ins/Automate/ works just fine on my OSX installation).\n\nHTML\n\nIt’s basically the same of Tip #7, plus the new linked script. As you see, I’m using the Topcoat CSS library to style the panel GUI (refer to Tip #6 for detailed instruction).\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n  &lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\" /&gt;\n  &lt;link id=\"theme\" rel=\"stylesheet\" href=\"css/light.css\" /&gt;\n  &lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n\n  &lt;div style=\"width: 80%; margin:0 auto\"&gt;\n  &lt;h3 class=\"center\"&gt;PS Events&lt;/h3&gt;\n  &lt;label class=\"topcoat-switch\"&gt;\n  &lt;input id=\"registerEvent\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n  &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n  &lt;/label&gt;\n  &lt;input type=\"text\" id=\"result\" class=\"topcoat-text-input\" style=\"margin-left:10px\" placeholder=\"Listen for Events\" value=\"\"&gt;\n  &lt;/div&gt;\n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/ps\\_host\\_adapter-2.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n\n Javascript\n\nCompared to Photoshop Events, Take 1 the Register() function is where things are different - the rest of the code is unchanged. It’s a matter of adding an Event Listener to a PSEventAdapter Instance, passing either a constant (see in ps_host_adapter-2.0.js for a list of the available ones) or an event TypeID, which you can grab as usual from the Scripting Listener log. The PSCallback() function communicates then with the JSX the event.type in order to retrieve the StringID, which is passed to the Text Area.\n\n(function() {\n  'use strict';\n\n  var csInterface = new CSInterface();\n\n  function Register(inOn) {\n\n    // Events from ps\\_host\\_adapter-2.0.js\n    // PSEvent.CUT = \"1668641824\";\n    // PSEvent.COPY = \"1668247673\";\n    // PSEvent.PASTE = \"1885434740\";\n    if (inOn) {\n      PSEventAdapter.getInstance().addEventListener(PSEvent.CUT, PSCallback);\n      PSEventAdapter.getInstance().addEventListener(PSEvent.COPY, PSCallback);\n      PSEventAdapter.getInstance().addEventListener(PSEvent.PASTE, PSCallback);\n    } else {\n      PSEventAdapter.getInstance().removeEventListener(PSEvent.CUT, PSCallback);\n      PSEventAdapter.getInstance().removeEventListener(PSEvent.COPY, PSCallback);\n      PSEventAdapter.getInstance().removeEventListener(PSEvent.PASTE, PSCallback);\n    }\n  }\n\n  function init() {\n\n    themeManager.init();\n\n    $('#registerEvent').change(function() {\n      Register($(this).is(':checked'));\n    });\n  }\n\n  function PSCallback(csEvent) {\n    // send to JSX to convert typeIDs to stringIDs\n    csInterface.evalScript('convertTypeID(' + JSON.stringify(csEvent.type) + ')', function(res) {\n      $('#result').val(res.toString());\n    });\n  }\n\n  init();\n\n}());\n\nWhich is similar to the code for Flash panels in Extension Builder 2:\n\nPSEventAdapter.getInstance().addEventListener(PSEvent.ROTATE, handleEvent);\nprivate function handleEvent(event:PSEvent):void\n{\n  //handle rotate event\n}\n\n JSX\n\nJust a dedicate function here:\n\nfunction convertTypeID (tID) {\n  return typeIDToStringID(Number(tID));\n}\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 53
    });
    

    index.add({
      title: "HTML Panels Tips: #7 Photoshop Events, Take 1",
      category: ["CEP"],
      content: "This tip shows you the first way to make an HTML Panel listen and react to Photoshop Events: via PhotoshopRegisterEvent. Back in the Flash land, there were two ways to register callbacks for Photoshop Events: using either a PSHosttAdapter library, or an ExternalInterface object and PhotoshopCallback. Today’s tip is about the latter (see Tip #8 for the PSHostAdapter libraries). What we used to write in ActionScript was something like:\n\n// Set TypeID for desired events\nprivate const COPY_INT:int = Photoshop.app.charIDToTypeID(’copy’);\nprivate const PASTE_INT:int = Photoshop.app.charIDToTypeID(’past’);\n\n// Register for events\nCSInterface.PhotoshopRegisterEvent(COPY_INT + \",\" + PASTE_INT);\n// attach a callback\nExternalInterface.addCallback(\"PhotoshopCallback\" + CSInterface.getExtensionId(), myPhotoshopCallback);\n// Define the callback\nprivate function myPhotoshopCallback(eventID:Number, descID:Number):void {\n//\n}\n\nThings are slightly different now in Javascript. and are based on CSInterface and the dispatching of a custom Event. As follows the mechanism in a nutshell, then I’ll show you an actual example:\n\nvar csInterface = new CSInterface();\n\n// Create a new Event\nvar event = new CSEvent(\"com.adobe.PhotoshopRegisterEvent\", \"APPLICATION\");\n\n// Set Event properties: extension id\nevent.extensionId = \"com.example.psevents\";\n\n// Set Event properties: data (as Type ID string, comma separated)\n// 1668247673 = charIDToTypeID( \"copy\" ) = copy\n// 1885434740 = charIDToTypeID( \"past\" ) = paste\n// 1668641824 = charIDToTypeID( \"cut \" ) = cut\nevent.data = \"1668247673, 1885434740, 1668641824\";\n\n// Dispatch the Event\ncsInterface.dispatchEvent(event);\n\n// Attach a callback\ncsInterface.addEventListener(\"PhotoshopCallback\", PSCallback);\n\n// Define the callback\nfunction PSCallback(csEvent) {\n  // ...\n}\n\nExample\n\n\n\nThis Panel has a switch to turn ON/OFF the listener for few PS Events (cut, copy, paste). When the listener is active and the user cuts, copies or pastes, the Event’s StringID is written in the text area. In case you need a refresh of HTML Panels related topics to go through this one, here’s the whole list. You might need a refresh because this example uses Topcoat CSS (see HTML Panels Tip #6) and communication between HTML and JSX (Tip #4).\n\nHTML\n\nThe Panel uses boilerplate code by David Deraedt with the addition of Topcoat CSS (refer to HTML Panels Tip #6 for the details). I’ve set up a Topcoat Switch and a Text Area:\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n  &lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\" /&gt;\n  &lt;link id=\"theme\" rel=\"stylesheet\" href=\"css/light.css\" /&gt;\n  &lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n  &lt;div style=\"width: 80%; margin:0 auto\"&gt;\n    &lt;h3 class=\"center\"&gt;PS Events&lt;/h3&gt;\n    &lt;label class=\"topcoat-switch\"&gt;\n      &lt;input id=\"registerEvent\" type=\"checkbox\" class=\"topcoat-switch__input\"&gt;\n      &lt;div class=\"topcoat-switch__toggle\"&gt;&lt;/div&gt;\n    &lt;/label&gt;\n    &lt;input type=\"text\" id=\"result\" class=\"topcoat-text-input\" style=\"margin-left:10px\" placeholder=\"Listen for Events\" value=\"\"&gt;\n  &lt;/div&gt;\n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n\nJavascript\n\nIn main.js it’s defined the Register() function that attaches (or remove) the listener, dispatching a CSEvent as you’ve already seen. A jQuery function binds the switch to Register() and finally a PSCallback function is defined: there, the Event data String is split and passed for evaluation to JSX (see Tip #4 for details about HTML to JSX data exchange). The evalScript callback (that is, the JSX return value) is displayed in the Text Area.\n\n(function() {\n  'use strict';\n\n  var csInterface = new CSInterface();\n\n  function Register(inOn) {\n\n    if (inOn) {\n      var event = new CSEvent(\"com.adobe.PhotoshopRegisterEvent\", \"APPLICATION\");\n    } else {\n      var event = new CSEvent(\"com.adobe.PhotoshopUnRegisterEvent\", \"APPLICATION\");\n    }\n    event.extensionId = \"com.example.psevents\";\n\n    // some events:\n    // 1668247673 = charIDToTypeID( \"copy\" ) = copy\n    // 1885434740 = charIDToTypeID( \"past\" ) = paste\n    // 1668641824 = charIDToTypeID( \"cut \" ) = cut\n    event.data = \"1668247673, 1885434740, 1668641824\";\n    csInterface.dispatchEvent(event);\n  }\n\n  function init() {\n\n    themeManager.init();\n\n    // Switch onChange callback\n    $('#registerEvent').change(function() {\n      Register($(this).is(':checked')); // true or false\n    });\n  }\n\n  function PSCallback(csEvent) {\n    var dataArray = csEvent.data.split(\",\");\n    // send to JSX to convert typeIDs to stringIDs\n    csInterface.evalScript('convertTypeID(' + JSON.stringify(dataArray\\[0\\]) + ')', function(res) {\n      $('#result').val(res.toString());\n    });\n  }\n\n  init();\n  csInterface.addEventListener(\"PhotoshopCallback\", PSCallback);\n\n}());\n\nJSX\n\nIn the JSX file there’s just a function, that converts the TypeID to StringID and returns it.\n\nfunction convertTypeID (typeArray) {\n  return typeIDToStringID(Number(typeArray));\n}\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 54
    });
    

    index.add({
      title: "HTML Panels Tips: #6 integrating Topcoat CSS",
      category: ["CEP"],
      content: "Today’s tip is about integration of the Topcoat CSS library with themeManager.js in Photoshop HTML Panels. Adobe’s Topcoat is:\n\n\n  a brand new open source CSS library designed to help developers build web apps with an emphasis on speed. It evolved from the Adobe design language developed for Brackets, Edge Reflow, and feedback from the PhoneGap app developer community.\n\n\nIt comes with Desktop and Mobile themes, both in Light of Dark shades - a really neat design. HTML Panels have sort of a built-in system to keep in sync with Photoshop interface (four shades, from light gray to black). I’ve said “sort of” because you need to explicitly set the functions, but they’re provided by default by both Extension Builder and boilerplate code by David Deraedt. Basically it’s a matter of triggering some styling when the PS Interface changes - you do this listening for the CSInterface.THEME_COLOR_CHANGED_EVENT:\n\n// themeManager.js\n\nfunction updateThemeWithAppSkinInfo(appSkinInfo) {\n  // ... All styling stuff here\n}\n\nfunction onAppThemeColorChanged(event) {\n  var skinInfo = JSON.parse(window.__adobe_cep__.getHostEnvironment()).appSkinInfo;\n  updateThemeWithAppSkinInfo(skinInfo);\n}\n\nfunction init() {\n  var csInterface = new CSInterface();\n  updateThemeWithAppSkinInfo(csInterface.hostEnvironment.appSkinInfo);\n  csInterface.addEventListener(CSInterface.THEME_COLOR_CHANGED_EVENT, onAppThemeColorChanged);\n}\n\n\n\nThe above code first runs updateThemeWithAppSkinInfo(), (the function that performs the actual styling - changes CSS rules, etc)  passing as a parameter an appSkinInfo object. It contains various host environment (i.e. Photoshop) information, as the screenshot from the Chrome Developer Tools console shows on the right. Depending on these values, you might decide for instance to change the background of your HTML Panel to match the Photoshop own panelBackgroundColor (or font size, color, whatever).\n\nPlease note: Topcoat already comes in two flavors (Light and Dark) so you can always decide to switch to the corresponding CSS depending on the Photoshop’s interface color. You’d pick Topcoat’s Light for the PS LigthGray and MidGray, and Topcoat’s Dark for the PS DarkGray and Black interface. I personally don’t like this option. Topcoat’s background color doesn’t match with Photoshop (I admit I’m a bit picky on this - after all we’re adapting a library that’s not been built for this very purpose). Let’s call 1, 2, 3, 4 the PS interface shades: Topcoat Dark is too light for 1 and too dark for 2, while Topcoat Light is too light for 3 and too dark for 4. I’ll show you in the following example how to better integrate the two systems.\n\nExample\n\nAs I’m compiling these tips while I discover new things working on actual projects, so I’ll show you as the example a simple panel I’m currently porting to HTML (called PS Projects).\n\nHTML\n\nThe code here is really basic - I don’t need anything fancy but three buttons: I’ve set a Large Button Bar using a snippet from Topcoat examples. As you can see, I’ve set IDs for the stylesheets - the reason is going to be clear soon.\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\"&gt;\n&lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\"/&gt;\n&lt;link id=\"theme\" rel=\"stylesheet\" href=\"css/light.css\"/&gt;\n&lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h3 class=\"center\"&gt;PS Projects 2.0&lt;/h3&gt;\n  &lt;div class=\"topcoat-button-bar\" style=\"margin-left:20px\"&gt;\n    &lt;div class=\"topcoat-button-bar__item\"&gt;\n      &lt;button id=\"create\" class=\"topcoat-button-bar__button--large\"&gt;Create New&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;div class=\"topcoat-button-bar__item\"&gt;\n      &lt;button id=\"load\" class=\"topcoat-button-bar__button--large\"&gt;Load...&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;div class=\"topcoat-button-bar__item\"&gt;\n      &lt;button id=\"modify\" class=\"topcoat-button-bar__button--large\"&gt;Modify...&lt;/button&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;          \n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\nJavascript\n\nThe main.js contains (besides stuff I need for JSX operations - the actual routines of my panel) a call to themeManager.init(). The themeManager.js is as follows:\n\nvar themeManager = (function () {\n  'use strict';\n\n  //Convert the Color object to string in hexadecimal format;\n  function toHex(color, delta) {\n    function computeValue(value, delta) {\n      var computedValue = !isNaN(delta) ? value + delta : value;\n      if (computedValue &lt; 0) {\n        computedValue = 0;\n      } else if (computedValue &gt; 255) {\n        computedValue = 255;\n      }            \n      computedValue = Math.floor(computedValue);\n      computedValue = computedValue.toString(16);\n      return computedValue.length === 1 ? \"0\" + computedValue : computedValue;\n    }\n    var hex = \"\";\n    if (color) {\n      hex = computeValue(color.red, delta) + computeValue(color.green, delta) + computeValue(color.blue, delta);\n    }\n    return hex;\n  }\n\n  function addRule(stylesheetId, selector, rule) {\n    var stylesheet = document.getElementById(stylesheetId);   \n    if (stylesheet) {\n      stylesheet = stylesheet.sheet;\n      if (stylesheet.addRule) {\n        stylesheet.addRule(selector, rule);\n      } else if (stylesheet.insertRule) {\n        stylesheet.insertRule(selector + ' { ' + rule + ' }', stylesheet.cssRules.length);\n      }\n    }\n  }\n\n  //Update the theme with the AppSkinInfo retrieved from the host product.\n  function updateThemeWithAppSkinInfo(appSkinInfo) {\n    // console.log(appSkinInfo)\n    var panelBgColor = appSkinInfo.panelBackgroundColor.color;\n    var bgdColor = toHex(panelBgColor);\n    var fontColor = \"F0F0F0\";\n    if (panelBgColor.red &gt; 122) {\n      fontColor = \"000000\";\n    }\n\n    var styleId = \"hostStyle\";\n    addRule(styleId, \"body\", \"background-color:\" + \"#\" + bgdColor);\n    addRule(styleId, \"body\", \"color:\" + \"#\" + fontColor);\n\n    var isLight = appSkinInfo.panelBackgroundColor.color.red &gt;= 127;\n    if (isLight) {\n      $(\"#theme\").attr(\"href\", \"css/light.css\");\n    } else {\n      $(\"#theme\").attr(\"href\", \"css/dark.css\");\n    }\n  }\n\n  function onAppThemeColorChanged(event) {\n    var skinInfo = JSON.parse(window.__adobe_cep__.getHostEnvironment()).appSkinInfo;\n    updateThemeWithAppSkinInfo(skinInfo);\n  }\n\n  function init() {   \n    var csInterface = new CSInterface();\n    updateThemeWithAppSkinInfo(csInterface.hostEnvironment.appSkinInfo);\n    csInterface.addEventListener(CSInterface.THEME_COLOR_CHANGED_EVENT, onAppThemeColorChanged);\n  }\n\n  return {\n    init: init\n  };\n\n}());\n\nThe toHex() and addRule() are utility functions, while init() (orders the styling update and adds a theme change event listener) and onAppThemeColorChanged() (retrieves the AppSkinInfo object and pass it forwards) are the same as in the first code chunk, up in the page. updateThemeWithAppSkinInfo() extracts from the AppSkinInfo object the panels background color and set new background-color and color rules in the stylesheet with id=\"hostStyle\". Then, depending on the Red component of the background color (an arbitrary choice) it determines whether the interface isLight or not, and load a dark or light Topcoat CSS using jQuery.\n\nCSS\n\nThe HTML loads two CSS files, even if the panel deals with a total of three. The one with id=\"hostStyle\" is theme.css and contains  minor tweaks - it’s the target of the addRule() function, which will add there background-color and color rules depending on the PS interface theme (lightGray, darkGray, etc). Then there’s dark.css and light.css - which are the original topcoat-desktop-dark.css and topcoat-desktop-light.css renamed. In order not to conflict with styles set via themeManager.js I’ve commented out some bits in the body section of both of them:\n\n/* dark.css and light.css */\nbody {\n  margin: 0;\n  padding: 0;\n  /*background: #4b4d4e;\n  color: #000;*/\n  font: 16px \"Source Sans\", helvetica, arial, sans-serif;\n  font-weight: 400;\n}\n// ...\n\nAs a result I’ve made the two Topcoat Dark and Light versions synchronize with the exact background-color and color.\n\n\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 55
    });
    

    index.add({
      title: "HTML Panels Tips: #5 passing Objects from JSX to HTML",
      category: ["CEP"],
      content: "This tip covers how to pass Objects from Photoshop (the JSX file) to the HTML Panel. Panels technology doesn’t allow (for the moment) to communicate from the JSX to the HTML except via function return (see Tip #3). In order to successfully pass Objects, you need to include json2.js in the JSX and stringify the Object as a JSON Object. Then, in the HTML Panel you must parse the JSON Object in the callback.\n\nExample\n\nThis panel has just one button: it triggers a function in the JSX that returns an Object, which is then parsed and logged in the console. In order to follow the demo code, you’d better know how to debug a panel (Tip #1), evaluate external JSX  code (Tip #2) and have a good grasp on how primitive data is passed from JSX to HTML (Tip #3).\n\nHTML\n\nThe usual bare code stripped from boilerplate by David Deraedt.\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\"&gt;\n&lt;link rel=\"stylesheet\" href=\"css/styles.css\"/&gt;\n&lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\"/&gt;\n&lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div id=\"content\"&gt;\n    &lt;h1&gt;Object communication&lt;/h1&gt;\n    &lt;button id=\"btnSend\"&gt;JSX -&gt; HTML&lt;/button&gt;\n  &lt;/div&gt;\n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nMain.js\n\nIn the Javascript it is first declared the loadJSX() function (hardwired to look in the jsx/ folder), called in the init() in order to evaluate in the Photoshop context the json2.js library. Mind you, ExtendScript doesn’t support JSON natively so the original library must be included. I’ve used the commented version by Douglas Crockford, beware that minification in ExtendScript can be tricky. I’ve provided to the CSInterface.evalScript() a callback as an anonymous function, which parses the result as a JSON Object (see next section).\n\n(function() {\n  'use strict';\n  var csInterface = new CSInterface();\n\n  function loadJSX(fileName) {\n    var extensionRoot = csInterface.getSystemPath(SystemPath.EXTENSION) + \"/jsx/\";\n    csInterface.evalScript('$.evalFile(\"' + extensionRoot + fileName + '\")');\n  }\n\n  function init() {\n    themeManager.init();\n    loadJSX(\"json2.js\");\n\n    $(\"#btnSend\").click(function() {\n      csInterface.evalScript('sendObjToHTML()', function(result) {\n        var o = JSON.parse(result);\n        var o = result;\n        var str = \"\";\n        for (var prop in o) {\n          str += prop + \" \\[\" + typeof o\\[prop\\] + \"\\]: \" + o\\[prop\\] + \".\\\\n\";\n        }\n        console.log(str);\n      });\n    });\n  }\n\n  init();\n}());\n\nJSX\n\nHere the sendObjToHTML() is declared: it first builds an Object literal with arbitrary data (you’ll fill it with useful, actual stuff: Photoshop active Document’s width, height, ecc.); then the Object is returned as a JSON (stringified) Object. This very Object is the result argument in the anonymous callback in the main.js above.\n\nfunction sendObjToHTML() {\n  var obj = {\n    str: “Love your job!”,\n    num: 38,\n    today: new Date(),\n    nestedObj: {\n      nestedStr: “Even if nothing works”,\n      nestedNum: 8,\n      nestedDate: new Date()\n    }\n  }\n  return JSON.stringify(obj);\n}\n\nWhen the panel is open in Photoshop - provided you’ve set things according to Tip #1 - open Chrome (or Safari, or Firefox) and go to localhost:8088 or whatever port you’ve opened in the .debug file.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 56
    });
    

    index.add({
      title: "HTML Panels Tips: #4 passing Objects from HTML to JSX",
      category: ["CEP"],
      content: "How do you pass complex Objects from the HTML Panel to a Photoshop’s ExtendScript function in a JSX file? You may want to pack different information from the HTML Panel (for instance all the values in the GUI) in a single Object, and pass just it as a parameter to a JSX function - that the Photoshop interpreter will eventually execute. For primitive, multiple parameters this is easily done via CSInterface.evalScript(), in the main.js:\n\nvar csInterface = new CSInterface();\ncsInterface.evalScript('sayHello(\"Ciao\", 2014)');\n\n…and providing a corresponding function in the JSX file:\n\nfunction sayHello(str, num) {\n  alert(\"Photoshop says \" + str + \"! Have a great \" + num);\n}\n\nHow do you deal with Objects? Stringify them as JSON objects first. No need to parse them afterwards.\n\nExample\n\nI use as always the boilerplate code by David Deraedt.\n\nHTML\n\nThis is a minimal index.html with just a button.\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\"&gt;\n&lt;link rel=\"stylesheet\" href=\"css/styles.css\"/&gt;\n&lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\"/&gt;\n&lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div id=\"content\"&gt;        \n    &lt;h1&gt;Object communication&lt;/h1&gt;\n      &lt;button id=\"btnSend\"&gt;HTML -&gt; JSX&lt;/button&gt;\n  &lt;/div&gt;\n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nMain.js\n\nHere I’ve hardcoded an Object literal (feel free to fill it with actual values from the HTML GUI), which is stringified as a JSON object in order to pass it as a parameter:\n\n(function() {\n  'use strict';\n  var csInterface = new CSInterface();\n\n  function init() {\n    themeManager.init();\n    var obj = {\n      str: \"Love your job!\",\n      num: 38,\n      today: new Date(),\n      nestedObj: {\n        nestedStr: \"Even if nothing works\",\n        nestedNum: 8,\n        nestedDate: new Date()\n      }\n    }\n    $(\"#btnSend\").click(function() {\n      csInterface.evalScript('parseObj(' + JSON.stringify(obj) + ')');\n    });\n  }\n  init();\n}());\n\nJSX\n\nIn the ExtendScript file I’ve provided a logging function to actually verify that the Object is interpreted correctly.\n\nfunction parseObj(o) {\n  var str = \"\";\n    for (prop in o) {\n      str += prop + \" \\[\" + typeof o\\[prop\\] + \"\\]: \" + o\\[prop\\] + \".\\\\n\";\n    }\n  alert(str);\n}\n\n\n\nMind you: I’ve experimented importing a JSON library in the ExtendScript code (which doesn’t natively support it) and tried to var obj=JSON.parse(o); to no avail. Apparently the code is OK as it is, without it.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 57
    });
    

    index.add({
      title: "HTML Panels Tips: #3 Get data from JSX and send it to HTML",
      category: ["CEP"],
      content: "This is how Photoshop (i.e. the JSX) sends back data to the HTML Panel. The HTML Panel communicates with Photoshop via evalScript - this is in the main.js:\n\nvar csInterface = new CSInterface();\ncsInterface.evalScript('jsxFun()');\n\nIn the JSX there must be some:\n\nfunction jsxFun() {\n  // ...\n}\n\nIn order to enable double-way communication, provide a callback in the evalScript function:\n\nvar csInterface = new CSInterface();\ncsInterface.evalScript('jsxFun()', callback);\n// callback is a function defined somewhere (or anonymous)\n\nExample\n\n\n\nA very trivial example for a panel with one button that retrieves and displays in the panel the Photoshop active document’s name (forgive my total lack of design for this panel - it’s just to show you the bare mechanism) is as follows.\n\nHTML\n\nI define a button with id=\"btnDocName\", and an h3 tag which will contain the Doc’s name.\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n  &lt;link rel=\"stylesheet\" href=\"css/styles.css\" /&gt;\n  &lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\" /&gt;\n  &lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n\n  &lt;div id=\"content\"&gt;\n    &lt;h1&gt;JSX -&gt; HTML&lt;/h1&gt;\n    &lt;h3 id=\"docName\"&gt;&amp;nbsp;&lt;/h3&gt;\n    &lt;button id=\"btnDocName\"&gt;Retrieve Document Name&lt;/button&gt;\n  &lt;/div&gt;\n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n\n Main.js\n\nBesides themeManager (boilerplate code is by the great David Deraedt) you see I evaluate (i.e. call) a getDocName() function (which belongs to the JSX) ‘ve used an anonymous function to define a callback on the fly. The result argument is just the return value in the JSX, and it’s used to overwrite the h3 tag content in the Panel.\n\n(function() {\n  'use strict';\n  var csInterface = new CSInterface();\n\n  function init() {\n    themeManager.init();\n    $(\"#btnDocName\").click(function() {\n      csInterface.evalScript('getDocName()', function(result) {\n        document.getElementById(\"docName\").innerHTML = result;\n      });\n    });\n  }\n  init();\n}());\n\nJSX\n\nContains the getDocName() function and returns the active document name as a String (the toString() shouldn’t be mandatory):\n\nfunction getDocName(){\n  return app.documents.length ? app.activeDocument.name : \"No docs open!\";\n}\n\nSo as a result when you click the button on the HTML Panel, it evaluates the function in the JSX, which in turn passes its return value to the callback.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 58
    });
    

    index.add({
      title: "HTML Panels Tips: #3 Get data from JSX and send it to HTML",
      category: ["CEP"],
      content: "This is how Photoshop (i.e. the JSX) sends back data to the HTML Panel. The HTML Panel communicates with Photoshop via evalScript - this is in the main.js:\n\nvar csInterface = new CSInterface();\ncsInterface.evalScript('jsxFun()');\n\nIn the JSX there must be some:\n\nfunction jsxFun() {\n  // ...\n}\n\nIn order to enable double-way communication, provide a callback in the evalScript function:\n\nvar csInterface = new CSInterface();\ncsInterface.evalScript('jsxFun()', callback);\n// callback is a function defined somewhere (or anonymous)\n\nExample\n\n\n\nA very trivial example for a panel with one button that retrieves and displays in the panel the Photoshop active document’s name (forgive my total lack of design for this panel - it’s just to show you the bare mechanism) is as follows.\n\nHTML\n\nI define a button with id=\"btnDocName\", and an h3 tag which will contain the Doc’s name.\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"utf-8\"&gt;\n  &lt;link rel=\"stylesheet\" href=\"css/styles.css\" /&gt;\n  &lt;link id=\"hostStyle\" rel=\"stylesheet\" href=\"css/theme.css\" /&gt;\n  &lt;title&gt;&lt;/title&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n\n  &lt;div id=\"content\"&gt;\n    &lt;h1&gt;JSX -&gt; HTML&lt;/h1&gt;\n    &lt;h3 id=\"docName\"&gt;&amp;nbsp;&lt;/h3&gt;\n    &lt;button id=\"btnDocName\"&gt;Retrieve Document Name&lt;/button&gt;\n  &lt;/div&gt;\n\n  &lt;script src=\"js/libs/CSInterface-4.0.0.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/libs/jquery-2.0.2.min.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/themeManager.js\"&gt;&lt;/script&gt;\n  &lt;script src=\"js/main.js\"&gt;&lt;/script&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;\n\n Main.js\n\nBesides themeManager (boilerplate code is by the great David Deraedt) you see I evaluate (i.e. call) a getDocName() function (which belongs to the JSX) ‘ve used an anonymous function to define a callback on the fly. The result argument is just the return value in the JSX, and it’s used to overwrite the h3 tag content in the Panel.\n\n(function() {\n  'use strict';\n  var csInterface = new CSInterface();\n\n  function init() {\n    themeManager.init();\n    $(\"#btnDocName\").click(function() {\n      csInterface.evalScript('getDocName()', function(result) {\n        document.getElementById(\"docName\").innerHTML = result;\n      });\n    });\n  }\n  init();\n}());\n\nJSX\n\nContains the getDocName() function and returns the active document name as a String (the toString() shouldn’t be mandatory):\n\nfunction getDocName(){\n  return app.documents.length ? app.activeDocument.name : \"No docs open!\";\n}\n\nSo as a result when you click the button on the HTML Panel, it evaluates the function in the JSX, which in turn passes its return value to the callback.\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 59
    });
    

    index.add({
      title: "HTML Panels Tips: #2 Including multiple JSX",
      category: ["CEP"],
      content: "A simple way to dynamically evaluate multiple JSX files in the Scripting context of Photoshop within HTML Panels. Back in the Flash world, I used to dynamically include external JSX files via:\n\n$.evalFile(\"\" + (File($.fileName).path) + \"/libs/external.jsx\");\n\nBeing $.fileName the current JSX file within the above line is written and the File.path call meaning the actual path. Alas, this doesn’t seem to work anymore when HTML panels are involved - the File($.fileName).path once resulted to me, don’t ask me why, as “33” -  which is not a bad approximation to the answer to the Ultimate Question of Life, the Universe, and Everything after all ;-) Anyway, what I personally use now is the following function (in the main.js file):\n\n// fileName is a String (with the .jsx extension included)\nfunction loadJSX(fileName) {\n  var csInterface = new CSInterface();\n  var extensionRoot = csInterface.getSystemPath(SystemPath.EXTENSION) + \"/jsx/\";\n  csInterface.evalScript('$.evalFile(\"' + extensionRoot + fileName + '\")');\n}\n\nThe key here is the above highlighted line, which is the JS equivalent of the JSX line I’ve been using. To load a JSX - which must belong to the root/jsx folder since it’s hardwired in the function, you would write:\n\nloadJSX(\"myFile.jsx\");\n\nBorrowing code from the Extension Builder 3 boilerplate you can get fancier. The first chunk goes in the main JSX, i.e. the one listed in the manifest.xml, enclosed in the ScriptPath tag:\n\nif (typeof($) == 'undefined') $ = {};\n\n$._ext = {\n  //Evaluate a file and catch the exception.\n  evalFile: function(path) {\n    try {\n      $.evalFile(path);\n    } catch (e) {\n      alert(\"Exception:\" + e);\n    }\n  },\n  // Evaluate all the files in the given folder\n  evalFiles: function(jsxFolderPath) {\n    var folder = new Folder(jsxFolderPath);\n    if (folder.exists) {\n      var jsxFiles = folder.getFiles(\"*.jsx\");\n      for (var i = 0; i &lt; jsxFiles.length; i++) {\n        var jsxFile = jsxFiles\\[i\\];\n        $._ext.evalFile(jsxFile);\n      }\n    }\n  }\n};\n\nThis instead is what goes in the main.js\n\n/**\n * Load JSX file into the scripting context of the product. All the jsx files in\n * folder \\[ExtensionRoot\\]/jsx will be loaded.\n */\nfunction loadJSX() {\n  var csInterface = new CSInterface();\n  var extensionRoot = csInterface.getSystemPath(SystemPath.EXTENSION) + \"/jsx/\";\n  csInterface.evalScript('$._ext.evalFiles(\"' + extensionRoot + '\")');\n}\n\nloadJSX() (for instance called in the init() function) will evaluate all the JSX files in the predefined folder.\n\nBonus - System Paths\n\nYou may be interested in these ones too (from CSInterface-4.0.0.js)\n\n/** Identifies the path to user data.  */\nSystemPath.USER_DATA = \"userData\";\n\n/** Identifies the path to common files for Adobe applications.  */\nSystemPath.COMMON_FILES = \"commonFiles\";\t\t\t\n\n/** Identifies the path to the user's default document folder.  */\nSystemPath.MY_DOCUMENTS = \"myDocuments\";\n\n/** Identifies the path to current extension.  */\n/** DEPRECATED, PLEASE USE EXTENSION INSTEAD.  */\nSystemPath.APPLICATION = \"application\";\n\n/** Identifies the path to current extension.  */\nSystemPath.EXTENSION = \"extension\";\n\n/** Identifies the path to hosting application's executable.  */\nSystemPath.HOST_APPLICATION = \"hostApplication\";\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 60
    });
    

    index.add({
      title: "HTML Panels Tips: #1 Debugging",
      category: ["CEP"],
      content: "Since latest updates, Photoshop (and possibly other Adobe apps too) need remote connecting for debugging HTML Extensions - here’s a quick how-to.\n\n\n\nIn the root folder of your extension, create a blank .debug file (you may need to make your OS show hidden files - download here an Automator app for OSX that toggles the visibility on/off). This file needs to be formatted as an XML and should contain information about the ID of your extension, and the host/port (in the range 1024 - 65534) you want to connect through.\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;ExtensionList&gt;\n  &lt;Extension Id=\"com.example.communication\"&gt;\n    &lt;HostList&gt;\n\n      &lt;!-- Comment Host tags according to the apps you want your panel to support --&gt;\n\n      &lt;!-- Photoshop --&gt;\n      &lt;Host Name=\"PHXS\" Port=\"8088\"/&gt;\n\n      &lt;!-- Illustrator --&gt;\n      &lt;Host Name=\"ILST\" Port=\"8088\"/&gt;\n\n      &lt;!-- InDesign --&gt;\n      &lt;Host Name=\"IDSN\" Port=\"8088\"/&gt;\n\n      &lt;!-- Premiere --&gt;\n      &lt;Host Name=\"PPRO\" Port=\"8088\"/&gt;\n\n      &lt;!-- AfterEffects --&gt;\n      &lt;Host Name=\"AEFT\" Port=\"8088\"/&gt;\n\n      &lt;!-- PRELUDE --&gt;\n      &lt;Host Name=\"PRLD\" Port=\"8088\"/&gt;\n\n      &lt;!-- FLASH Pro --&gt;\n      &lt;Host Name=\"FLPR\" Port=\"8088\"/&gt;\n\n    &lt;/HostList&gt;\n  &lt;/Extension&gt;\n&lt;/ExtensionList&gt;\n\nIn my case the ID is com.example.communication and the port 8088.\n\n\n\nNow, while your Photoshop extension is open, point your browser (either Chrome, Safari, Firefox…) to http://localhost:8088 (of course use the port you’ve set in the XML file). You should see something like the screenshot on the left. If you click the index.html link the Chrome Developer Tools or the Safari/Firefox equivalent will open and you’ll be able to inspect your extension like any other webpage on the internet. Mind you, as far as I know this debug method doesn’t reach the JSX level - it’s just for the HTML panel and its JS; if you’re wondering a debugger; call in the JSX doesn’t fire up ESTK.\n\n\n\n\n\nThe Photoshop HTML Panels Development Course\n\n\n  \n\t\n\tUpdated to CC 2019.\n\n\nIf you’re reading here, you might be interested in my Photoshop Panels development – so let me inform you that I’ve authored a full course:\n\n\n  300 pages PDF\n  3 hours of HD screencasts\n  28 custom Panels with commented code\n\n\nCheck it out! Please help me spread the news – I’ll be able to keep producing more exclusive content on this blog. Thank you!\n\n",
      tags: ["HTML Panels Tips"],
      id: 61
    });
    

    index.add({
      title: "2014 Code Learning Wishlist (and All the Rest)",
      category: ["Coding","Photoshop"],
      content: "\n  \n    I&#8217;ve thought that &#8220;New Year Resolutions&#8221; sounds bad compared to &#8220;Wishlist&#8221; &#8211; doesn&#8217;t it? Just an excuse to post a bunch of links and inventory topics I&#8217;d like to spend time on in 2014. \n  \n\n  \n    Coding\n  \n\n  \n    Don&#8217;t you know Processing? Quoting from the official website:\n  \n\n  \n    \n      Processing is a programming language, development environment, and online community. Since 2001, Processing has promoted software literacy within the visual arts and visual literacy within technology.\n    \n  \n\n  \n    As a language, it is basically Java (yes, you&#8217;re allowed to raise your eyebrow), which is quite friendly &#8211; if you&#8217;re new to Computer Science, do a favor to yourself and follow the free Stanford&#8217;s Programming Methodology (Java based, 28 lectures plus handouts, assignments, etc) by Professor Mehran Sahami &#8211; a really entertaining teacher.\n  \n\n  \n    By the way, there is a project that ports Processing in the browser as Javascript code using both the canvas 2d and OpenGL contexts. As a basic introduction on Processing, see the Hour of Code video by Daniel Shiffman &#8211; he shows applications for Art installations, 3D modeling, etc. but tons of libraries make it suitable for a wide range of purposes. Here are some basic examples showcasing Processing features with (mostly) easy to follow code. Myself, I&#8217;m fond of Generative Art and Nature simulations (just FIY).\n  \n\n  \n    Speaking of which, I strongly recommend you The Nature of Code (self published by the very Daniel Shiffman), an ebook/printed book which headline says:\n  \n\n  \n    \n      How can we capture the unpredictable evolutionary and emergent properties of nature in software? How can understanding the mathematical principles behind our physical world help us to create digital worlds? This book focuses on the programming strategies and techniques behind computer simulations of natural systems using Processing.\n    \n  \n\n  \n    I&#8217;ve found myself excited as a kid following a birds flock simulation, or even really simple experiments where you lay down implement physics laws and watch your creatures exist and behave according to them. It is amazing how clean and short your code can be to realistically reproduce real world interaction (fluid viscosity, movement, etc).\n  \n\n  \n    No matter how simple  Processing makes it, you have to know some math. In case you feel a bit rusty, Keith Peters has just started a website called Coding Math where he&#8217;s publishing on a regular basis videos teaching you the bits of trigonometry, vectors &amp; C. you need to know in order to write code dealing with visual effects and objects simulation. He uses the HTML canvas element and Javascript, so you can lower the eyebrow that was raised since I wrote &#8220;Java&#8221; in a previous paragraph (it&#8217;s quite easy to port the code back and forth &#8211; apparently the Processing Foundation has been using Keith Peters code to showcase some features!).\n  \n\n  \n    There&#8217;s also a book from Keith Peters worth mentioning, titled &#8220;Playing with Chaos: Programming Fractals and Strange Attractors in Javascript&#8221;.\n  \n\n  \n    As an example of Generative Art, have a look at Mario Klingermann Incubator (left, a still from an endless animation), or Casey Reas &#8211; one of the Processing authors &#8211; amazing exhibition work (right).\n  \n\n  \n    Photoshop\n  \n\n  \n    I will continue to work on Photoshop extensibility &#8211; because this blog titles &#8220;Photoshop, Coding and all the rest&#8221; for a reason. As you might (or might not) know, around mid-2014 Photoshop will discontinue the support of Flex panels, which means that at some point I (and all the developers in the world willing to support the CC version &#8211; it isn&#8217;t clear whether CS6 will keep them or not), will have to port all my panels to HTML.\n  \n\n  \n    Luckily enough, all my paid products exist as Scripts (and not panels &#8211; just Advanced Local Contrast Enhancer provides you with both of them) so I&#8217;m not particularly pressed &#8211; I can wait for the technology to stabilize a bit. Script support shouldn&#8217;t break, unless Adobe want to kill +15 years of retro-compatibility in one snap.\n  \n\n  \n    Early 2014 I will release a new product called VitaminBW (guess what is it all about) and the Version 2 of PS-Projects. Then I will pause a bit: I would like to devote some time (besides Generative Art coding and Nature simulation) experimenting a merge of all the Photoshop&#8217;s extensibility layer technologies &#8211; Scripting, Generator and HTML Panels (leaving alone TCP/IP connections).\n  \n\n  \n    I&#8217;m very interested in the 3D context of the HTML5 canvas element. Not because I particularly dig 3D, but because it is hardware accelerated (GPU pumped) &#8211; i.e. damn fast. Eventually I&#8217;d like to test the possibility to use it as a pixel crunching machine to generate imagery within the Panel, to be finally passed back to Photoshop (if you&#8217;re faster/better than me implementing this, please let me know!).\n  \n\n  \n    I&#8217;m reading also (and want to go through it this year) the WebGL Programming Guide by Kouichi Matsuda and Rodger Lea. Among the plethora of books on WebGL and OpenGL this one is really clear and has never failed yet to anticipate a doubt of mine with a thorough explanation and some example code. Not that GLSL (actually GLSL ES 3.0, the OpenGL scripting code flavor used to program Shaders in WebGL) is the easiest of the languages, but if you&#8217;ve passed some hours with all the links I&#8217;ve scattered in this post, somehow it gets less scary.\n  \n\n  \n    All the Rest\n  \n\n  \n    I&#8217;ve just noticed that I&#8217;ve switched priorities &#8211; it&#8217;s now Coding, Photoshop and All the Rest. Which is OK &#8211; I&#8217;ve been spending my +8 hours a day on Photoshop since version 5 (not CS5) and post-production / pre-press is still what brings 80% of the food on the table; but it&#8217;s not anymore fireworks of passion &#8211; I admit, or should I say: I observe.\n  \n\n  \n    After about 15 years of use, I tend to notice flaws rather than functionalities, limits rather than features, missed opportunities rather than wow-factors. Grumbling is fine, but I&#8217;m happy to breath some fresh air with coding. Not that I feel particularly talented, though &#8211; on the contrary: I spend an awful amount of time on problems that officially trained (and much younger than me) programmers would solve in a snap. Plus, I have to keep learning mostly in the nighttime, or put it in standby (because life intervenes) for some days or weeks &#8211; which makes me constantly lose the thread. But that&#8217;s the way it&#8217;s going to be this year too hence no more complaining.\n  \n\n  \n    After all, many friends of mine (professionals in the business I&#8217;m in) are experiencing some similar, slight switch of focus &#8211; it has to be physiologic somehow.\n  \n\n  \n    On the personal side, after the earthquake in my region in 2012, we&#8217;ve had whirlwind in 2013 and flooding in 2014. Next years the odds are for locusts &#8211; at least my bets are just halfway between locusts and a falling satellite. See you there!\n  \n\n  \n    &nbsp;\n  \n\n",
      tags: ["Java","Javascript","Processing","WebGL"],
      id: 62
    });
    

    index.add({
      title: "Up to 10 Color Samplers &#8211; Photoshop CC (14.2)",
      category: ["Photoshop"],
      content: "\n  \n    Thanks to last Photoshop World Codeathon in Las Vegas, we&#8217;ve finally broken the 4 Color Samplers limit in the Info Palette &#8211; we&#8217;re allowed to scatter up to 10 of them like a machine gun (provided we&#8217;ve Photoshop CC updated to the 14.2 dot release, available from Jan 16, 2014 onwards). If you&#8217;re in the color correction business, that is a big deal. [Fireworks popping, people getting naked, etc]\n  \n\n  \n    Now, to be fair, this is a pretty old feature request (ignored/postponed&#8230;) &#8211; something more like a JDI if you will. But let&#8217;s not be the usual mumbling guys for a little bit! Just a little bit 😉\n  \n\n  \n    PS I built an experimental Panel back in 2011, called Power Info Palette &#8211; which still shows that there&#8217;s always room for improvement! But it&#8217;s definitely nice to have native support for 10 samplers in PS now.\n  \n\n",
      tags: ["Codeathon","Color Sampler","Info Palette","JDI","Photoshop @en"],
      id: 63
    });
    

    index.add({
      title: "Adobe Scripting: collaborative code sharing",
      category: ["Coding"],
      content: "\n  \n    An open letter to fellow devs\n  \n\n  \n    If you&#8217;re involved building third party solution for Adobe applications (mainly Scripts/Extensions) and you care about it, please invest 10 minutes of your time, read along and chime in.\n  \n\n  \n    Hi, I&#8217;m Davide\n  \n\n  \n    I&#8217;ve started scripting Adobe apps (mainly PS) back in 2008, with no prior programming experience whatsoever. I work full time as a post-production freelance since the year 2000, so my main focus can&#8217;t be on coding; yet over the years I&#8217;ve learned, mostly from others in public forums, enough to start a small paid scripts/extensions business that so far has been contributing for about 1:5 on average to my total incomes.\n  \n\n  \n    As a senior PS professional I can&#8217;t consider myself but as an amateur coder, yet I&#8217;m very passionate (isn&#8217;t passion what amateurs are about, after all?!)\n  \n\n  \n    What follows are my own and personal thoughts, that I&#8217;ve shared privately with some folks in the industry: nobody put me down (on the contrary, they strangely seemed to sustain my point of view!) so I&#8217;ve decided to eventually write this one publicly.\n  \n\n  \n    The Scripting communities\n  \n\n  \n    I can&#8217;t speak but about PS people; yet I&#8217;ve been in touch with many InDesign coders lately &#8211; from whom I got the personal, possibly incorrect impression that their community is stronger: in terms of available resources produced (forum threads, free and paid products, documentation), vitality, and, well, collaboration and sense of community. As a result I got the personal, possibly incorrect impression that they have more influence on Adobe as a software company.\n  \n\n  \n    Mind you, developers (no matter whether PS-ID-IL-AE-centric) understand they are just a tiny fraction of the company&#8217;s users base and it&#8217;s not from them the cash flow comes &#8211; hence their needs aren&#8217;t top priority, so to speak. We know.\n  \n\n  \n    The Photoshop scripting community, and by scripting I mean basically everything in the extensibility layer but C/C++ plugins (ExtendScript and Flash/HTML Panels, Generator/Node.js) do have some essential and well known keystones: preferences may vary, but I confidently list first the independent ps-scripts.com forum moderated by Mike Hall, which is where Ross Huitt (aka XBytor) has been sharing his great XTools and many skilled and very talented people constantly keep contributing. The official PS Scripting forum by Adobe (where in my opinion the signal/noise ratio is a bit lower). And lots of other personal websites devoted to PS scripting: Adobe&#8217;s Jeffrey Tranberry, Russell Preston Brown (aka. Dr. Brown), Tom Krcha, David Deraedt; Paul Riggott, Michel Mariani, Trevor Morris, John J. McAssey and many others  &#8211; I try to do my little part too.\n  \n\n  \n    Bad times?\n  \n\n  \n    Undoubtedly the PS Scripting community is undergoing some change. Paul Riggott, among the most skilled and helpful contributors, left forums altogether in protest with Adobe. XBytor declared to be burned out and not willing to start any major new project. In an official forum&#8217;s thread called &#8220;Will scripting be supported in future?&#8221; (worth reading too) John McAssey collected bits of different posts showing (some influent) community members&#8217; bad mood &#8211; so to speak.\n  \n\n  \n    Private conversations I&#8217;ve had with people in this business confirmed that, generally speaking, there&#8217;s a lot of disaffection going on among developers: recent technologies Adobe rolls out of its hat are promising (HTML Extensions, Generator/Node.js) yet the past has taught us not to embrace them too enthusiastically nor too soon, because they might end unsupported in the mid-term. In a dog-chasing-its-tail loop that could even accelerate the process: take as an example Kevin Goldsmith&#8216;s PixelBender, which never left the Labs and drop discontinued. Was that because Adobe judged the coders&#8217; involvement too low to justify the development cost (as it&#8217;s been declared) or are the developers, who weren&#8217;t embracing the technology because Adobe didn&#8217;t put enough resources on it in order to make it really profitable? That was well before the Flash thing, which is by the way going to put an end to Flex Panels and Configurator very soon.\n  \n\n  \n    True, IT world is constantly moving on, but as someone who&#8217;s learned ActionScript from scratch &#8220;just&#8221; to code Flex panels back in CS4, I now approach HTML Panels in CC (three generations afterwards) and Generator with a tad of cold blood &#8211; I&#8217;m working full-time, married, in my late thirties and time is not exactly what I&#8217;m plenty of.\n  \n\n  \n    Cahiers de doléances\n  \n\n  \n    Bear with me and let me express some personal criticism before getting into some more pro-active, constructive comments. Here are my own Cahiers de doléances, provided that I assume engineers don&#8217;t like not to fix bugs, nor avoid implementing new features just to liven up the otherwise boring life of the third party developer &#8211; it&#8217;s management that decides where to put (or not to put) development resources.\n  \n\n  \n    \n      Scripting as we know it (i.e. ExtendScript) needs some love. It&#8217;s a language with its own true gems &#8211; as pointed out by the ID developer Marc Autret (who proved to be among the finest scripting connoisseurs): the JSXBIN compiler, File/Folder management, Reflection and Dictionary interfaces, Debugging/tracking services, preprocessor directives, native XML support, etc. &#8211; and some nightmares. Now that scripters are supposed to interbreed with web-devs for HTML panels (running on a Google Chromium implementation / V8 engine) at least in the long run the way data is passed/evaluated by the two engines should be unified &#8211; and ExtendScript upgraded to latest ECMAScript standards without losing its peculiarities.\n    \n    \n      ScriptUI is a mess. Hopefully the switch to an HTML adapter that Tom Ruark&#8217;s envisioned will end up fixing the undisguised rendering differences and behavior discrepancies we&#8217;re drowning into now: inter-app (PS/ID/IL etc.), inter-platform (PS Mac/Win), inter-version (PS CSx/CC), it&#8217;s a minefield. I&#8217;ve been told that while ExtendScript specs are the Standard to refer to, each app&#8217;s team (PS/ID/IL/&#8230;) deals with its implementation independently: as a result, the consistency one would expect for a set of applications formerly knows as Creative Suite vanishes.\n    \n    \n      ESTK (ExtendScript ToolKit) is&#8230; rather raw as an IDE for 2014 standards, suffers from many (old) bugs, code hinting is often inaccurate, it doesn&#8217;t provide any ScriptUI visual tooling, not to mention the fact its own ScriptUI implementation differs from all the other apps&#8217; (aspect/behavior) implementation, which makes the whole thing quite pointless, and sometimes dangerous.\n    \n    \n      PS scripting would greatly benefit from the implementation of some kind of Image Processing language available through scripting. It&#8217;s quite paradoxical that HTML Canvas element + JS (that is: HTML Panels!) can do image processing yet there&#8217;s no straightforward way to pass back and forth the open Document&#8217;s BitmapData. As far as I know, one could even port Processing code that way &#8211; wouldn&#8217;t that be just awesome?\n    \n    \n      ActionManager code and the ScriptingListener plugin are a godsend, but first: there are many useful things beyond the AM reach, second: AM isn&#8217;t an excuse not to extend the DOM support anymore.\n    \n    \n      HTML Panels are a wise move in the long run for sure &#8211; provided their implementation will overcome several teething problems and lack of documentation (I&#8217;m among the ones who remember the amazing series of articles written by the great Olav Kvern on CS SDK). Which is OK to deal with, unless the deadline for the Flex panels support in PS is Mid 2014 (that is: tomorrow morning, so to speak). Extensions cannot replace scripted windows (that are action recordable for instance), but are interfaces to, and evaluates, &#8220;traditional&#8221; scripting.\n    \n  \n\n  \n    Others might have a different list of doléances, the above ones are just my favorite refrain.\n  \n\n  \n    Call for (some) action!\n  \n\n  \n    Notwithstanding, the PS scripting community is doing some interesting stuff and is spread over the internet under many forms. Yet all the available resources I&#8217;ve mentioned earlier, while undoubtedly valuable, appears very sparse. They&#8217;re able to answer to the question: &#8220;How do I&#8230;?&#8221; with snippets and explanation or just exposing source code, but mostly it&#8217;s just a matter of personal efforts. Disconnected from other personal efforts &#8211; by someone else &#8211; and for that reason less effective from the educational point of view.\n  \n\n  \n    Forums somehow fill the gap: it&#8217;s where you go asking questions and interacting with people, who are either likely to have run into your problem, or just more experienced to suggest a solution. Forums are an excellent archive, collecting the community&#8217;s accumulated knowledge, providing historically sorted references and discussions.\n  \n\n  \n    Yet, as a code repository, Forums cannot beat other tools that have been specifically designed to be exactly that: code repositories.\n  \n\n  \n    I have nothing revolutionary to propose: I&#8217;ve noticed how people involved in PS coding are sometimes very talented but either isolated, or interacting in ways that could be more profitable for the whole community &#8211; specifically for those who are not yet in the number, but will enter the community in the future: willing to learn and to make use of our accumulated knowledge.\n  \n\n  \n    Open Source Code Repositories\n  \n\n  \n    Over the years I&#8217;ve collected tons of Scripting related material, and you&#8217;re likely to have done the very same thing yourself.\n  \n\n  \n    Why don&#8217;t we create a set of collaboratively maintained, open sourced GitHub repositories, to store:\n  \n\n  \n    \n      Snippets (functions, AM code, etc).\n    \n    \n      Boilerplate code.\n    \n    \n      Demo code and Tutorials/HOWTOs.\n    \n    \n      Libraries and Frameworks.\n    \n    \n      Links to notable Forum Threads, valuable external resources, etc.\n    \n    \n      Official and Unofficial Documentation.\n    \n    \n      Community&#8217;s feature requests.\n    \n    \n      You name it.\n    \n  \n\n  \n    I have no intention to create the umpteenth PS-website, nor to substitute Forums (which keep being the best place for discussion to happen and ideas to be shared): I&#8217;d like to have a place to go to when I&#8217;m in need to borrow or learn code from (a-ehm, what Adobe&#8217;s DevNet should have been). I&#8217;m not exactly the smartest Git user in town, but I know that GitHub repos:\n  \n\n  \n    \n      Are forkable (you can clone it locally).\n    \n    \n      There is a group of maintainers who is in charge of the not obvious problem of their design/structure.\n    \n    \n      Allow Pull requests: that is, you can submit your own tweaks in order to have them integrated into the main line of development.\n    \n    \n      Can contain not only sheer code but HTML/markdown documents &#8211; i.e. all sort of Documentation, Tutorials, etc.\n    \n    \n      Provide collaborative coding tools such as Wiki, Issues tracking, etc.\n    \n    \n      Are free and open source &#8211; you give bits of your time and knowledge for the whole community&#8217;s benefit.\n    \n    \n      Can scale easily to include other apps (ID/IL/AE/&#8230;)\n    \n    \n      Because they have been designed as a repositories, code there is version tracked and can be maintained more effectively (compared to Forums, where not very often old threads get revamped).\n    \n  \n\n  \n    Chime in!\n  \n\n  \n    When I&#8217;ve happened to code commercial scripts I&#8217;ve tried to build tools that I would have liked to use myself &#8211; and this is the spirit I&#8217;m asking you for support with.\n  \n\n  \n    Looks like a complex thing to build from the ground up, yet it would help not only ourselves but those approaching scripting in the future too (and it doesn&#8217;t need to be built overnight); not to mention the fact that there could be a tiny chance this might strengthen the Devs Community to Adobe&#8217;s eyes &#8211; I don&#8217;t want to create a lobby, nor find a place to fill with narcissism: I&#8217;m a happy sociopath who lives in the countryside. I would like to consider Adobe Scripting a nice and profitable platform to develop for in the next, say, 5 years. To date, I&#8217;m not sure I would, frankly.\n  \n\n  \n    I&#8217;m still in a phase where I&#8217;m evaluating whether this project can arouse other people&#8217;s interest &#8211; it can&#8217;t be but a collaborative effort &#8211; or not (in case I&#8217;ll easily turn it into a personal hobby). I&#8217;m skating over the next big problem on the list, i.e. the design of such repository.\n  \n\n  \n    Would you like to chime in? Do you have any idea, feedback, suggestion along these lines? Can you provide GitHub/social coding experience and skills? Just want to high five and disappear? The comment section below is for you.\n  \n\n  \n    Thank you for your time,\n  \n\n  \n    Davide\n  \n\n",
      tags: ["Adobe","Extendscript","extensions","GitHub","scripting"],
      id: 64
    });
    

    index.add({
      title: "Photoshop Script Boilerplate Code on GitHub",
      category: ["Coding","CoffeeScript","ExtendScript / Javascript","Photoshop"],
      content: "\n  \n    Back when I started writing Scripts for Photoshop, uhm, five or six years ago, I&#8217;d have loved having a (set of) templates to borrow code from. Luckily 90% of scripts around aren&#8217;t binary encoded, so you can peep at the structure of publicly available JSX files from talented contributors, or ask in the forums, or find your own way yourself.\n  \n\n  \n    Still, it&#8217;s kind of bizarre to me that while Boilerplate code is the norm for most of the stuff related to web development, there&#8217;s nothing similar in the ExtendScript ecosystem, as far as I know.\n  \n\n  \n    I&#8217;ve started devoting some of my (copious, of course) spare time uploading my take on Boilerplate code for Photoshop Scripts on my GitHub account. Everything&#8217;s still in an early, alpha stage, but you can already find there a little something written on CoffeeScript.\n  \n\n  \n    Time permitting I will expand the repository, so check back and feel free to fork and contribute!\n  \n\n",
      tags: ["Boilerplate","GitHub","Photoshop @en","script"],
      id: 65
    });
    

    index.add({
      title: "Sublime Text snippet: try/catch wrapping in CoffeeScript",
      category: ["Coding","CoffeeScript"],
      content: "\n  \n    One of the Sublime Text editor&#8217;s cool features are Snippets (bits of code you use very often and want to keep handy). The CoffeeScript package that I use, Better-CoffeeScript, already has a snippet for try/catch block which creates some empty code to be filled:\n  \n\n  try\n\t# ...\ncatch e\n\t# ...\n\n  \n    Yet, I&#8217;d like to select some text, then have a shortcut-bound Snippet that acts on selected code and wraps it in a try/catch block. From this:\n  \n\n  w = (s) -&gt;\n\t$.writeln s\nw app.activeDocument.name\n\n  \n    To this:\n  \n\n  try\n\tw = (s) -&gt;\n\t\t$.writeln s\n\tw app.activeDocument.name\ncatch e\n\talert \"Error: #{e.message}\\nLine: #{e.line}\"\n\t# ...\n\n  \n    (Don&#8217;t worry if the code is weird, it&#8217;s what ExtendScript, a Javascript superset made by Adobe for scripting purposes, looks like). It seems trivial &#8211; yet I&#8217;ve had issues with proper indentation unless I&#8217;ve eventually turned to Regular Expressions.\n  \n\n  \n    There are plenty of tutorial about Sublime Text Snippet creation, so I&#8217;ll keep it as short as possible.\n  \n\n  \n    1. Create a New Snippet\n  \n\n  \n    In Sublime, go to the Tools &gt; New Snippet&#8230; menu and replace the boilerplate XML-like code with the following:\n  \n\n  &lt;snippet&gt;\n&lt;content&gt;&lt;![CDATA[\ntry\n${SELECTION/([^\\n]*)/\\t$0/g}\ncatch e\n\talert \"Error: #{e.message}\\nLine: #{e.line}\"\n\t${1:# ...}\n]]&gt;&lt;/content&gt;\n\t&lt;scope&gt;source.coffee&lt;/scope&gt;\n\t&lt;description&gt;Try/Catch wrap&lt;/description&gt;\n&lt;/snippet&gt;\n\n  \n    The RegEx basically acts on the selected text ${SELECTION (mind you, there&#8217;s no tab in the snippet before it) and adds tabs to each line. Everything is wrapped in the try/catch block, and the caret is set in the last catch line, a comment ${1: #...}. Feel free to customize the error handling to your needs.\n  \n\n  \n    The &lt;scope&gt; tag instruct Sublime not to use this snippet unless the file is CoffeeScript, while the &lt;description&gt; is the label that will appear in the Snippets menu. You can follow the Sublime Text Documentation to learn more about Snippet creation, and this summary for Regular Expressions in Perl.\n  \n\n  \n    2. Save the Snippet\n  \n\n  \n    Save to Sublime Text &gt; Packages &gt; User folder, with a .sublime-snippet extension. I&#8217;ve used TryCatch_Wrap.sublime-snippet.\n  \n\n  \n    3. Bind to a Shortcut\n  \n\n  \n    Go to the Sublime Text &gt; Preferences &gt; Key-Bindings &#8211; User menu. A .sublime-keymap file will open, mine was:\n  \n\n  [\n\t{ \"keys\": [\"alt+shift+j\"], \"command\": \"js_coffee\", \"args\":{\"new_file\": true}}\n]\n\n  \n    In case (like me) you already have a line, add a comma and the following:\n  \n\n  [\n\t{ \"keys\": [\"alt+shift+j\"], \"command\": \"js_coffee\", \"args\":{\"new_file\": true}},\n\t{ \"keys\": [\"ctrl+super+t\"], \"command\": \"insert_snippet\", \"args\": {\"name\": \"Packages/User/TryCatch_Wrap.sublime-snippet\"} }\n]\n\n  \n    As I&#8217;ve written it, the Snippet is bound to Command+Control+t, but you can use the combination you like the most (have a look at Key-Bindings &#8211; Default to check whether you&#8217;re overriding some shortcut already in use). If you&#8217;re wondering, accepted modifiers are:\n  \n\n  \n    \n      shift\n    \n    \n      ctrl\n    \n    \n      alt\n    \n    \n      super (Windows key, Command key)\n    \n  \n\n  \n    Be sure to refer to the same .sublime-snippet filename, save and close the keymap.\n  \n\n  \n    4. Enjoy!\n  \n\n  \n    You&#8217;ve now built a Sublime Text Snippet for CoffeeScript that wraps and indent selected code in a try/catch block. You can select a portion of your code, hit ctrl+command+t and voilà! Hope this helps, you coffee fellows out there.\n  \n\n",
      tags: ["CoffeeScript","indentation","Snippet","Sublime Text","try catch"],
      id: 66
    });
    

    index.add({
      title: "&#8216;PS Projects&#8217; for Photoshop CC/CS6",
      category: ["Extensions and Scripts","Photoshop","PS Projects"],
      content: "\n  \n     I&#8217;m glad to announce my latest script for Photoshop CC / CS6, called PS Projects.\n  \n\n  \n    \n  \n\n  \n    What is it?\n  \n\n  \n    \n      \n        PS Projects is a script for Photoshop CC and CS6, that lets you Create, Load and Modify Project files &#8211; i.e. collections of images in any format, from any folder in your hard drive as file references, that for some reason you might need to open in Photoshop often.\n      \n    \n\n    \n    \n\n    \n      \n    \n\n    \n      Do I need it?\n    \n\n    \n      Have you ever had the need to open the same set of images more than once, maybe because you&#8217;re a UX designer working on a project that includes dozens of assets, or a still life photographer post-producting images that the Art Director has finally chosen for the Ad campaign, from the hundreds you&#8217;ve shot in total?\n    \n\n    \n      PS Projects implements the idea of Project Files: a lightweight, encrypted file that contains only the links to the assets you want to open in Photoshop.\n    \n\n    \n      \n    \n\n    \n      How does it work?\n    \n\n    \n      Create a New Project selecting files from any folder in you hard disk, any format that PS can open: PSD, TIFF, JPG, DNG, etc. and Save a .psproject file with a meaningful name.\n    \n\n    \n      Each time you need that very set of images, just Load the .psproject file and you&#8217;ll get them all opened in Photoshop in a glance.\n    \n\n    \n      Is it cool?\n    \n\n    \n      PS Projects relieves you from the need to duplicate, move or tag a selection of images that you need to pick up from the bottomless pits of your hard drives &#8211; the actual files are always kept in their original position, and you won&#8217;t risk to end up with duplicates or trashing something important.\n    \n\n    \n      \n    \n\n    \n      \n    \n\n    \n      Project files weight few Kilobytes, are portable and safe &#8211; you can basically create as many of them as you need, risk-free.\n    \n\n    \n      Future versions\n    \n\n    \n      PS Project is actively developed: just while version 1 takes off, I&#8217;m working hard on the next major release, which will be a big feature upgrade. Including a brand new &#8220;Load&#8221; dialog that allows you to:\n    \n\n    \n      \n        Load files in Photoshop (as before).\n      \n      \n        Copy the referenced images to a Folder.\n      \n      \n        Apply Actions to the referenced images, with the possibility to save the result in a new location, and logging errors.\n      \n      \n        Inspect the project (referenced images list, owner, date, project version&#8230;)\n      \n    \n\n    \n      Basically you&#8217;ll be able to do a lot more with the images referenced in the Project File, not only open them. By the way, version 2.0 will be a free update for licensed 1.x users. I plan to open a public beta (for certified 1.x customers) so stick around for the announce.\n    \n\n    \n      Features and Specs\n    \n\n    \n      PS Projects requires Photoshop CS6 and Photoshop CC, on either OSX or Windows. It provides path and thumb* preview, Project file creation, modification and loading.\n    \n\n    \n      The &#8216;PS Project&#8217; script for Photoshop is for sale at USD14.95 via Adobe Exchange and Bigano e-store.\n    \n\n    \n      \n      &lt;/div&gt;\n\n      \n      \n\n      \n      &lt;/div&gt;\n      \n\n",
      tags: ["Photoshop CC","Photoshop CS6","PS Projects","script"],
      id: 67
    });
    

    index.add({
      title: "PS Projects for Photoshop CC / CS6 User Manual",
      category: ["Scripting"],
      content: "\n  \n    PS Projects is a Photoshop CC / CS6 script that implements the idea of Project Files (a lightweight container of links to images on your disk, that you need to open frequently). This &#8216;User Manual&#8217; post will go into every detail of it.&lt;/p&gt;\n\n    \n      Introduction\n    \n\n    \n      If you haven&#8217;t seen them already, please go over this one minute presentation and read the companion Introducing PS Project blogpost to get a brief, visual overview about what the product is all about.\n    \n\n    \n      http://www.youtube.com/watch?v=5EYZNU6gAZw\n    \n\n    \n      Installation\n    \n\n    \n      You can get PS Projects from Adobe Exchange or Bigano e-store. In order to streamline the installation process, please make sure you have the latest version of Adobe Extension Manager (download here: CC, CS6). If you run into problems with the installation, ask for support: Exchange users can post here, Bigano customers will ask here.&lt;/span&gt;\n    \n\n    \n      \n    \n\n    \n      Where is it?\n    \n\n    \n      PS Projects belongs to the File &#8211; Automate menu -&gt;\n    \n\n    \n      Main Dialog\n    \n\n    \n      PS Projects has three modules that you can access from the Main Dialog, this User Manual will cover them all. Just below the buttons, the [INFO] link points to this very page.\n    \n\n    \n      \n    \n\n    \n      \n        Create New &#8211; select files from any folder in your hard drive and Save a new PS Project.\n      \n      \n        Load &#8211; load a PS Project from the disk to open the associated images in Photoshop.\n      \n      \n        Modify &#8211; load a PS Project from the disk and modify the list of the included images, then Save the updated PS Project.\n      \n    \n\n    \n      Create New\n    \n\n    \n      Clicking the Create New button opens the Save a Project dialog:\n    \n\n    \n      \n    \n\n    \n      First click the Browse&#8230; button to run the Photoshop &#8220;Open dialog&#8221; and choose images from your hard disks. You can include in a PS Project all formats Photoshop is able to deal with (PSD, TIFF, JPG, DNG&#8230; etc.).\n    \n\n    \n      If you happen to have files already open in Photoshop, provided that they have been saved at least once (and they haven&#8217;t been modified afterwards), you can include them with the Add Open Files button &#8211; which is grayed out in the screenshot above since there are no open files at all.\n    \n\n    \n      The Remove button does what you guess &#8211; it removes from the list all the selected files.\n    \n\n    \n      The Files list shows filenames of the included assets. You can click on them (multiple selection allowed): the Path will appear below and you&#8217;ll get a Preview thumbnail. File formats other than JPG and PNG require an instance of Adobe Bridge open to display the preview &#8211; the same way MiniBridge requires his bigger brother to work properly. No problem if you haven&#8217;t it or don&#8217;t want to run it &#8211; these preferences can be set (the window at left pops up the first time) and you&#8217;ll get a generic thumbnail as the preview.\n    \n\n    \n      When you click the Save button, you&#8217;ll be prompted with the usual dialog: choose a meaningful name (without extension: the script will add .psproject automatically) and save the file wherever you like.\n    \n\n    \n      As soon as the save dialog is closed, an alert will ask you if you&#8217;re willing to immediately open the images of the Project file just saved or not.\n    \n\n    \n      Load&#8230;\n    \n\n    \n      Select Load&#8230;  to locate an existing .psproject file on your disk. All the images that are referenced in the Project will be opened in Photoshop.\n    \n\n    \n      In case the Project contains missing images (say you&#8217;ve moved, or deleted them from the disk after saving the Project) an alert will log all the lost entries.\n    \n\n    \n      \n    \n\n    \n      Modify&#8230;\n    \n\n    \n      The Modify&#8230; button is a combination of the other two. You&#8217;ve first to select an existing .psproject file; then a dialog identical to the Create New one will appear, automatically filled in with the list of images referenced in the Project file you&#8217;ve just loaded.\n    \n\n    \n      In case of missing images you&#8217;ll get both a warning pop-up and a red icon besides the filenames, showing an exclamation mark thumbnail.\n    \n\n    \n      Requirements\n    \n\n    \n      PS Projects runs on Photoshop CC and CS6, on both Mac and Windows.\n    &lt;/div&gt;\n\n",
      tags: ["User Manual"],
      id: 68
    });
    

    index.add({
      title: "BridgeTalk Export as Binary failure",
      category: ["Coding","ExtendScript / Javascript"],
      content: "\n  \n    BridgeTalk, the system that enables messaging between applications in the Adobe scripting ecosystem, is prone to failure when evaluating functions via .toSource() &#8211; be aware and stringify them in advance!\n  \n\n  \n    BridgeTalk 101\n  \n\n  \n    This simple script sends a message from Photoshop to Adobe Bridge &#8211; that is to say, Photoshop gives Bridge some code to execute:\n  \n\n  #target photoshop\nvar bt = new BridgeTalk()\nbt.target = 'bridge';\nbt.timeout = 30;\nbt.body = \"$.writeln('Greetings from Bridge'); true;\";\nbt.onReceived = function(btObj) {\n\treturn $.writeln('Message received.');\n};\nbt.onResult = function(btObj) {\n\treturn $.writeln('BridgeTalk result: ' + btObj.body);\n};\nbt.onError = function(btObj) {\n\treturn $.writeln(btObj.body);\n};\nbt.onTimeOut = function(btObj) {\n\treturn $.writeln(btObj.body);\n};\nbt.send(31);\n\n  \n    The Console outputs:\n  \n\n  Greetings from Bridge\nBridgeTalk result: true\nResult: true\n\n  \n    It does the job configuring a BridgeTalk Object, which .body is the actual part of the script in charge of Bridge (here just a String).\n  \n\n  \n    You can specify callbacks invoked by the target application (in the example Bridge) such as:\n  \n\n  \n    \n      onResult() &#8211; to return a response (to Photoshop), which is handled by the function;\n    \n    \n      onError() &#8211; to return and handle an error;\n    \n    \n      onTimeout() &#8211; when a timeout occurs;\n    \n    \n      onReceived() &#8211; handler fired when the target app receives the message. For some reason, it&#8217;s triggered after, and not before, onResult().\n    \n  \n\n  \n    Mind you, if the .send() method is called without any argument, the message is asynchronous, while if you specify the number of seconds to wait for before returning from the function, the message is sent synchronously. More info about BridgeTalk in the JavaScript Tools Guide CC, from page 170 onwards.\n  \n\n  \n    Using functions\n  \n\n  \n    When the message body is trivial, it can be assigned to a String literal; conversely, when you need the target app to perform some more elaborated tasks, it&#8217;s common practice to use a separate function. This is the same as the first example (I will omit some of the callbacks):\n  \n\n  var bridgeFun = function () {\n\t$.writeln('Greetings from Bridge');\n\treturn true;\n}\nvar bt = new BridgeTalk()\nbt.target = 'bridge';\nbt.body = \"var bridgeFun = \" + bridgeFun.toSource() + \"; bridgeFun();\";\nbt.onResult = function(btObj) {\n\treturn $.writeln('BridgeTalk result: ' + btObj.body);\n};\nbt.send(31);\n\n  \n    The .toSource() method returns a string representing the source code for the function. So the message body (bt.body) is:\n  \n\n  var bridgeFun = (function () { $.writeln('Greetings from Bridge'); return true; }); bridgeFun();\n\n  \n    Beware .toSource()\n  \n\n  \n    There are at least a couple of issues with .toSource(). First be careful when commenting lines in the separate function &#8211; always use /* */ and not // because chances are that the latter will comment out everything after it.\n  \n\n  \n    Second, if you Export as Binary the code from ESTK, it fails:\n  \n\n  eval(\"@JSXBIN@ES@2.0@MyBbyBn0AJJAnASzJjCjSjJjEjHjFiGjVjOByBNyBnAMAbyBn0ACJBnAEXzHjXjSj\\\nJjUjFjMjOCfjzBhEDfRBFeViHjSjFjFjUjJjOjHjThAjGjSjPjNhAiCjSjJjEjHjFffZCnAFct0DzAE\\\nCDnftJFnASzCjCjUFyBEjzKiCjSjJjEjHjFiUjBjMjLGfntnftJGnABXzGjUjBjSjHjFjUHfVFfyBne\\\nGjCjSjJjEjHjFfJHnABXzHjUjJjNjFjPjVjUIfVFfyBndgefJInABXzEjCjPjEjZJfVFfyBCzBhLKCK\\\nnEXzIjUjPiTjPjVjSjDjFLfVBfyBnfeQjWjBjShAjCjSjJjEjHjFiGjVjOhAhdhAnnneOhbhAjCjSjJ\\\njEjHjFiGjVjOhIhJhbnfJJnABXzIjPjOiSjFjTjVjMjUMfVFfyBNyBnAMJbyBn0ABZKnAEXCfjDfRBC\\\nKnXJfVzFjCjUiPjCjKNfAeTiCjSjJjEjHjFiUjBjMjLhAjSjFjTjVjMjUhahAnffABN40BhAB0AECLn\\\nfJMnABXzHjPjOiFjSjSjPjSOfVFfyBNyBnAMMbyBn0ABZNnAEXCfjDfRBXJfVNfAffABN40BhAB0AEC\\\nOnfJPnABXzJjPjOiUjJjNjFiPjVjUPfVFfyBNyBnAMPbyBn0ABZQnAEXCfjDfRBXJfVNfAffABN40Bh\\\nAB0AECRnfJSnAEXzEjTjFjOjEQfVFfyBRBFdgfffACF4B0AiAB40BiAACAEByB\");\n\n  \n    (Remember to surround the binary chars blob with eval(\" \") and close the lines with a \\). The Console outputs:\n  \n\n  Error in\nLine 2: var bridgeFun = (function anonymous() {  [compiled code] } ); bridgeFun();\nExpected: ]\n\nResult: true\n\n  \n    That&#8217;s because the purpose of binary encoding is hiding the code, so the function reads [compiled code] and is not stringified nor interpreted correctly.\n  \n\n  \n    Workarounds\n  \n\n  \n    You have a couple of choices to make it work. Either you can move the separate function out of the binary encoding and leave it readable:\n  \n\n  var bridgeFun = function () {\n\t$.writeln('Greetings from Bridge');\n\treturn true;\n}\n\neval(\"@JSXBIN@ES@2.0@MyBbyBn0AIJAnASzCjCjUByBEjzKiCjSjJjEjHjFiUjBjMjLCfntnftJBnABXzGjU\\\njBjSjHjFjUDfVBfyBneGjCjSjJjEjHjFfJCnABXzHjUjJjNjFjPjVjUEfVBfyBndgefJDnABXzEjCjP\\\njEjZFfVBfyBCzBhLGCGnEXzIjUjPiTjPjVjSjDjFHfjzJjCjSjJjEjHjFiGjVjOIfnfeQjWjBjShAjC\\\njSjJjEjHjFiGjVjOhAhdhAnnneOhbhAjCjSjJjEjHjFiGjVjOhIhJhbnfJEnABXzIjPjOiSjFjTjVjM\\\njUJfVBfyBNyBnAMEbyBn0ABZFnAEXzHjXjSjJjUjFjMjOKfjzBhELfRBCGnXFfVzFjCjUiPjCjKMfAe\\\nTiCjSjJjEjHjFiUjBjMjLhAjSjFjTjVjMjUhahAnffABM40BhAB0AzANCGnfJHnABXzHjPjOiFjSjSj\\\nPjSOfVBfyBNyBnAMHbyBn0ABZInAEXKfjLfRBXFfVMfAffABM40BhAB0ANCJnfJKnABXzJjPjOiUjJj\\\nNjFiPjVjUPfVBfyBNyBnAMKbyBn0ABZLnAEXKfjLfRBXFfVMfAffABM40BhAB0ANCMnfJNnAEXzEjTj\\\nFjOjEQfVBfyBRBFdgfffABB40BiAABANByB\");\n\n  \n    (Mind you: I&#8217;ve cut the function out, then exported the binary, then pasted the function back above the binary).\n  \n\n  \n    Or you can stringify the function and insert it as a String literal: for instance calling $.writeln(bt.body) on the original version and copy/paste the result from the Console back to the message body. The following version exports as binary without a glitch!\n  \n\n  var bt = new BridgeTalk()\nbt.target = 'bridge';\nbt.body = \"var bridgeFun = (function () { $.writeln('Greetings from Bridge'); return true; }); bridgeFun();\"\nbt.onResult = function(btObj) {\n\treturn $.writeln('BridgeTalk result: ' + btObj.body);\n};\nbt.send(31);\n\n  \n    &nbsp;\n  \n\n",
      tags: ["binary","BridgeTalk","export","Extendscript","toSource"],
      id: 69
    });
    

    index.add({
      title: "Photoshop CC Color Range Enhancements",
      category: ["Photoshop"],
      content: "\n  \n    Latest Photoshop CC update (version 14.1) has introduced a good deal of new features. One of them is the refined Color Range Selection for Highlights, Midtones and Shadows, which now includes a proper Threshold called Range: let&#8217;s see what is that all about.\n  \n\n  \n    Threshold\n  \n\n  \n    As an Adjustment that is in Photoshop since early versions, Threshold hasn&#8217;t gained much popularity so far, unless you&#8217;re into Color Correction or forensic image processing. Have a look at what it does to a test image (click to enlarge).\n  \n\n  \n    \n  \n\n  \n    Basically it blows to White every pixel in the image which value that exceeds (but not include) the Threshold, and clips to Black everything else. You see it clearly in the third example (value: 128) where the image is split in two halves: from mid-grays to shadows they get to black, from mid-grays to highlights they get to white, so to speak. Another way to put it: Threshold makes your images 1bit (i.e. just Black or White, no grays allowed).\n  \n\n  \n    Engineers have now built in the Photoshop CC Color Range Selection tool a Threshold in disguise &#8211; whose name has become Range.\n  \n\n  \n    Range and Fuzziness\n  \n\n  \n    The Color Range Enhancement can be found if you pick from the drop down menu either the Highlights, Midtones or Shadows (see the screenshot), otherwise you won&#8217;t see any new feature (if you&#8217;re wondering, Skin Tones has been introduced in PS CS6).\n  \n\n  \n    Mind you: we&#8217;ll be dealing with B/W images because that&#8217;s how Photoshop &#8220;thinks&#8221; about selections: what is going to get white means selected, what is going to get black means not selected. All the grays in the middle are, depending on their value, halfway between selected and not selected &#8211; that is: the level of gray is tied with the opacity of the selection. So an Alpha Channel is just a way to visualize complex selections.\n  \n\n  \n    The two sliders we&#8217;ll be playing with are the Fuzziness and Range; the latter as I said is just a Threshold (plenty of screenshot below don&#8217;t worry) while the former controls how much the actual selection adhere to the numbers you set in the Range. So to speak, zero Fuzziness means harsh, Threshold like, just-black-or-white selections; higher Fuzziness means smooth selections, that include more grays. Let&#8217;s see.\n  \n\n  \n    Highlights\n  \n\n  \n    I&#8217;ve run Select &#8211; Color Range, then chosen Highlights. I&#8217;ve set Fuzziness to zero: restricting to one variable at the time helps understanding what happens more clearly &#8211; playing with the Range only you get:\n  \n\n  \n    \n  \n\n  \n    As you see, it&#8217;s very much like Threshold! With the apparent difference that Range at 255 still selects the pure whites (the pixels which value is 255), while actual Threshold doesn&#8217;t include it.\n  \n\n  \n    What happens if you raise the Fuzziness to, say, 50%?\n  \n\n  \n    \n  \n\n  \n    As you see the selection &#8220;grows&#8221; through the blacks and gains more gray levels there &#8211; that is to say, you have a smoother selection that includes partially selected pixels.\n  \n\n  \n    Shadows\n  \n\n  \n    Similarly, if you pick the Shadows from the drop-down menu and keep the Fuzziness at zero:\n  \n\n  \n    \n  \n\n  \n    Again you see the Range as a Threshold in disguise &#8211; with the difference that (compared to the actual Threshold) the resulting image is inverted, and you&#8217;re allowed to input 0 as a value (while Threshold sticks with 1). Basically selecting Shadows is just the same as inverting the image first, then selecting the Highlights &#8211; hopefully this makes sense to you.\n  \n\n  \n    Fuzziness does what you would expect:\n  \n\n  \n    \n  \n\n  \n    Midtones\n  \n\n  \n    Things get more interesting when it comes to the Midtones. There, you can simultaneously play with two Range sliders, and combine (so to speak) two Threshold at the same time, that restrict the concept of &#8220;midtone&#8221; that you need:\n  \n\n  \n    \n  \n\n  \n    You can move the two sliders, shifting them to the left (the shadows) or to the right (the highlights), or make them more apart in order to select a bigger part of the grays. Fuzziness is handy here too:\n  \n\n  \n    \n  \n\n  \n    As it happened before, you can refine the selection combining Fuzziness and Range, to get exactly where you want to. The higher the Fuzziness, the more the selection dilates and includes gray levels.\n  \n\n  \n    Curves\n  \n\n  \n    Mind you, the idea of Midtone selection isn&#8217;t new &#8211; Photoshop engineers have been doing a nice enhancement to the Color Range dialog, which surely helps those unwilling to do it the old way. That is to say using Curves:\n  \n\n  \n    \n  \n\n  \n    Davide Barranca\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n  \n\n",
      tags: ["14.1","Color Range","Fuzziness","Photoshop CC","Range"],
      id: 70
    });
    

    index.add({
      title: "ScriptUI tip: decoupling components&#8217; Event handling",
      category: ["Coding","ExtendScript / Javascript"],
      content: "\n  \n    In a ScriptUI Window different components are usually registered for Events, and fire their own Handlers. You can build some interconnection, so for instance a Button&#8217;s &#8216;click&#8217; handler triggers a change in a ListBox, which in turn reacts to its own &#8216;onChange&#8217; Event. It&#8217;s quite easy to decouple this interaction, provided that you set up your code properly.\n  \n\n  \n    Example\n  \n\n  \n    As a starting point, let&#8217;s write some basic demo code to build a Window with just a ListBox and a Button. The ListBox contains five children items, and the Button randomly selects one of them.\n  \n\n  var win = new Window('dialog');\nwin.lbx = win.add('listbox', undefined, undefined, {items:['First Item', 'Second Item', 'Third Item', 'Fourth Item', 'Fifth Item']});\nwin.btn = win.add('button', undefined, \"Select Random\");\n// ListBox handler\nwin.lbx.onChange = function () {\n\t$.writeln(\"ListBox onChange fired.\");\n}\n// Button handler\nwin.btn.onClick = function () {\n\t$.writeln(\"Button clicked.\");\n\tvar i = Math.floor( 5 * Math.random());\n\twin.lbx.children[i].selected = true;\n}\nwin.show();\n\n  \n    As it is the code now, I&#8217;ve set a button .onClick() function that makes the ListBox elements selection switch at random, and a ListBox .onChange() function just logging a message in the Console.\n  \n\n  \n    If you click few times the button, the ListBox elements selection changes: the super-simple flowchart is:\n  \n\n  \n    \n  \n\n  \n    and the Console will contain something like:\n  \n\n  Button clicked.\nListBox onChange fired.\nButton clicked.\nButton clicked.\nButton clicked.\nListBox onChange fired.\nButton clicked.\nListBox onChange fired.\nButton clicked.\nListBox onChange fired.\n\n  \n    Mind you: when it happens, randomly, that the selected item is the same, the &#8220;ListBox onChange fired&#8221; message isn&#8217;t output because there&#8217;s no actual change in the ListBox itself.\n  \n\n  \n    While if you just manually select different items in the Listbox:\n  \n\n  \n    \n  \n\n  \n    This is what is logged then:\n  \n\n  ListBox onChange fired.\nListBox onChange fired.\nListBox onChange fired.\nListBox onChange fired.\n\n  \n    Say that, instead, you want the ListBox to respond to &#8216;change&#8217; events only when you directly interact with it. That is, the Button will keep to change the selection, but it doesn&#8217;t have to trigger the ListBox handler anymore, this way:\n  \n\n  \n    \n  \n\n  \n    That is to say, the two Events must be decoupled. In order to make things work this way you&#8217;ve to rewrite some code.\n  \n\n  var win = new Window('dialog');\nwin.lbx = win.add('listbox', undefined, undefined, {items:['First Item', 'Second Item', 'Third Item', 'Fourth Item', 'Fifth Item']});\nwin.btn = win.add('button', undefined, \"Select Random\");\n\nwin.lbx.addEventListener('change', function(event) {\n\t$.writeln(\"ListBox Change fired\");\n}, true ); // Responds only in the capture phase, not bubbling\n\nwin.btn.addEventListener('click', function(event){\n\tvar i = Math.floor( 5* Math.random() );\n\twin.lbx.children[i].selected = true;\n    $.writeln(\"Clicked Button\");\n}, false );\n\nwin.show()\n\n  \n    First, .onClick() and .onChange() don&#8217;t actually have any Event object available to the function (which may be handy), while .addEventListener() provides you with one.\n  \n\n  \n    Second, using .addEventListener() lets you specify a third argument, which is a Boolean that controls whether the handler responds only in the Capture Phase rather than the Bubbling Phase (for details about Events propagation see pag. 84 of the JavaScript Tools Guide CC). Default is false.\n  \n\n  \n    Bubbling is mostly used, as far as I understand, to diversify handlers in situations when, say, a container such as a Group has several Checkboxes in it. You can attach a handler to the Group, so that the click on every checkbox can &#8220;bubble&#8221; through it (and trigger a response), and attach a different handler on each checkbox, which fires its own peculiar function.\n  \n\n  \n    Here, I&#8217;ve been using the bubbling boolean to control the Event propagation, particularly:\n  \n\n  win.lbx.addEventListener('change', function(event) {\n\t$.writeln(\"ListBox Change fired\");\n}, true ); // Responds only in the capture phase, not bubbling\n\n  \n    That &#8216;true&#8217; means, so to speak, that the 'change' event triggers the anonymous callback it&#8217;s associated with only when the user directly interact with the ListBox &#8211; i.e. selecting items. Conversely, the 'change' event in the ListBox caused by the Button function is ignored. And finally the Console logs only:\n  \n\n  Button clicked.\nButton clicked.\nButton clicked.\nButton clicked.\n\n  \n    Update (and correction)\n  \n\n  \n    Apparently I&#8217;ve been either too lazy to check or completely drunk not to notice that the above example doesn&#8217;t entirely work. True, the Button click makes the ListBox selection change and doesn&#8217;t fires the 'change' event, yet when you manually click items in the list (i.e. change them) the list&#8217;s handler doesn&#8217;t fire either. Which is a problem, because the whole rationale of this post is about decoupling the events (and to kill one isn&#8217;t quite the right way to do it).\n  \n\n  \n    The workaround involves a deeper understanding of Events propagation. Quoting a short excerpt from the Documentation:\n  \n\n  \n    \n      Capture phase — When an event occurs in an object hierarchy, it is captured by the topmost ancestor object at which a handler is registered (the window, for example). If no handler is registered for the topmost ancestor, ScriptUI looks for a handler for the next ancestor (the dialog, for example), on down through the hierarchy to the direct parent of actual target. When ScriptUI finds a handler registered for any ancestor of the target, it executes that handler then proceeds to the next phase. At-target phase — ScriptUI calls any handlers that are registered with the actual target object. Bubble phase — The event bubbles back out through the hierarchy; ScriptUI again looks for handlers registered for the event with ancestor objects, starting with the immediate parent, and working back up the hierarchy to the topmost ancestor. When ScriptUI finds a handler, it executes it and the event propagation is complete.\n    \n  \n\n  \n    That said, I&#8217;ve worked around the problem nesting the ListBox in a Group (with its own handler):\n  \n\n  var win = new Window('dialog');\nwin.grp = win.add('group');\nwin.grp.lbx = win.grp.add('listbox', undefined, undefined, {items:['First Item', 'Second Item', 'Third Item', 'Fourth Item', 'Fifth Item']});\nwin.btn = win.add('button', undefined, \"Select Random\");\n\nwin.grp.addEventListener('click', function(event) {\n    $.writeln(\"ListBox 'change' Handler [\" + event.eventPhase + \"]\")\n}, true );\n\nwin.btn.addEventListener('click', function(event){\n    var i = Math.floor( 5* Math.random() );\n    win.grp.lbx.children[i].selected = true;\n    $.writeln(\"Clicked Button\");\n}, false );\n\nwin.show()\n\n  \n    The Group listens for 'click', (if you set the third argument to true, it will report the event phase as capture, otherwise as bubble), so it intercepts the clicks first and fires the handler, which can&#8217;t be triggered by the button. It works, but even if the ListBox doesn&#8217;t change (i.e. you click the same item that is already selected), it fires the handler anyway.\n  \n\n  \n    &nbsp;\n  \n\n\n\n&lt;a href=”javascript:void(0);” myshare_id=”mys_shareit” myshare_url=”/2013/09/scriptui-tip-decoupling-components-event-handling/” myshare_title=”ScriptUI tip: decoupling components’ Event handling” rel=”nofollow” onclick=” return false;” style=”text-decoration:none; color:#000000; font-size:11px; line-height:20px;”&gt;\n",
      tags: ["bubbling","Event","Extendscript","ScriptUI"],
      id: 71
    });
    

    index.add({
      title: "Testing minified JS Libraries in ExtendScript",
      category: ["Coding","ExtendScript / Javascript"],
      content: "\n  \n    It&#8217;s not uncommon, when scripting for Adobe applications, to borrow JS libraries that have been originally written for web development. While the new generation of HTML Extensions will run on the Chromium Embedded Framework, traditional Adobe ExtendScript code is based upon the implementation of a different, older Javascript engine. Besides ECMAScript unsupported features (i.e. ES 5) I&#8217;ve noticed that using minified JS libraries is a risky business &#8211; scripts can break or fail silently. I&#8217;ve set up a proper testing environment to inspect them.\n  \n\n  \n    Minified libraries\n  \n\n  \n    Web developers need to keep their data transfer footprints as light as possible so they minify their code &#8211; i.e. use engines such as Google Closure Compiler or UglifyJS2 to transform eloquent, commented and nicely indented code into an unreadable blob of characters &#8211; for instance as follow is a minified version of a CryptoJS library for Base64 encoding:\n  \n\n  (function(){var h=CryptoJS,k=h.f.c;h.e.b={stringify:function(b){var e=b.h,f=b.g,c=this.a;b.d();b=[];for(var a=0;a&lt;f;a+=3)for(var d=(e[a&gt;&gt;&gt;2]&gt;&gt;&gt;24-8*(a%4)&amp;255)&lt;&lt;16|(e[a+1&gt;&gt;&gt;2]&gt;&gt;&gt;24-8*((a+1)%4)&amp;255)&lt;&lt;8|e[a+2&gt;&gt;&gt;2]&gt;&gt;&gt;24-8*((a+2)%4)&amp;255,g=0;4&gt;g&amp;&amp;a+0.75*g&lt;f;g++)b.push(c.charAt(d&gt;&gt;&gt;6*(3-g)&amp;63));if(e=c.charAt(64))for(;b.length%4;)b.push(e);return b.join(\"\")},parse:function(b){var e=b.length,f=this.a,c=f.charAt(64);c&amp;&amp;(c=b.indexOf(c),-1!=c&amp;&amp;(e=c));for(var c=[],a=0,d=0;d&lt;e;d++)d%4&amp;&amp;(c[a&gt;&gt;&gt;2]|=(f.indexOf(b.charAt(d-\n1))&lt;&lt;2*(d%4)|f.indexOf(b.charAt(d))&gt;&gt;&gt;6-2*(d%4))&lt;&lt;24-8*(a%4),a++);return k.create(c,a)},a:\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=\"}})();\n\n  \n    Being generally installed on the machine the users work on, Adobe scripts don&#8217;t benefit from a reduction in filesize (which is pretty small per se) so developers don&#8217;t bother with minifiers; and ESTK can deploy a binary version, so code readability is not an issue.\n  \n\n  \n    A couple of pretty neat libraries that I use &#8211; and will test in this post &#8211; are:\n  \n\n  \n    \n      ES5-Shim &#8211; monkey-patch a JavaScript context to contain all EcmaScript 5 methods that can be faithfully emulated with a legacy JavaScript engine.\n    \n    \n      CryptoJS &#8211; a growing collection of standard and secure cryptographic algorithms implemented in JavaScript. I wrote a &#8220;CryptoJS For Dummies&#8221; tutorial some time ago that appears to be quite popular.\n    \n  \n\n  \n    Testing\n  \n\n  \n    Libraries that I will test are the actual ES5-Shim.js and the core.js + enc-Base64.js (routine for Base64 encoding, as you may guess) from CryptoJS. Below you can find a comparative view over the original files, the minified version provided by the libs authors and minified versions that I&#8217;ve compiled using UglifyJS (default params) and Closure Compiler (three versions: whitespaces only, simple and advanced &#8211; detailed info about their differences here).\n  \n\n  ============================================\nES5-Shim\nbytes - Library\n--------------------------------------------\n45658 - es5-shim.js\n16324 - es5-shim.min.js\n16308 - es5-shim-min-uglify.js\n16288 - es5-shim-min-closure-whitespace.js\n11400 - es5-shim-min-closure-simple.js\n11367 - es5-shim-min-closure-advanced.js\n\n============================================\nCryptoJS\nbytes - Library\n--------------------------------------------\n21468 - core.js\n 3298 - core-min.js\n 4973 - core-min-uglify.js\n 4964 - core-min-closure-whitespace.js\n 3176 - core-min-closure-simple.js\n 2682 - core-min-closure-advanced.js\n--------------------------------------------\n 3338 - enc-base64.js\n  869 - enc-base64-min.js\n 1251 - enc-base64-min-uglify.js\n 1245 - enc-base64-min-closure-whitespace.js\n  728 - enc-base64-min-closure-simple.js\n  674 - enc-base64-min-closure-advanced.js\n\n  \n    As you see the compression can be remarkable! (~21.000 -&gt; ~3.000 bytes). Let&#8217;s set up a test environment: I&#8217;ve created a Lib2Test folder in Photoshop CC/Presets/Scripts where I&#8217;ve put all the above listed files. In order to include a library in a jsx add the line:\n  \n\n  $.evalFile(\"\" + app.path + \"/\" + (localize(\"$$$/ScriptingSupport/InstalledScripts=Presets/Scripts\")) + \"/Libs2Test/&lt;yourLibName&gt;.js\");\n\n  \n    $.evalFile is more flexible than #include in my opinion, since you can load external code when you need it (i.e. testing a condition &#8211; if this happens, then I need this lib, else I don&#8217;t or I need some other lib).\n  \n\n  \n    ES5-Shim\n  \n\n  \n    I&#8217;ve written a simple evaluation test that cycle through all the original, provided minified version and custom made compression:\n  \n\n  var scriptPath = \"\" + app.path + \"/\" + (localize(\"$$$/ScriptingSupport/InstalledScripts=Presets/Scripts\")) + \"/Libs2Test/\";\nvar postfix = [\"\", \"-min\", \"-min-uglify\", \"-min-closure-whitespace\", \"-min-closure-simple\", \"-min-closure-advanced\"];\nvar libKind = \"es5-shim\";\nvar libName = undefined;\nfor (var i = 0; i &lt; postfix.length; i++) {\n\tlibName = \"\" + libKind + postfix[i] + \".js\";\n\ttry {\n\t\t$.evalFile(scriptPath + libName);\n\t\t$.writeln(\"Testing \" + libName + \": OK.\");\n\t} catch (e) {\n\t\t$.writeln(\"Testing \" + libName + \": ERROR \" + e.number + \"; \" + e.message);\n\t}\n}\n\n  \n    As a result, I&#8217;ve found that:\n  \n\n  Testing es5-shim.js: OK.\nTesting es5-shim-min.js: ERROR 25; Expected: :\nTesting es5-shim-min-uglify.js: ERROR 25; Expected: :\nTesting es5-shim-min-closure-whitespace.js: ERROR 25; Expected: :\nTesting es5-shim-min-closure-simple.js: ERROR 25; Expected: :\nTesting es5-shim-min-closure-advanced.js: ERROR 25; Expected: :\n\n  \n    Problem: for some reason &#8211; that I&#8217;m not willing to investigate &#8211; each and every compressed versions for Photoshop is indigestible. Conclusion: keep the original version!\n  \n\n  \n    CryptoJS\n  \n\n  \n    The test for the CryptoJS libraries is slightly more complex. First let&#8217;s just evaluate the core.js\n  \n\n  Testing core.js: OK.\nTesting core-min.js: OK.\nTesting core-min-uglify.js: OK.\nTesting core-min-closure-whitespace.js: OK.\nTesting core-min-closure-simple.js: OK.\nTesting core-min-closure-advanced.js: OK.\n\n  \n    Then (including core.js because it&#8217;s a dependency) the enc-Base64.js file:\n  \n\n  Testing enc-Base64.js: OK.\nTesting enc-Base64-min.js: OK.\nTesting enc-Base64-min-uglify.js: OK.\nTesting enc-Base64-min-closure-whitespace.js: OK.\nTesting enc-Base64-min-closure-simple.js: OK.\nTesting enc-Base64-min-closure-advanced.js: ERROR 21; undefined is not an object\n\n  \n    Problem: apparently there are no evaluation errors, but for the Base64 module when compressed with Closure (advanced).\n  \n\n  \n    Let&#8217;s do a functional test on the minified versions of core.js &#8211; true, it&#8217;s evaluated without errors, but this doesn&#8217;t mean it works as expected. The following is not the most bulletproof test in the northern hemisphere, but should work. Basically I&#8217;m converting a Latin1 encoded String to a WordArray, then the reverse (WordArray to Latin1). If the two strings match, the test is passed. (If you have doubts about how CryptoJS works, please have a look at my my tutorial to refresh your 007 skills).\n  \n\n  var scriptPath = \"\" + app.path + \"/\" + (localize(\"$$$/ScriptingSupport/InstalledScripts=Presets/Scripts\")) + \"/Libs2Test/\";\nvar postfix = [\"\", \"-min\", \"-min-uglify\", \"-min-closure-whitespace\", \"-min-closure-simple\", \"-min-closure-advanced\"];\nvar libKind = \"core\";\nvar text = \"My name is Bond, James Bond.\";\nvar libName, Latin1ToWA, WAToLatin1, result;\n\nfor (var i = 0; i &lt; postfix.length; i++) {\n\tlibName = \"\" + libKind + postfix[i] + \".js\";\n\t$.evalFile(scriptPath + libName);\n\n\tLatin1ToWA = CryptoJS.enc.Latin1.parse(text);\t\t// Latin1 to WordArray\n\tWAToLatin1 = CryptoJS.enc.Latin1.stringify(Latin1ToWA);\t// WordArray to Latin1\n\n\tresult = text === WAToLatin1 ? \"Passed\" : \"Failed\";\n\t$.writeln(\"Testing \" + libName + \": \" + result);\n}\n\n  \n    The results are encouraging:\n  \n\n  Testing core.js: Passed\nTesting core-min.js: Passed\nTesting core-min-uglify.js: Passed\nTesting core-min-closure-whitespace.js: Passed\nTesting core-min-closure-simple.js: Passed\nTesting core-min-closure-advanced.js: Passed\n\n  \n    Apparently &#8211; as long as this simple test is concerned &#8211; core.js has no functional problems.\n  \n\n  \n    Let&#8217;s involve the enc-base64.js library: this time the test will be a roundtrip:\n  \n\n  \n    \n      from Latin1 to Word Array;\n    \n    \n      from WordArray to Base64;\n    \n    \n      from Base64 back to WordArray;\n    \n    \n      from WordArray back to Latin1.\n    \n  \n\n  \n    If the two Latin1 strings are identical, the test is passed. I&#8217;ve removed the version compressed with Closure Compiler (advanced) because it gives evaluation errors.\n  \n\n  var scriptPath = \"\" + app.path + \"/\" + (localize(\"$$$/ScriptingSupport/InstalledScripts=Presets/Scripts\")) + \"/Libs2Test/\";\n$.evalFile(scriptPath + \"core.js\"); // needed by the enc-Base64 library\nvar postfix = [\"\", \"-min\", \"-min-uglify\", \"-min-closure-whitespace\", \"-min-closure-simple\"];\nvar libKind = \"enc-Base64\";\nvar text = \"My name is Bond, James Bond.\";\nvar libName, Latin1ToWA, WAToBase64, Base64ToWA, WAToLatin1, result;\n\nfor (var i = 0; i &lt; postfix.length; i++) {\n\tlibName = \"\" + libKind + postfix[i] + \".js\";\n\t$.evalFile(scriptPath + libName);\n\n\tLatin1ToWA = CryptoJS.enc.Latin1.parse(text);           // Latin1 to WordArray\n\tWAToBase64 = CryptoJS.enc.Base64.stringify(Latin1ToWA); // WordArray to Base64\n\tBase64ToWA = CryptoJS.enc.Base64.parse(WAToBase64);     // Base64 to WordArray\n\tWAToLatin1 = CryptoJS.enc.Latin1.stringify(Base64ToWA); // WordArray to Latin1\n\n\tresult = text === WAToLatin1 ? \"Passed\" : \"Failed\";\n\t$.writeln(\"Testing \" + libName + \": \" + result);\n}\n\n  \n    Mixed results:\n  \n\n  Testing enc-Base64.js: Passed\nTesting enc-Base64-min.js: Failed\nTesting enc-Base64-min-uglify.js: Passed\nTesting enc-Base64-min-closure-whitespace.js: Passed\nTesting enc-Base64-min-closure-simple.js: Failed\n\n  \n    Problem: the minified file provided by CryptoJS authors and a version compressed with Closure (simple) fail. If you&#8217;re curious, their converted strings is miBdJeBd. Digression: you can&#8217;t directly output in the console the &#8220;wrong&#8221; string, you&#8217;ve to loop through it this way (mind you the string length appears to be correct, 28):\n  \n\n  if (result ===\"Failed\") {\n    var str = \"\"\n        for (var j = 0; j &lt; WAToLatin1.length; j++) {\n            str += WAToLatin1.charAt(j);\n        }\n    $.writeln(\"String is: \" + \"'\" + str + \"'\");\n}\n\n  \n    Conclusions\n  \n\n  \n    These tests &#8211; even if trivially simple &#8211; have shown that there&#8217;s a dangerous variability in the different output minified code can lead to.\n  \n\n  \n    \n      2/3 of the minified versions provided by Libraries authors failed a simple evaluation test.\n    \n    \n      Closure Compiler (advanced) has the highest failure rate.\n    \n    \n      UglifyJS and Closure (whitespace) have the highest success rate, though not 100%. Returning more or less the same filesize, they can be used interchangeably.\n    \n    \n      Beware minified version, stick to uncompressed code if you can afford some extra bytes on the disk! You&#8217;ll save yourself some useless debug time!\n    \n  \n\n  \n    \n  \n\n  \n    Mind you: this post is far from being an exhaustive review of code minification in ExtendScript- yet should suggest that an appropriate testing environment is a useful tool &#8211; so test your code!\n\n    \n\n    \n\n    \n\n    \n\n    \n  \n\n",
      tags: ["Extendscript","Javascript","libraries","minify","testing"],
      id: 72
    });
    

    index.add({
      title: "ScriptUI Events: call(), dispatchEvent(), notify()",
      category: ["Coding","ExtendScript / Javascript"],
      content: "\n  \n    In your code you may need to run a ScriptUI component&#8217;s callback, or simulate a user interaction, maybe as a part of a subroutine. There are few ways to do this, with slight differences in the behavior: directly, using Javascript methods call(), notify() or dispatchEvent() &#8211; I&#8217;ve set up a commented demo Dialog that shows them all.\n  \n\n  \n    The Dialog\n  \n\n  \n    I&#8217;ve used a resource string to build the initial dialog (please scroll down to the post&#8217;s end to find the example completed with listeners, callbacks and button functions):\n  \n\n  var windowRef = \"dialog { \\\n\ttext: 'ScriptUI Events - www.davidebarranca.com', \\\n\torientation: 'column', \\\n\talignChildren: 'fill', \\\n\tcbGroup: Group { \\\n\t\tcbPanel: Panel { \\\n        preferredSize: ['155',''], \\\n\t\t\talignChildren: 'left', \\\n\t\t\tcb1: Checkbox { text: 'CheckBox #1'}, \\\n\t\t\tcb2: Checkbox { text: 'CheckBox #2'}, \\\n\t\t\tcb3: Checkbox { text: 'CheckBox #3'}, \\\n\t\t\tcb4: Checkbox { text: 'CheckBox #4'}, \\\n\t\t}, \\\n\t\tbtGroup: Group { \\\n             preferredSize: ['170',''], \\\n\t\t\talignChildren: ['fill','center'], \\\n\t\t\torientation: 'column', \\\n\t\t\ttestCb1: Button { text: '#1 - onClick ()' }, \\\n\t\t\ttestCb2: Button { text: '#2 - notify ()' }, \\\n\t\t\ttestCb3: Button { text: '#2 - dispatchEvent ()' }, \\\n\t\t\ttestCb4: Button { text: '#1 - call (CB #3)' }, \\\n\t\t}, \\\n\t}, \\\n    spacer: Panel { size: [100,0]}, \\\n\trbGroup: Group { \\\n\t\trbPanel: Panel { \\\n        preferredSize: ['155',''], \\\n        alignChildren: 'left', \\\n\t\t\trb1: RadioButton { text: 'RadioButton #1', value: 'true'}, \\\n\t\t\trb2: RadioButton { text: 'RadioButton #2'}, \\\n\t\t}, \\\n        btGroup: Group { \\\n             preferredSize: ['170',''], \\\n\t\t\talignChildren: ['fill','center'], \\\n\t\t\torientation: 'column', \\\n\t\t\ttestRb1: Button { text: 'Panel - dispatchEvent ()' }, \\\n\t\t\ttestRb2: Button { text: 'RB #2 - notify ()' }, \\\n\t\t}, \\\n\t}, \\\n    spacer: Panel { size: [100,0]}, \\\n    closeBtn: Button { text: 'Close' }\\\n}\";\n\n// Build the Window\nvar w = new Window(windowRef);\nw.closeBtn.onClick = function() { w.close() };\nw.show();\n\n  \n    Regular Listeners and Callbacks\n  \n\n  \n    Let&#8217;s code for the CheckBoxes a response to the 'click' event in two different ways:\n  \n\n  // Shortcuts for cb\nvar cb1 = w.cbGroup.cbPanel.cb1;\nvar cb2 = w.cbGroup.cbPanel.cb2;\nvar cb3 = w.cbGroup.cbPanel.cb3;\n\n// CheckBox #1 onClick listener: no event is passed.\ncb1.onClick = function(event) {\n\talert(\"Clicked checkbox: \" + this.text + \"\\nValue: \" + this.value);\n\t// uncomment the following alert to test that no event is passed:\n\t// alert(\"Using event.target.value: \" + event.target.value); // Error: event is undefined\n};\n\n// CheckBox #2 'click' listener: an event is passed,\n// and the alert correctly shows the event.target.value\ncb2.addEventListener('click', function(event) {\n\talert(\"Clicked checkbox: \" + this.text + \"\\nValue: \" + event.target.value);\n});\n\n  \n    The first CheckBox registers an onClick event: mind you, there&#8217;s no Event actually passed to the callback. If you uncomment the second alert, you see that event.target.value is undefined.\n  \n\n  \n    The second CheckBox uses addEventListener, listening for the 'click' event with an anonymous callback. Here, an actual Event is passed.\n  \n\n  \n    For the RadioButtons, I&#8217;ve set up a callback triggered by clicks on the container (i.e. the Panel) level:\n  \n\n  // Shortcut for the RB panel\nvar rbPanel = w.rbGroup.rbPanel;\n\n// RadioButton 'click' listener\n// alerts the active RB text (works differently on PS and ESTK)\nrbPanel.addEventListener('click', function(event) {\n    for (var i = 0; i &lt; rbPanel.children.length; i++) {\n        if (rbPanel.children[i].value == true) {\n            alert(\"Active: \" + rbPanel.children[i].text);\n            return;\n         }\n     }\n});\n\n  \n    The loop finds the selected RadioButton and alerts its name: mind you, this works differently on Photoshop and ESTK because of the way they deal with event propagation.\n  \n\n  \n    \n      Photoshop gets the 'click' on the Panel, lets it go through it, down to the RadioButton, then fires the callback &#8211; which evaluates the RadioButtons status and returns the name of the selected one: that is, the one you&#8217;ve clicked.\n    \n    \n      ESTK gets the 'click' on the Panel and immediately fires the callback, before allowing the 'click' to reach the RadioButton: this way the alerted name isn&#8217;t the one of RadioButton you&#8217;ve clicked on, but the one that was selected before. When the callback is done, the 'click' is allowed to propagate to the RadioButton, that is finally selected.\n    \n  \n\n  \n    Simulated events: CheckBox\n  \n\n  \n    This is the code for the four checkbox&#8217;s buttons, which duty is to trigger the callbacks and sometimes simulate user interaction:\n  \n\n  // Button: test the CB#1 callback via direct cb1.onClick() call.\n// Result: the callback's alert shows, but the checkbox isn't toggled\nw.cbGroup.btGroup.testCb1.onClick = function() { cb1.onClick() };\n\n// Button: test the CB#1 callback via cb1.notify() call.\n// Result: the callback's alert shows, and the checkbox is toggled\nw.cbGroup.btGroup.testCb2.onClick = function() { cb2.notify('onClick') };\n//w.cbGroup.btGroup.testCb2.onClick = function() { cb1.notify(\"onClick\") }; // works as well\n\n// Button: test the CB#2 callback via dispatchEvent() call.\n// Result: the callback's alert shows, but the checkbox isn't toggled\nw.cbGroup.btGroup.testCb3.onClick = function() { cb2.dispatchEvent(new UIEvent ('click')) };\n\n// Button: uses the onClick.call() method of CB#1 passing the CB#2 as parameter\n// Result: the CB#2 'borrows' the callback from the CB#1\n// (i.e. the CB#2 becomes the callback's 'this')\nw.cbGroup.btGroup.testCb4.onClick = function() { cb1.onClick.call(cb3) };\n\n  \n    The first button directly calls the CheckBox #1 callback via cb1.onClick(). Mind you, onClick() is a regular method of the cb1 component, so it can be called anytime like any other method of any other object. The callback is fired but the CheckBox is not toggled.\n  \n\n  \n    Conversely, the second button uses .notify() to simulate the user interaction, as if the CheckBox #1 was actually clicked. As a result, its value (checked/unchecked, i.e. true/false) is toggled and the callback is fired. Mind you, the notify() parameter in this case is optional &#8211; not in other cases when you might want to simulate, say, a Window.notify('onMove') event.\n  \n\n  \n    The third button acts on CheckBox #2, which registers the listener and the callback via .addEventListener(). To trigger its callback (since .onClick() doesn&#8217;t exist) you&#8217;ve to make the component dispatch a new Event via .dispatchEvent(): as a result the callback is fired but the CheckBox is not toggled. If you want to simulate user interaction, you can always rely on .notify().\n  \n\n  \n    The fourth button uses .call() to borrow from the CheckBox #1 the callback, and temporarily apply it to the CheckBox #3 (that hasn&#8217;t any .onClick() method). The .call() parameter cb3 is used as a &#8220;this&#8221; reference: so when the function needs to evaluate this.value, it doesn&#8217;t refer anymore to cb1 (the actual &#8220;owner&#8221; of the callback) but to cb3. As a result the alert is fired but the CheckBox isn&#8217;t toggled.\n  \n\n  \n    Simulated events: RadioButton\n  \n\n  \n    Things are similar when it comes to the RadioButtons:\n  \n\n  // Button: test the RadioButton Panel via dispatchEvent()\n// Result: triggers the Panel's callback and fires the alert\nw.rbGroup.btGroup.testRb1.onClick = function() { rbPanel.dispatchEvent(new UIEvent ('click')) };\n\n// Button: test the RadioButton #2 via notify()\n// Result: selects the second RadioButton and fires the Panel's callback\nw.rbGroup.btGroup.testRb2.onClick = function() { w.rbGroup.rbPanel.rb2.notify('onClick') };\n\n  \n    The first button uses .dispatchEvent() on the container level (i.e. it&#8217;s the Panel that fires the UIEvent), so that the callback is triggered and the alert shown.\n  \n\n  \n    The second button instead uses .notify() directly on the RadioButton simulating a user click. As a result, the RadioButton #2 is selected and the callback is fired (at least it is fired in Photoshop: ESTK ignores the callback for some reason). Mind you, .addEventListener() and dispatchEvent() uses 'click' as the Event, while what you notify() is an 'onClick'.\n  \n\n  \n    Conclusion and full script\n  \n\n  \n    Hopefully this has shed some light on the different possibilities you&#8217;ve to run or trigger callbacks and simulate user interaction with ScriptUI components. This appears to be a field where the ExtendScript implementation differs a lot among the host apps (ESTK, Photoshop, InDesign, etc…). I&#8217;ve tried out the above on ESTK and PS only, so please test your portings!\n  \n\n  \n    \n  \n\n  // Davide Barranca - www.davidebarranca.com\n// V1.0 - 2014.08.04\n\nvar windowRef = \"dialog { \\\n    text: 'ScriptUI Events - www.davidebarranca.com', \\\n\torientation: 'column', \\\n\talignChildren: 'fill', \\\n\tcbGroup: Group { \\\n\t\tcbPanel: Panel { \\\n        preferredSize: ['155',''], \\\n\t\t\talignChildren: 'left', \\\n\t\t\tcb1: Checkbox { text: 'CheckBox #1'}, \\\n\t\t\tcb2: Checkbox { text: 'CheckBox #2'}, \\\n\t\t\tcb3: Checkbox { text: 'CheckBox #3'}, \\\n\t\t\tcb4: Checkbox { text: 'CheckBox #4'}, \\\n\t\t}, \\\n\t\tbtGroup: Group { \\\n             preferredSize: ['170',''], \\\n\t\t\talignChildren: ['fill','center'], \\\n\t\t\torientation: 'column', \\\n\t\t\ttestCb1: Button { text: '#1 - onClick ()' }, \\\n\t\t\ttestCb2: Button { text: '#2 - notify ()' }, \\\n\t\t\ttestCb3: Button { text: '#2 - dispatchEvent ()' }, \\\n\t\t\ttestCb4: Button { text: '#1 - call (CB #3)' }, \\\n\t\t}, \\\n\t}, \\\n    spacer: Panel { size: [100,0]}, \\\n\trbGroup: Group { \\\n\t\trbPanel: Panel { \\\n        preferredSize: ['155',''], \\\n        alignChildren: 'left', \\\n\t\t\trb1: RadioButton { text: 'RadioButton #1', value: 'true'}, \\\n\t\t\trb2: RadioButton { text: 'RadioButton #2'}, \\\n\t\t}, \\\n        btGroup: Group { \\\n             preferredSize: ['170',''], \\\n\t\t\talignChildren: ['fill','center'], \\\n\t\t\torientation: 'column', \\\n\t\t\ttestRb1: Button { text: 'Panel - dispatchEvent ()' }, \\\n\t\t\ttestRb2: Button { text: 'RB #2 - notify ()' }, \\\n\t\t}, \\\n\t}, \\\n    spacer: Panel { size: [100,0]}, \\\n    closeBtn: Button { text: 'Close' }\\\n}\";\n\n// Build the Window\nvar w = new Window(windowRef);\n\n/* =======================================\n   CheckBox listeners and callbacks\n   ======================================= */\n\n// Shortcuts for cb\nvar cb1 = w.cbGroup.cbPanel.cb1;\nvar cb2 = w.cbGroup.cbPanel.cb2;\nvar cb3 = w.cbGroup.cbPanel.cb3;\n\n// CheckBox #1 onClick listener: no event is passed.\ncb1.onClick = function(event) {\n\talert(\"Clicked checkbox: \" + this.text + \"\\nValue: \" + this.value);\n\t// uncomment the following alert to test that no event is passed:\n\t// alert(\"Using event.target.value: \" + event.target.value); // Error: event is undefined\n};\n\n// CheckBox #2 'click' listener: an event is passed,\n// and the alert correctly shows the event.target.value\ncb2.addEventListener('click', function(event) {\n\talert(\"Clicked checkbox: \" + this.text + \"\\nValue: \" + event.target.value);\n});\n\n// Button: test the CB#1 callback via direct cb1.onClick() call.\n// Result: the callback's alert shows, but the checkbox isn't toggled\nw.cbGroup.btGroup.testCb1.onClick = function() { cb1.onClick() };\n\n// Button: test the CB#1 callback via cb1.notify() call.\n// Result: the callback's alert shows, and the checkbox is toggled\nw.cbGroup.btGroup.testCb2.onClick = function() { cb2.notify('onClick') };\n//w.cbGroup.btGroup.testCb2.onClick = function() { cb1.notify(\"onClick\") }; // works as well\n\n// Button: test the CB#2 callback via dispatchEvent() call.\n// Result: the callback's alert shows, but the checkbox isn't toggled\nw.cbGroup.btGroup.testCb3.onClick = function() { cb2.dispatchEvent(new UIEvent ('click')) };\n\n// Button: uses the onClick.call() method of CB#1 passing the CB#2 as parameter\n// Result: the CB#2 'borrows' the callback from the CB#1\n// (i.e. the CB#2 becomes the callback's 'this')\nw.cbGroup.btGroup.testCb4.onClick = function() { cb1.onClick.call(cb3) };\n\n/* =======================================\n   RadioButton listeners and callback\n   ======================================= */\n\n// Shortcut for the RB panel\nvar rbPanel = w.rbGroup.rbPanel;\n\n// RadioButton 'click' listener\n// alerts the active RB text (works differently on PS and ESTK)\nrbPanel.addEventListener('click', function(event) {\n    for (var i = 0; i &lt; rbPanel.children.length; i++) {\n        if (rbPanel.children[i].value == true) {\n            alert(\"Active: \" + rbPanel.children[i].text);\n            return;\n         }\n     }\n});\n\n// Button: test the RadioButton Panel via dispatchEvent()\n// Result: triggers the Panel's callback and fires the alert\nw.rbGroup.btGroup.testRb1.onClick = function() { rbPanel.dispatchEvent(new UIEvent ('click')) };\n\n// Button: test the RadioButton #2 via notify()\n// Result: selects the second RadioButton and fires the Panel's callback\nw.rbGroup.btGroup.testRb2.onClick = function() { w.rbGroup.rbPanel.rb2.notify() };\n\n/* Close Button */\nw.closeBtn.onClick = function() { w.close() };\n\nw.show();\n\n  \n    &nbsp;\n  \n\n",
      tags: ["call()","callback","dispatchEvent()","Event","notify()","ScriptUI"],
      id: 73
    });
    

    index.add({
      title: "L&#8217;App per iPad di TIAB: Sharpening in Photoshop",
      category: ["Photoshop @it"],
      content: "\n  \n    \n      Teacher in a Box ha pubblicato su App Store la versione per iPad del suo &#8220;Videocorso per Photoshop Sharpening: Dettaglio e Nitidezza perfetti nelle fotografie digitali&#8220;, registrato da Marco Olivotto. Si tratta di un&#8217;App decisamente sostanziosa (678MB per €59.99), i cui contenuti video comprendono sia i fondamentali teorici che gran parte delle tecniche in Photoshop. Spesso sul mio blog ho scritto di questi argomenti, quindi la recensisco volentieri: dimenticando per il tempo necessario che Marco non è solo un collega ma anche un amico e ringraziando TIAB per la possibilità di averla avuta in anteprima.\n    \n\n    \n      La App\n    \n\n    \n      Tra i supporti disponibili per un videocorso (DVD o Lezionario online), l&#8217;app per iPad è probabilmente la migliore combinazione tra proprietà dei contenuti e modalità di visualizzazione &#8211; essere slegati dal computer in questo caso è un vantaggio, potendo tenere il tablet accanto e mettere in pratica quanto Marco spiega senza rimbalzare continuamente tra Photoshop ed il player.\n    \n\n    \n      \n    \n\n    \n      Superato lo scoglio di scaricare tutti i dati &#8211; il che non sempre è banale in Italia &#8211; l&#8217;app in sé è ben congegnata. Il layout è compatto, la suddivisione in menu a fisarmonica della lista dei video evita in gran parte lo scrolling e per le funzioni di navigazione di base (cioè per il 95% dell&#8217;utilizzo dell&#8217;app da parte del 95% degli utenti) tutto succede in un&#8217;unica vista senza necessità di aprire pop-up o pagine secondarie. Ottima usabilità in genere; un po&#8217; meno efficaci forse i menu ad icone, sdoppiati in due colonne ai bordi dello schermo.\n    \n\n    \n      I video vanno sia a pieno schermo che in riquadro, c&#8217;è la possibilità di impostare segnalibri, costruire una playlist personale o filtrarli in base a termini di ricerca. Alcuni utenti (pochi o molti non so dire, sicuramente il sottoscritto) sentiranno la mancanza di controlli extra sul player rispetto a quelli di default di iOS: mi riferisco alla possibilità di riprodurre il video a velocità superiori (come in VLC: 1.3x o più, ad esempio) che il più delle volte è davvero utile. Altra feature request per i prossimi aggiornamenti: al termine di un video, il passaggio automatico al successivo in lista, operazione al momento manuale.\n    \n\n    \n      Il Teacher\n    \n\n    \n      Lo stile di Marco Olivotto, pur in evoluzione da quando ha cominciato ad insegnare i suoi CCC (Color Correction Campus, workshop intensivi di due giorni che dal 2011 ha tenuto in svariate città italiane), è definibile in un efficace mix di rigore scientifico e senso pratico. Il linguaggio è molto preciso, il modo di parlare chiaro e la sintassi neutra. Rispetto ai corsi dal vivo, nei quali può permettersi variazioni di ritmo e qualche salto logico per assecondare le necessità del momento degli studenti, nel videocorso (per sua natura un media che richiede una impostazione più regolare, sia per esigenze di tipo didattico che per questioni di accuratezza) adotta uno stile formalmente ineccepibile dal sapore British, una specie di Piero Angela della Color Correction. Gli esempi pratici sono tutti estremamente chiari nel loro svolgimento e la teoria affrontata a passo regolare in tutte le sue (non sempre semplici) sfaccettature.\n    \n\n    \n      Il Corso\n    \n\n    \n      Marco Olivotto ha deciso di suddividere le 3 ore, 35 minuti (e 7 secondi, pare) del corso in sette parti di circa mezz&#8217;ora, più una breve conclusione:\n    \n\n    \n      \n        Introduzione\n      \n      \n        Strumenti per iniziare\n      \n      \n        Maschera di Contrasto: concetti di base\n      \n      \n        La Maschera di Contrasto in azione\n      \n      \n        Il Flusso di lavoro di Bruce Fraser\n      \n      \n        Esempi ed applicazioni\n      \n      \n        Argomenti avanzati e trucchi\n      \n      \n        Conclusioni\n      \n    \n\n    \n      Per ragioni di tempo alcuni argomenti e/o tecniche sono evidentemente rimasti fuori, ma è piuttosto notevole la quantità (e qualità) di considerazioni che trovano spazio in questo raggruppamento.\n    \n\n    \n      La Introduzione affronta di petto (ed efficacemente) concetti fondamentali quali Nitidezza, Dettaglio, Scala e Frequenza Spaziale; rispondendo alla domanda &#8220;Perché mai occorre fare sharpening?&#8221; nell&#8217;unico modo sensato &#8211; ovvero richiamando la differenza tra la visione come fenomeno ottico (cosa vede una fotocamera) e la percezione (cosa il nostro cervello, sulla base dei dati che riceve dai nostri occhi, capisce del mondo là fuori) fino al paradosso che gli artefatti della maschera di contrasto sono necessari per rendere la riproduzione del vero più verosimile ai nostri occhi (e di nuovo al nostro cervello). Altra domanda che viene posta nell&#8217;introduzione è la classica e ricorrente &#8220;In che punto del flusso di lavoro è opportuno applicare la maschera di contrasto?&#8221;. Infine si divulga una tecnica dal nome insolito (La Mazzetta) e dal sottotitolo ambizioso (Oggetto Universale) che ho presentato in un suo workshop nel 2012 e che sembra essere stata generalmente ben recepita.\n    \n\n    \n      Negli Strumenti per Iniziare sono raccolte informazioni di base per essere operativi, ad uso degli utenti meno avanzati: filtri tipici per lo sharpening (Sfocatura Gaussiana, Accentua Passaggio, Maschera di Contrasto, Trova Bordi), fusione condizionale e maschere di livello.\n    \n\n    \n      \n    \n\n    \n      Coi Concetti di Base cominciamo ad esplorare pro e contro della Maschera di Contrasto nei diversi spazi colore (RGB, CMYK e Lab), non senza aver prima scoperto che i pittori del Rinascimento già introducevano artefatti nelle loro opere &#8211; gli stessi aloni chiari e scuri nei bordi che sono la base dello sharpening. Marco comincia anche a presentare alcune considerazioni qualitative che Dan Margulis (suo e mio Maestro) ha fatto negli anni scorsi a proposito di questi artefatti: percepiamo gli aloni chiari e quelli scuri nello stesso modo? In quali parti della gamma tonale siamo meglio disposti ad accettare lo sharpening? Le risposte determineranno parte delle tecniche trattate nella sezione successiva.\n    \n\n    \n      La Maschera di Contrasto in azione affronta sia il classico dilemma della scelta dei parametri del filtro UnSharpMask, che la cosiddetta Maschera di Contrasto inversa (HiRaLoAm &#8211; High Radius Low Amount, cioè a raggi alti e fattori bassi) utile per stimolare frequenze medio basse. Con un video sulle maschere per i bordi, Marco si avvicina verso metà corso ad un punto di discontinuità con quanto ha trattato fin&#8217;ora.\n    \n\n    \n      Ovvero il Flusso di Lavoro di Bruce Fraser. Fraser è stato, nel cosiddetto gruppo dei Pixel Genius (che comprende tra gli altri Jeff Schewe e Martin Evening &#8211; tutti in qualche modo vicini ad Adobe) colui che ha approfondito di più la ricerca sullo Sharpening, scrivendo il primo libro interamente dedicato all&#8217;argomento (Real World Image Sharpening). Poco tempo dopo la pubblicazione Fraser è venuto a mancare, ed il testimone per l&#8217;edizione successiva è stato raccolto da Schewe, che ne ha grosso modo mantenuto l&#8217;impianto e i principi. Presentando Bruce Fraser, Marco mette a confronto due punti di vista completamente diversi &#8211; e sotto molti aspetti inconciliabili &#8211; sul problema della Maschera di Contrasto. Da una parte Dan Margulis, che rappresenta (quello che un tempo veniva identificato come) il mondo della prestampa; le sue linee guida derivano dalla ricerca personale sugli aspetti percettivi della Color Correction. Per Dan lo sharpening:\n    \n\n    \n      \n        E&#8217; come da tradizione da applicare in un&#8217;unica passata, al termine del flusso di lavoro.\n      \n      \n        Differenziato qualitativamente e quantitativamente tra Aloni Chiari ed Aloni Scuri.\n      \n      \n        Applicato attraverso maschere di luminosità, o provenienti da canali elaborati.\n      \n    \n\n    \n      Dall&#8217;altra parte Bruce Fraser ed il suo gruppo rappresenta forse più il mondo della fotografia; secondo i suoi dettami lo sharpening:\n    \n\n    \n      \n        Può essere applicato in più passaggi (di Input, Creativo, di Output) in momenti diversi del flusso di lavoro.\n      \n      \n        Va applicato generalmente attraverso maschere &#8220;Trova Bordi&#8221;.\n      \n    \n\n    \n      Non è un mistero che storicamente Dan Margulis ha avuto pochi fan in Adobe, mentre i Pixel Genius hanno spesso collaborato come consulenti. Sia quel che sia, il modulo dello sharpening di Camera Raw ha uno slider che dinamicamente modifica la soglia di un filtro trovabordi, per cui sembra che Adobe abbia scelto da che parte stare. E Marco Olivotto? Riconosce che i due punti di vista sono largamente inconciliabili, tendenzialmente ne preferisce uno, ma li spiega e dimostra entrambi, scegliendo ora l&#8217;uno ora l&#8217;altro in relazione alle immagini di esempio.\n    \n\n    \n      \n    \n\n    \n      Negli Esempi ed Applicazioni (sezione più lunga di tutte, circa tre quarti d&#8217;ora) Marco tenta una sistematizzazione di genere, approcciando con tecniche diverse tre ritratti, due architetture, un paesaggio, una fotografia di scena, un tessuto, e affrontando il problema della riduzione del rumore negli scatti digitali ad alte sensibilità. L&#8217;ultima sezione, Argomenti Avanzati e Trucchi è una raccolta di temi ausiliari, quali gli altri strumenti di Sharpening (da Nitidezza Avanzata in giù), HiRaLoAm su a e b di Lab, salvataggio di originali con oversharpening, e (last but not least) la Maschera di Contrasto in Adobe Camera Raw. Ultimo argomento molto &#8220;caldo&#8221; &#8211; nel quale Marco compara ACR e PS giungendo a conclusioni che non posso che condividere, basate su considerazioni che riguardano praticità, velocità, qualità ed automazione.\n    \n\n    \n      Conclusioni\n    \n\n    \n      E&#8217; sempre difficile essere imparziali quando si tratta di dare un parere sul lavoro di colleghi coi quali si ha un rapporto di amicizia\n\n      \n\n      \n      . Ciò detto, mi pare che l&#8217;app di TIAB sia\n      veramente ben fatta e consigliabile, e meriti i 59.99 EUR che costa su App Store. La maschera di contrasto è un argomento dai confini estesi e non sempre ovvi e Marco Olivotto ha registrato un videocorso che è fruibile a più livelli: da un lato getta delle basi teoricamente molto forti ad uso dei principianti, in modo da segnare nettamente il proprio raggio d&#8217;azione, dall&#8217;altro non perde mai di vista chi è più smaliziato (mi piacerebbe sapere quanti corsi sullo Sharpening citano Jan Koenderink).&lt;/p&gt;\n\n      \n        Gli esempi sono in genere appropriati, e definiscono un target di utenza che personalmente individuo tra il principiante e l&#8217;intermedio. Data questa impostazione, Marco riesce a toccare molti argomenti importanti, costruendo un corso di carattere &#8220;ecumenico&#8221; &#8211; allo stesso tempo maggior pregio e forse anche difetto del corso stesso, che manca di un&#8217;impronta stilistica forte, prediligendo un&#8217;esposizione neutra e più didascalica di principi e tecniche, senza però rinunciare all&#8217;accuratezza.\n      \n\n      \n        Per quanto sono convinto che anche l&#8217;utenza cosiddetta professionale possa trovare in questo videocorso un&#8217;ottima sintesi in grado di completare le proprie conoscenze sull&#8217;argomento (siete sicuri di saper rispondere con certezza a tutte le domande che ho sparso in questo post?) è auspicabile che TIAB produca prima o poi un secondo videocorso nel quale si affrontino temi e tecniche più avanzate, quali la separazione degli aloni, l&#8217;automazione di Dan Margulis, Sharpening multiplo e maschere avanzate, ecc. Tutte frecce già nella faretra di Marco Olivotto, che per comprensibili ragioni non hanno potuto essere scoccate in questa occasione. &lt;/span&gt;\n      &lt;/div&gt; &lt;/div&gt;\n      \n\n",
      tags: ["Marco Olivotto","Sharpening @it","Teacher in a Box"],
      id: 74
    });
    

    index.add({
      title: "CPT &#8211; Channel Power Tools for Photoshop CC",
      category: ["Extensions and Scripts","Photoshop"],
      content: "\n  \n    \n  \n\n  \n    I&#8217;m happy to report that my dear friend and talented coder Giuliana Abbiati aka Cromaline has just updated to version 1.1.3 her amazing Photoshop extension CPT &#8211; Channel Power Tools. in order to support Photoshop CC.\n  \n\n  \n    If you&#8217;ve never used it, CTP is the the Swiss Army knife of Channels Operations (channel chops anyone?), with on-the-fly application of Channels from any colorspace (HSB/HSL and Bogus Black included) and live preview and composite generation of RGB-CMYK-Lab channels. Did I mention that is freaking fast? Well, it is.\n  \n\n  \n    To keep herself busy, Cromaline is also the software architect of Dan Margulis&#8216; Modern Photoshop Color Workflow book companion PPW Panel for Photoshop &#8211; that she managed to fill up with fine scripts and smart user interfaces.\n  \n\n  \n    CPT &#8211; Channel Power Tools for Photoshop CC is available for sale through Adobe Exchange and the Bigano store.\n  \n\n",
      tags: ["Channel Power Tools","CPT","extension","Photoshop CC"],
      id: 75
    });
    

    index.add({
      title: "Adobe Extension Manager CC and Exchange issues",
      category: ["Extensions and Scripts","Photoshop"],
      content: "\n  \n    Soon after the public launch of Creative Cloud applications, some users reported to have run into troubles installing extensions from the Adobe Exchange panel because of Adobe Extension Manager CC errors. Here are the most common issues and the suggested workarounds, waiting for the official fix.\n  \n\n  \n    UPDATE (July 2015): if you&#8217;re in troubles with Adobe Extension Manager CC after having updated to CC2015 be aware that you&#8217;re out of luck. It&#8217;s been discontinued, that is: Adobe Extension Manager will not support CC2015. Please read this page for updated information and workaround.\n  \n\n  \n    Troubleshooting\n  \n\n  \n    First and above all, please check to have every CC app up-to-date &#8211; you may need to quit the CC app and restart to see the updates.\n  \n\n  \n    To the date of this writing, the Adobe Exchange panel&#8217;s version is 1.0.0. When you launch Exchange, it should automatically checks whether updates are available or not. To be sure, please look for the ExtensionBundleVersion=\"1.0.0\" string in these files:\n  \n\n  [Mac] /Library/Application Support/Adobe/CEPServiceManager4/extensions/AdobeExchange/CSXS/manifest.xml\n[Win] C:\\Program Files (x86)\\Common Files\\Adobe\\CEPServiceManager4\\extensions\\AdobeExchange\\CSXS\\manifest.xml\n\n  \n    The most frequent error I&#8217;ve been reported is Adobe Extension Manager CC saying something along these lines:\n  \n\n  \n    \n      Error occurred during copying file  &#8216;/private/var/folders/sg/39jyqx0s5fl6d41yxpvm5qm0000gn/T/TemporaryItems/FlashTmp7/com-cs-extensions-AdvancedLocalContrastEhancer.zxp to  &#8216;/Users/***/Library/ApplicationSupport/Adobe/Extension Manager CC/EM Store/Photoshop/com-cs-extensions-AdvancedLocalContrastEhancer_20130620003905.ZXP&#8217; The extension will not be installed.\n    \n  \n\n  \n    \n  \n\n  \n    Adobe engineers are working on that and suggest to: &#8220;Click OK and close Extension Manager. Disregard the error messages in the Panel and try the installation again&#8220;.\n  \n\n  \n    It turns out that what fires the error is some kind of variation of this pattern:\n  \n\n  \n    \n      You choose to install an extension from Exchange Panel (download is fairly fast, but possibly quite long wait while &#8220;Installing&#8221; with no progress information)\n    \n    \n      Exchange Panel alerts: Adobe Extension Manager is required to install this extension. Do you want to launch Extension Manager now?\n    \n    \n      Choosing Yes launches Adobe Extension Manager CC, and installation begins.\n    \n    \n      Adobe Extension Manager shows the &#8220;Accept License&#8221; stuff.\n    \n    \n      Adobe Extension Manager alerts: &#8220;You have to quit Photoshop CC before installing, click Retry to continue&#8220;\n    \n    \n      In Photoshop, the Exchange panel shows an error message: &#8220;Installation failed. Please close Extension Manager and try again.&#8220;\n    \n    \n      Quit Photoshop, back to Extension Manager CC you click Retry.\n    \n    \n      Adobe Extension Manager CC may ask for administrator password.\n    \n    \n      Adobe Extension Manager CC fires the error message: &#8220;Error occurred copying file [&#8230;] The extension will not be installed&#8220;. You click OK.\n    \n    \n      There&#8217;s no extensions listed as installed in Adobe Extension Manager; the Exchange panels doesn&#8217;t mark the extension as installed.\n    \n    \n      If you restart Photoshop, the extension is installed. (some report a crash after the first launch).\n    \n  \n\n  \n    \n  \n\n  \n    Other suggestions coming from Adobe\n  \n\n  \n    \n      If you install Extension Manager CC through CC then please close CC before launching the Exchange panel.\n    \n    \n      If you see the message in the screenshot at right in the Exchange panel during a product installation please click yes. CC will load and download Extension Manager CC. Once the download is complete, please close the CC application before clicking ’install’ again in the Exchange panel.\n    \n    \n      If you are prompted to launch Extension Manager as part of a product installation in the Panel then please click Cancel and try the installation again. If you are prompted to launch Extension Manager again then please click OK.\n    \n  \n\n  \n    If Adobe Extension Manager doesn&#8217;t list your application\n  \n\n  \n    It may happen that Photoshop or your app of choice (Illustrator, InDesign, etc) isn&#8217;t listed in the left column of Adobe Extension Manager (CC or CS6). If this happens, open the Adobe ExtendScript Tool Kit app, input BridgeTalk.__diagnostics__; and from the menu choose Debug &#8211; Run. (shortcut: ⌘R).\n  \n\n  \n    \n  \n\n  \n    In the Javascript Console (if you don&#8217;t see it, find it in the Window &#8211; Javascript Console menu) look for the Photoshop or your favorite app path, and check that it is correct &#8211; i.e. it correspond with the actual application&#8217;s path. If that&#8217;s not the case, chances are that you&#8217;ve to uninstall and reinstall it in order to let Extension Manager properly recognize it.\n  \n\n  \n    Adobe Extension Manager CC known bugs\n  \n\n  \n    These are going to be included in the forthcoming fix.\n  \n\n  \n    \n      AEM doesn&#8217;t show product icons &#8211; just the default jigsaw one.\n    \n    \n      Extensions updates don&#8217;t show up in AEM.\n    \n  \n\n  \n    If you&#8217;re in need to use the Update feature, please use the command line (run it to see usage help information):\n  \n\n  [Win] ExManCmd.exe (run in Command Prompt)\n[Mac] Adobe Extension Manager CC.app/Contents/MacOS/ExManCmd (run in Terminal)\n\n  \n    Keep calm and don&#8217;t panic\n  \n\n  \n    We all hate transitions, don&#8217;t we? Yet Apparently extensions are installed anyway and Adobe engineers are fully aware of the problem and currently working on a patch. Hey, it could be worse 😉\n  \n\n",
      tags: ["Adobe Exchange","Adobe Extension Manager CC","Creative Cloud","error","extensions","installation"],
      id: 76
    });
    

    index.add({
      title: "Sublime Text 2 and 3 ExtendScript Photoshop package (Updated)",
      category: ["Coding","ExtendScript / Javascript"],
      content: "\n  \n    This post contains a Sublime Text 2 custom build package that lets you run ExtendScript code targeting Adobe Photoshop. I&#8217;ve coded it applying few tweaks and customization to an existing package made by Sébastien Lavoie for After Effects scripting. \n  \n\n  \n    What is that for?\n  \n\n  \n    When you code Photoshop scripting, chances are that you do it with the ExtendScript ToolKit (aka ESTK). While this Adobe provided piece of software has some unique features that make it very efficient while debugging and looking for DOM documentation, as an editor it really sucks. I&#8217;m sorry about that, but compared to other modern editors it&#8217;s barely usable.\n  \n\n  \n    Sublime Text 2, (and the 3 beta) conversely, are an amazing editor with tons of features and plugins. Yet so far the only workflow for Photoshop scripters was to code ExtendScript (which is a superset of Javascript) in Sublime Text, then copy and paste it to ESTK and run the script from ESTK.\n  \n\n  \n    This custom build package lets you run ExtendScript code targeting Photoshop in Sublime Text 2 and 3. Type ⌘B (or ctrl+B) and your code is being executed in the host app!\n  \n\n  \n    UPDATE (September 2016)\n  \n\n  \n    I&#8217;ve tweaked the Package described in this article, originally written in 2013. Important news:\n  \n\n  \n    \n      The code is now hosted and maintained in a GitHub repository that you can find here.\n    \n    \n      It&#8217;s been updated to support Photoshop CC 2015.5.\n    \n    \n      Simplified the code a lot\n    \n  \n\n  \n    Please refer to the GitHub repository for updated instruction – I keep the rest of the article as a document of the past.\n  \n\n  \n    Installation\n  \n\n  \n    Download ExtendScript-PS (V2.0 &#8211; April 2014, defaults to Photoshop CC, customizable for CS6), the Sublime Text 2 build package for Photoshop ExtendScript. Unzip it and move the ExtendScript-PS folder in:\n  \n\n  \n    \n      [OSX] ~/Library/Application Support/Sublime Text 2/Packages\n    \n\n    \n      [Win] ~\\AppData\\Roaming\\Sublime Text 2\\Packages\n    \n  \n\n  \n    If you&#8217;re unsure where the Packages are located on your system, find them from the Sublime Text menu: Preferences &#8211; Browse Packages&#8230; Do an app restart, just in case!\n  \n\n  \n    How to use it\n  \n\n  \n    Write your ExtendScript as usual, and save the JSX file you&#8217;re working on (if you don&#8217;t, it won&#8217;t work).\n  \n\n  \n    Make sure the Tools &#8211; Build System &#8211; ExtendScript_PS is checked, then build the script with ⌘B (Mac), ctrl+B (PC) or the Tools &#8211; Build menu item.\n  \n\n  \n    As you build it the JSX script is copied to the Photoshop&#8217;s Presets/Scripts/ folder, a confirmation message appears on the Sublime Text 2 console, then it&#8217;s run in the host application.\n  \n\n  \n    Depending on your platform and/or PS version, a couple of tweaks may be needed in order to make the it run in your system: please open the ExtendScript-PS folder (in the Sublime Text 2 packages) and read along.\n  \n\n  \n    Mac Hacks\n  \n\n  \n    The build package, as it is, works on Photoshop CC, CS6 (and possibly CS5 too). In the script.scpt file make sure you are targeting the correct Photoshop version:\n  \n\n  on run arg\n\n  set theFile to arg's item 1\n\n  open for access theFile\n  set fileContents to (read theFile)\n  close access theFile\n\n  tell application \"Adobe Photoshop CC\"\n    do javascript fileContents (* show debugger before running / never / on runtime error *)\n    activate\n  end tell\n\nend run\n\n  \n    Moreover (see line 10) you can uncomment show debugger to activate ExtendScript Toolkit: available options are either before running, never, on runtime error.\n  \n\n  \n    Windows Hacks\n  \n\n  \n    The build package, as it is, works on Photoshop CC (64 bit). I did test it for CS6 on a virtualized Windows 7, so feedback from actual PC users is welcome. Check that the build.bat file contains information that matches your system in these lines:\n  \n\n  :: Change this accordingly to your CS/CC version\nset version=CC\n\n:: Adobe Photoshop folder location 64 bit versions:\nset ps_folder_path=c:\\Program Files\\Adobe\\Adobe Photoshop %version% (64 Bit)\n\n:: Adobe Photoshop folder location 32 bit versions:\n:: set ps_folder_path=c:\\Program Files (x86)\\Adobe\\Adobe Photoshop %version%\n\n  \n    Particularly, set the correct:\n  \n\n  \n    \n      Photoshop version (CS5, CS6, CC&#8230;) which has to match the application&#8217;s folder name.\n    \n    \n      Photoshop folder (in the comments there&#8217;s the default 32 bit path if you need it) &#8211; change it accordingly if PS has been installed in a custom disk/directory.\n    \n  \n\n  \n    Credits\n  \n\n  \n    I&#8217;ve cloned from GitHub the original Adobe After Effects package for Sublime Text 2 made by Sébastien Lavoie &#8211; which I have tweaked to fit Photoshop&#8217;s scripting needs (Version 1 in May 2013, Version 2 in April 2014).\n  \n\n  \n    I&#8217;m a poor hacker, so if the package doesn&#8217;t work properly it&#8217;s my fault only. Feedback is welcome as usual, happy coding!\n  \n\n",
      tags: ["build","Extendscript","package","Sublime Text 2"],
      id: 77
    });
    

    index.add({
      title: "DropBox, Git and SublimeText on the Mac",
      category: ["Coding"],
      content: "\n  \n    This post will describe how to setup a local Git repository, sync it with a private DropBox repository for cloud backup (as it were a GitHub hosted one) and use the Sublime Text Git plugin to manage it.\n  \n\n  \n    Get ready with Git\n  \n\n  \n    Download the latest Git version from the official website. To date, the latest package available is called git-1.8.2.1-intel-universal-snow-leopard.dmg (I&#8217;m running it successfully on OSX 10.8.3 Mountain Lion). Launch the installer, and when it&#8217;s done open the Terminal (Applications/Utilities/Terminal.app) and type which git\n  \n\n  \n    \n  \n\n  \n    This is what I got, meaning that the path for the git executable is /usr/local/bin/git (or whatever it is on your machine) which is something you&#8217;ve to remember.\n  \n\n  \n    If you want (I do), you can download as well a GUI Client &#8211; providing you with a nice graphic interface. There are plenty of them, I&#8217;ll stick with the free original GitHub app for the Mac.\n  \n\n  \n    Depending on your skills (I&#8217;m not a very seasoned Git user, to put it mildly) and preferences, you&#8217;ll be able to drive your version control system either via command-line and/or visually through the GitHub app.\n  \n\n  \n    Get ready with Sublime Text\n  \n\n  \n    Sublime Text is a really powerful text editor (TextMate is another one I&#8217;d suggest): among the plethora of available plugins, install the free Sublime Package Control first and you&#8217;ll never regret it (installation instruction here). It will help you look for, and install, all the (other) plugins you may ever need.\n  \n\n  \n    Then, in Sublime Text do the usual ⌘⇧P to open the Command Palette and start typing &#8220;install&#8221; &#8211; as you type, the actual command will appear, which is Package Control: Install Package. Then look for Git and Enter to install it &#8211; visit also at the plugin official page for more information.\n  \n\n  \n    So far so good &#8211; yet the Git plugin isn&#8217;t working yet because Sublime Text needs to know the Git&#8217;s path (the one you&#8217;ve found before). Open with a text editor of your choice (guess which one!) the file called Git.sublime-settings found here:\n  \n\n  \n    /Users/&lt;yourUser&gt;/Library/Application Support/Sublime Text 2/Packages/Git/\n  \n\n  \n    And look for the following lines:\n  \n\n    // if present, use this command instead of plain \"git\"\n  // e.g. \"/Users/kemayo/bin/git\" or \"C:\\bin\\git.exe\"\n  ,\"git_command\": \"/usr/local/git/bin/git\"\n\n  \n    Of course be careful to type the correct path for the git_command, (recall what was the output of the which git command) then save and close (this issue is covered here).\n  \n\n  \n    Set a DropBox &#8220;remote&#8221; repository\n  \n\n  \n    One thing you could do (I did it &#8211; but it&#8217;s not what I&#8217;ll be suggesting) is to create a Local Repository in the Dropbox folder on your HD: here both your files and the repository are backed-up on DropBox servers. Unfortunately, this way you&#8217;re forced to keep all your dev stuff within the local DropBox folder &#8211; i.e. all your files &#8211; and this is not what you want.\n  \n\n  \n    As a more efficient alternative, you&#8217;ll keep a local repository in your usual development folder, then push it to a DropBox repository as it were a GitHub hosted one.\n  \n\n  \n    Let&#8217;s assume that your DropBox folder is ~/Dropbox and the folder where you do your development is ~/dev/photoshop. You&#8217;re going to work on a job called psProject01. Open the Terminal app.\n  \n\n  \n    1. Create the DropBox repo\n  \n\n  $ cd /users/Davide/Dropbox\n$ mkdir psProject01.git\n$ cd psProject01.git\n$ git init --bare\n\n  \n    You&#8217;ve made and initialized a bare psProject01.git repository in the DropBox folder.\n  \n\n  \n    2. Create the Dev repo\n  \n\n  \n    This is on your usual development folder.\n  \n\n  $ cd /users/Davide/dev/photoshop\n$ mkdir psProject01\n$ cd psProject01\n$ git init\n\n  \n    As a result, a hidden .git folder is created inside psProject01. Now there are two empty repos, one on the DropBox folder, one on the dev folder.\n  \n\n  \n    3. Link the repositories\n  \n\n  \n    The following command will link the DropBox repository to the dev one (you&#8217;re still in /users/Davide/Dev/photoshop/psProject01)\n  \n\n  $ git remote add origin /users/Davide/Dropbox/psProject01.git\n\n  \n    Someone is using a different syntax, but apparently the former one works too:\n  \n\n  $ git remote add origin file:///users/Davide/Dropbox/psProject01.git\n\n  \n    4. Add files to the Dev repo, Commit and Push\n  \n\n  \n    For some reason, I haven&#8217;t been able to use the Sublime Text Git plugin to Git: Add Current File (i.e. create a new file in Sublime Text and add it to the repo, I get this error: &#8220;git branch: fatal: ambituous argument head unknow revision or path not in the working tree&#8221;) at least until a first file has been added via command line, the change committed and the repository pushed:\n  \n\n  $ touch lib.jsx\n$ git add lib.jsx\n$ git commit -m \"Initial commit, lib.jsx added\"\n$ git push origin master\n  Counting objects: 3, done.\n  Writing objects: 100% (3/3), 225 bytes, done.\n  Total 3 (delta 0), reused 0 (delta 0)\n  To /Users/Davide/Dropbox/repo.git\n  * [new branch] master -&gt; master\n\n  \n    The above creates a lib.jsx file and adds it to the dev repository. Then the change is committed, and the repo is pushed to the DropBox &#8220;remote&#8221; repository (which is on the HD, but it&#8217;s the same command you&#8217;d write for an actual remotely hosted repo).\n  \n\n  \n    Mind you, if you look into the /users/Davide/Dropbox/psProject01.git folder you won&#8217;t see any lib.jsx file: the Dropbox repository only contains the Git&#8217;s guts. If you want to double-check that everything went fine (and the file is actually stored in the Git database), you can clone the repo to a different location, for instance:\n  \n\n  $ cd /users/Davide/dev/\n$ git clone /users/Davide/Dropbox/psProject01.git testRepository\n  Cloning into 'testRepository'...\n  done.\n\n  \n    Now if you inspect the /users/Davide/dev/testRepository folder you&#8217;ll find the lib.jsx file there! Does it make sense? Articles I&#8217;ve used as a reference for these commands are found here and here.\n  \n\n  \n    Using Sublime Text Git plugin\n  \n\n  \n    Now start doing something with your newly created lib.jsx file, like:\n  \n\n  // Test file\n#target photoshop\n\nalert(\"Hello World\");\n\n  \n    and save.\n  \n\n  \n    Commit\n  \n\n  \n    You can commit with ⌘⇧P to open the Command Palette and then typing Git: Quick Commit:\n  \n\n  \n    \n  \n\n  \n    Then enter a short description:\n  \n\n  \n    \n  \n\n  \n    And the following is the correct output:\n  \n\n  \n    \n  \n\n  \n    Log and Diff\n  \n\n  \n    ⌘⇧P and then Git: Log Current File shows you the commit history:\n  \n\n  \n    \n  \n\n  \n    Choosing one item shows you the details:\n  \n\n  \n    \n  \n\n  \n    Push\n  \n\n  \n    ⌘⇧P and then Git: Push Current Branch:\n  \n\n  \n    \n  \n\n  \n    A list of supported commands is found in the Sublime Text Git plugin page.\n  \n\n  \n    Using the GitHub app\n  \n\n  \n    If you feel more comfortable with a graphic user interface, start the GitHub application you&#8217;ve downloaded before and add a Local Repository (pointing to the dev folder) with the little plus button in the bottom bar:\n  \n\n  \n    \n  \n\n  \n    Double click on the psProject repository to see the History list:\n  \n\n  \n    \n  \n\n  \n    And eventually double click on a single item to view the commit&#8217;s details:\n  \n\n  \n    \n  \n\n  \n    From the app you can Publish/Sync the branch, add new files, etc &#8211; the very same things Git command line allows you. I&#8217;m still finding my way around all the options, so as a very basic user I won&#8217;t suggest you anything but the initial setup.\n  \n\n  \n    Caveat\n  \n\n  \n    True, DropBox sync may let you run into troubles if you and someone else are pushing stuff at the very same time, but as a single developer this won&#8217;t be a problem. Be aware, just in case you use DropBox to share your Git repository to others.\n\n    \n  \n\n",
      tags: ["DropBox","Git","repository","Sublime Text"],
      id: 78
    });
    

    index.add({
      title: "Theme Switcher &#8211; free extension for Photoshop",
      category: ["Extensions and Scripts","Theme Switcher"],
      content: "\n  \n    It&#8217;s now available through Adobe Exchange my latest, and most minimal so far, free extension for Photoshop called Theme Switcher. It simply lets you pick up the GUI Theme (that is: the Photoshop interface color, which since CS6 goes from Original to Light Gray, Medium Gray and Dark Gray) with one click &#8211; handy little panel that eliminates the need to dig into the Preferences.\n  \n\n  \n    Please find the Exchange page here and the product page on my commercial website CS-Extensions. If you happen to like it, please leave a review or some stars along your way!\n\n    \n\n     \n  \n\n",
      tags: ["extension","free","GUI","Photoshop @en","theme"],
      id: 79
    });
    

    index.add({
      title: "Dan Margulis&#8217; Modern Photoshop Color Workflow, beta-readers interview",
      category: ["Photoshop"],
      content: "\n  \n    Color Correction Maestro Dan Margulis has just published a new book about his latest research, titled &#8220;Modern Photoshop Color Workflow &#8211; The Quartertone Quandary, the PPW, and Other Ideas for Speedy Image Enhancement&#8221;.\n\n    \n\n    \n    &lt;/p&gt;\n\n    \n      Waiting for it to be shipped in my country too (it&#8217;s for sale on Amazon.com priced for a limited time 64.95 USD\n\n      \n      , and with a well deserved\n      average customer review rating of 5 stars) I&#8217;ve asked to three beta-readers, who happens to be among my best friends and experienced colleagues, few questions about the book&#8217;s topics. Read along, I&#8217;ve a special guest too!\n    \n\n    \n      The Book\n    \n\n    \n      As it is in his style &#8211; and this one is no exception &#8211; Dan&#8217;s books have two apparently conflicting characteristics:\n    \n\n    \n      \n        They&#8217;re somehow timeless: Dan teaches concepts first, then he elaborates the techniques exploiting them.\n      \n      \n        They&#8217;re deeply rooted in the years they&#8217;re published in: for instance, if early books were mostly CMYK oriented, a mixed approach has been adopted lately, when the consumer digital devices market changed the business.\n      \n    \n\n    \n      So to speak, the times, they are a-changing… and Modern Photoshop Color Workflow shows their evolution &#8211; targeting professionals&#8217; need of impressive results in a fraction of the time (which equals a fraction of the money) that was the norm in the past. The fact that, in order to learn speed, you&#8217;ve got to go through 450 pages of dense writing should suggest that the theory is quite structured. Nevertheless, the effort is well rewarded.\n    \n\n    \n      \n    \n\n    \n      The book website is now online, with tons of free learning resources such as videos and articles. Dan&#8217;s driven the development of a dedicated Photoshop PPW panel (Picture Postcard Workflow) too, which is available for free, that multiply your control over the workflow and makes it even more powerful.&lt;/span&gt;\n    \n\n    \n      The Beta-Readers\n    \n\n    \n      Dan&#8217;s been using beta-readers &#8211; i.e. a small group, about ten people strong, which is asked to read and comment the book as the author writes (and possibly re-write) it. Among the people who volunteered from all over the world, four italians were deeply involved in the project:\n    \n\n    \n      \n        Giuliana Abbiati [codename: GA, PPW panel software architect]. Creative mind, Photoshop extensions coder.\n      \n      \n        Alessandro Bernardi [codename: AB, beta-reader] Professional retoucher/colorist, founder of the PS-School project, which manages Dan Margulis classes in Italy.\n      \n      \n        Daniele Di Stanio [codename: DDS, coordinator]. Professional teacher and founder of Albero del Colore, a unique web-based learning experience of comparative color correction challenges.\n      \n      \n        Marco Olivotto [codename: MO, beta-reader]. Professional teacher and founder of the Color Correction Campus project, ACT-like classes in Italy.\n      \n    \n\n    \n      Interviews\n    \n\n    \n      Due to work commitments she&#8217;s been engaged into, Giuliana couldn&#8217;t join the interview &#8211; but I plan to ask her questions on a dedicated blogpost when she&#8217;ll be back.\n    \n\n    \n      PPW Evolution\n    \n\n    \n      DB: I&#8217;ve bought my first Margulis&#8217; &#8220;Professional Photoshop&#8221; book in the year 2000 and I&#8217;ve followed him ever since, so I&#8217;ve a pretty decent grasp on how much, and in which direction, Dan&#8217;s techniques have evolved over time. To my eyes, it looks like this latest workflow marks both a discontinuity from the past and makes a good point being &#8220;fit&#8221; &#8211; if I may use this darwinian terminology &#8211; in our times: that&#8217;s probably because the image business, and as a consequence our jobs, have changed dramatically in few years. Could you tell us how Dan has shaped, refined, and systematized it since you&#8217;ve heard about it first?\n    \n\n    \n      AB: Well, you’d surprised, but I often use a gastronomic example to explain the PPW because it’s like cooking a good recipe. Let’s say that you want to prepare a simple dish like “pasta alla carbonara”. Usually you start with very basic steps. The first ones are very important and will decide the success of your final dish. I mean that if you put the olive oil and then you burn too much the bacon, then your pasta will be too much “alla carbonara” and won’t be good. But also the middle steps are very important and so on.\n    \n\n    \n      We have learned in Dan’s classes, many “cooking” techniques, but when you have to correct an image, that is the moment in which you have to take some decisions. You must decide how much contrast, how much color or sharpening you want to add to an image, and the way you cook and mix all these ingredients can lead to a success or a failure.\n    \n\n    \n      Here comes the first key word: the workflow. My first impact with the Picture Postcard Workflow was in November 2007, in Dan’s Advanced class in Toronto. I had already attended his regular class in March, and Dan had presented the PPW in his first shape at Photoshop World one month later, in April. So, we were the first group of students that had the opportunity to test the workflow during the class. I was prepared to learn many new techniques, but when Dan showed the PPW in its first version, I was surprised to see how quickly he took all the critical decisions about luminosity and color that produced a good image in few minutes.\n    \n\n    \n      The secret behind was the workflow: he had summarized all his knowledge and techniques in an effective workflow to allow good results in a matter of minutes. We began to experiment with the first PPW and the final results were pretty good, compared to the traditional workflows.\n    \n\n    \n      The idea of splitting all the steps in three main ones (eliminating the color problems, optimizing luminosity and contrast, boosting colors) is simple, and simple ideas work in almost all cases. Once you have solved color issues, think of how liberating is to work on luminosity and contrast. You can concentrate only on that and decide better how to optimize them. And then the final touch, the color boost. And it was only the beginning.\n    \n\n    \n      On my personal side I had the opportunity to see how the PPW evolved during the years as I have arranged all Dan’s classes here in Italy from 2009. So I’ve seen how it was introduced in his regular classes and how was taught in the advanced ones as well. In 2009 the workflow was already improved and some steps were revised to improve both speed and quality. After only one year, in 2010, we recorded what is currently the most complete video resource about the PPW. While we were taping I saw that the workflow was improved again, introducing some actions to speed up steps like the Color Boost and the Sharpening. After that came one of the most important additions: the Man from Mars technique became “Modern” as it was done through a simple action that required a rough selection to guide Photoshop for introducing color variation.\n    \n\n    \n      Then the Skin Desaturation action was added to pre-desaturate images featuring faces and Dan’s attention moved on other aspects like the False Profile and, later, the Hemholtz Kohlrausch effect.\n    \n\n    \n      This leads to the second key word: evolution. Thanks to Dan’s luminous mind and experimentation, and to the students’ feedbacks, the workflow was evolving. At that point was clear that having all these cooking, er, enhancing techniques inside the workflow required a better kind of automation. Dan had already started to work on a panel to put together all the actions. But I’m proud to say that it was during his first Italian advanced class, that what we call now the “PPW Team” came into the light.\n    \n\n    \n      Giuliana Abbiati was involved in programming a PPW panel and we worked hardly on the layout to build a flexible and comfortable tool that could speed up the entire workflow and make it more reliable. We decided to put all the documentation written by Dan inside the panel and put also some additional articles written by me and one of the best Italian color correction teachers, aka Marco Olivotto. And even the panel had evolved, thanks to Giuliana’s constant efforts. She introduced scripting through which the panel can intelligently recognize the color profile of your image when you apply the false Profile technique, not to talk about the options that expand the possibilities of the Bigger Hammer, Modern Man from Mars, Color Boost and Sharpening actions. Now I would say that the PPW would not be the same without this panel, but the panel won’t be as it is without Dan’s improvements to the workflow.\n    \n\n    \n      And what about the workflow itself? I would say that in six years the key points are still the same (three main steps) but we have more options for specific cases and the speed has been improved around 40%, not to talk about the flexibility. This fits much better the current market’s needing, that is speed, quality and reliable results. Dan’s current approach has changed accordingly, moving from spending a lot of time on a single correction, to making different versions of the same image in the same amount of time, and blend their best part together.\n    \n\n    \n      This was not possible in 2007 when PPW was born, as all the techniques had to be made by hand. But now, with the power of automation and a fully tested workflow, it’s feasible and can give impressive, though unpredicted, results.\n    \n\n    \n      Teaching the Workflow\n    \n\n    \n      DB: Color correction has always implied a broad knowledge of principles and techniques, which experience reinforces. Yet, if you see Dan working with the PPW panel, it looks like a fast and apparently effortless process. Since you&#8217;ve been teaching the PPW in your own advanced classes, what are the pitfalls from the educator&#8217;s point of view? How have you managed to pass the workflow&#8217;s concepts to your students effectively?\n    \n\n    \n      MO: Short reply: I hope my efforts were effective, but I must say it&#8217;s not as easy as it may seem.\n    \n\n    \n      More in detail, I&#8217;ve had three different approaches: a rather formal one, in a video-course, where techniques must be made very clear to a potentially dishomogeneous audience; a rather &#8220;idealistic&#8221; one, in the class, where I may easily see if people are following or not, but usally attempt to deliver the philosophy of the workflow rather than every detail of the technique; finally, an informal approach, in workshops, where I have two to four hours to explain what the whole thing is about and show some examples.\n    \n\n    \n      Let me start from the bottom, i.e. workshops. The first time I attempted to explain the PPW in such a situation I failed flat. It was my fault &#8211; I had 90 minutes and that wasn&#8217;t even enough to scratch the surface. Moreover, it was at an exhibition, and these are known for utter chaos and a necessity to be quick and entertaining. I later discovered that I could throw together some decent ideas in a couple of hours, and finally settled on the fact that four hours are enough to cover the workflow in detail. What level of detail strictly depends on who is listening, though. In classes it&#8217;s easier: the people who attend already know the basics of color correction and can therefore can grasp ideas quickly. To me, the core of the PPW is perception: I try to always keep that light on when I explain things, because less experienced practitioners may think that the Helmholtz-Kohlrausch effect is a commodity rather than something intrinsic to our vision. All in all, the feedback has been good, so far. Finally, video-course: nobody ever complained, and the approach was not to do a set of images from start to finish, individually, but go through four images together, side by side and step by step. I thought that this approach might easily show when something is needed and when something is not needed at all.\n    \n\n    \n      It must also be said that the PPW is highly volatile and somewhat dangerous, in general. It is a short workflow, but it takes time to see through it. I recently launched a small contest on facebook, and about 30 different versions of the same picture arrived. I think at least 15 of them, maybe more, either went through the PPW or something which seriously attempted to mime it. My own version, done in about five minutes, was better than most but definitely worse than at least two made by students of mine. The basic difference was that they cobbled together a way to get better luminosity than mine, and there was no discussion in the end: their approach was better. In this sense, PPW is very democratic: if someone finds a way to correctly enhance an important aspect of the picture and others won&#8217;t, his chances of winning in a comparison go skyrocketing.\n    \n\n    \n      The PPW from the user&#8217;s POV\n    \n\n    \n      DB: The book thoroughly describes each step of a workflow that can branches into several sub-strategies, and Dan&#8217;s worked hard to systematize the PPW up to a great extent. What is, in your opinion, the degree of freedom that a retoucher using the PPW has? Color correction challenges you&#8217;re leading on your website expose you to a large amount of versions of the same image made by different operators; from this peculiar position, what are as a teacher your suggestions to really master the PPW?\n    \n\n    \n      DDS: Ciao Davide, and many greetings to your reader, it is a pleasure for me to be a guest in not-another-news-blog, but a website that goes in-depth and try to analyze what is around us. My experience as a color correction teacher is telling me that we have a big issue here.\n    \n\n    \n      Let me explain, first, that the word &#8220;freedom&#8221; that is a good word in every professional field is not something you can easily use in same sentence with &#8220;workflow&#8220;. A workflow, is a series of steps that we code to avoid over-thinking, to cut out mistakes or just to save time. So how can we have a workflow that grants us freedom? The answer is surprisingly easy, and as a teacher I&#8217;m just delighted Dan chose this way. The PPW entrusts the user to make the right calls, at the right time, on the right images. Of course we can now see, also, the major drawback. Uneducated users will produce poor results. Also we should split our professional field in two areas, users who already know Margulis methods, and all others. Let&#8217;s not be worried with the first, let&#8217;s focus on the latter category.\n    \n\n    \n      Our world is ruled by usually wrong ideas about Photoshop, and how to use it. But we need to be fair and understand that 100% users today starts learning it on web. Youtube videos, text and images tutorials. But teaching is a complicated matter, so large efforts are wasted on explain tools buttons, and showing where these tools are. But (at first) users grow their Photoshop culture like this, someone tells them where tools are, how to use it and what they do. But no one is telling me when they should use these or on what images (or why). And let&#8217;s not be mistaken, even this is already a huge effort for users, many of which are (for example) photographers that should be outside shooting.\n    \n\n    \n      \n    \n\n    \n      Now imagine what happens when these users meet the PPW. At first they are lost. They are overwhelmed by possibilities, by the numbers of tools. Also, we&#8217;re talking about new users who are not so strong in image evaluation, so they will probably fail in choosing the correct approach many times. But this, this right here, is where the PPW really shines. And this is the reason why I think that Mr. Margulis invented something that no one ever did in the technique history.\n    \n\n    \n      This is not (in my humble opinion) a workflow. This is a workflow&#8217;s framework. Dan built a subset of tools and instruments, and arranged them in a &#8220;progressive&#8221; way. All of this, more, is built on a software like Photoshop, that allows professional to work with degrees of freedom you can&#8217;t find anywhere else. Extremely beautiful, and very dangerous. I spent several months researching about how to teach the PPW. Not an easy matter, but I found just the best recipe. And it was so simple, I&#8217;m now feeling really stupid. Me and you, Davide, had the chance in seeing Dan start teaching PPW, it was 2009, and it was a stripped down version, it was the foundation of what it is today. Then we saw Dan adding tools, and refining procedures, and let me say we also saw Dan adapting everything to the latests technologies. I will start recording my videos that teaches PPW in the coming weeks, and my approach, the only sensible to me as of now (please note I&#8217;m talking about teaching to students that never heard about PPW) is to split my lessons in three categories, where in the first you get the core training, few essentials procedures. And built on top of that in the following two sessions, adding tools, degrees of freedom, choices and mistakes. And you can see my answer is complex like PPW, a workflow, well to me a workflow&#8217;s framework, that will allow each and every professional to adapt it to their own work, not the opposite we see every day.\n    \n\n    \n      New Frontiers\n    \n\n    \n      Dan&#8217;s been putting great effort constantly fine-tuning the workflow, and the Modern Color Workflow book will probably become a new standard in the field. Where do you think color correction research should point to, now? What&#8217;s the topic that interests you the most, given the PPW as the starting point?\n    \n\n    \n      AB: This kind of question is always difficult to answer as we don’t know how the market will evolve in the next years, and how fast. From my point of view I see two main areas in which I’ll try to invest my time and experimentation on color correction.\n    \n\n    \n      The first comes from my current specific field (retouch for advertising and fashion) and is creative color. What I think is one of the biggest Dan’s teachings is the way of controling images through the numbers, beyond the thought that is behind his techniques.\n    \n\n    \n      The advantage of having a full workflow like the PPW with so many automated steps, gives me the chance to experiment with only one or few steps to get creative results. For example, using the Bigger Hammer without doing it manually, can lead to some unconventional results on portraits. And with so many options available in the panel, I can concentrate on the final result and do my experiments very quickly.\n    \n\n    \n      Or, instead, think about the MMM action. We normally uses it to introduce color variation and make images more pleasant. But if you know how to use it, you can try the MMM on some “finished” images intentionally to introduce color shifts. This kind of results can be suitable for the creative market in which conventional colors are sacrificed in favor of the client’s personal taste to have particular color looks. Is what I call “Creative MMM” and thanks to Dan, we have now a dedicated video on the book’s website.\n    \n\n    \n      \n    \n\n    \n      The second area is one in which I’m currently investing the biggest amount of time: color correction for video. Now that Photoshop allows to manage decently video files, I can use some of Dan’s techniques for correcting colors in videos. It’s not so easy to realize and you must find some workarounds to make them suitable for videos, but it can produce stunning results. Enhancing a video through the use of channels, or some adjustment layers in combination with blending modes and smart filters is amazing and fascinating at the same time. And the nice thing is that you can integrate the possibilities offered by the PPW in Photoshop with the typical editing softwares like Adobe Premiere and After Effects.\n    \n\n    \n      That’s why I think that the key word in the next years will be “integration”. Who knows, maybe in the near future we’ll be able to have a version of the PPW suitable for videos with a panel like the current one that could be switched to video mode.\n    \n\n    \n      But without Dan’s techniques this could have never been possible to me. And, definitely, this is the most important key point.\n    \n\n    \n\n    \n      DDS: You&#8217;re not going to like my answer. I think our major shift in these last years, is that we&#8217;re struggling to understand exactly how our images are built or composed.\n    \n\n    \n      The hardware&#8217;s processes are a bit out of hands, and RAW processors are forcing us to steep learning curve. We can say that our world is getting quite complicate out of Photoshop, something I think we were not needing. I sometimes ask myself if there is more to research on color correction, not an answer I found, yet. Studying with Dan shifted many things, I started researching on perception, for example. But I also found out that many times what we need is not really something entirely new, but to be able to define, or to find a name, for something already exists.\n    \n\n    \n      As a collective I think the best way to move forward is to take several steps backwards. To drop for good the &#8220;tutorial&#8221; way, if it is not carefully inserted in a honest context. To stop believing everyone can be a teacher.\n    \n\n    \n      We&#8217;re blessed to live an era where Masters are still here, and still working for us, with us. I foresee the real direction for PPW, though, will not be pursued as a collective, but as singles. We have all the tools, and we have the chance to make those ours. As teachers the real challenge we&#8217;re up to is to successfully teach PPW. Something that happens, to me, not when the user understood all steps and is able to repeat those, but when feels confident in changing, experimenting and move the PPW forward to new directions. Thank you Davide. And many thanks to Dan Margulis that is, and will always be, my Maestro.\n    \n\n    \n\n    \n      MO: I think I understand about the future of color correction as much as I understand about women; that is, next to nothing.\n    \n\n    \n      Seriously? The push is towards a fast workflow: with few notable exceptions, the time when one could spend 30 or 40 minutes per image are gone. Entire fields of photography rely on software like Lightroom, which is fine, but it has no layers, which makes it a completely different universe from the one I am used to live in. Add to it that some visible personalities are seemingly preaching that the only button missing in ACR is a large red thing labeled &#8220;PRINT!!!&#8221;, and you have the raw picture (sorry for the pun). I wonder if they ever realized how ridiculously bad the ACR preview can be (question: can one really sharpen in ACR? evaluate banding? subtle color changes?), but that&#8217;s it.\n    \n\n    \n      \n    \n\n    \n      A few things I&#8217;d love to see in Photoshop could possibly jeopardize some of the steps in the PPW, but I guess the PPW is more a philosophy than a settled bunch of advanced techniques: the proof of this is that some (possible) steps were almost completely erased, like Multiply in Lab (which I liked a lot, by the way). Overlay in Lab is off, Man from Mars has evolved in a totally different direction than the original&#8230; so, who can say where it may go?\n    \n\n    \n      I can tell you what I would love to see in Photoshop CS-whatever, with whatever &gt; 6, but I think this is going to remain a dream:\n    \n\n    \n      \n        Better blurring algorithms, able to catch the borders of an object without being as clumsy as Surface Blur.\n      \n      \n        A revamped approach towards Unsharp Mask.\n      \n      \n        The ability to actually edit whatever color profile, 3D LUT&#8217;s included.\n      \n      \n        Better tools for local contrast: there is a huge literature out there, and seemingly someone is inventing something new everyday, but these tools never seem to get in the right place (but they may appear in ACR &#8211; guess why?)\n      \n      \n        Better demosaicing algorithms in ACR.\n      \n      \n        Some serious noise reduction.\n      \n    \n\n    \n      As far as I am concerned, 3D and the Timeline are more than enough as they are, thank you. When I think of the possibility that in the next release someone may decide to add the opportunity to uhm, hey!, edit CSS natively in Photoshop, my gut reaction is to play &#8220;Radio Clash&#8221; at outrageously loud volume, and cry like a baby at the immortal verse: &#8220;Please save US, not the whales!&#8221;\n    \n\n    \n      A Very Special Guest\n    \n\n    \n      I&#8217;ve reached Dan Margulis\n\n      \n       few days before the book pressrun, and he&#8217;s been so kind (which is not a surprise at all) to answer my question too. Codename:\n      DM.\n    \n\n    \n      DB: Over the years the techniques you&#8217;ve been advocating have evolved, because the (shared) knowledge in the color correction field has progressed. What looks bizarre to my eyes is that the PPW, as the latest application of a constantly advancing theory, makes use of tools (False profiles, Lab, masks, blending modes, etc.) that were available since Photoshop CS, i.e. 10 years ago. Of course we now use them in a modern way. Technologically speaking, the only real new, distinctive PPW tool (that fully exploit its power) is the panel &#8211; that is a tool that somehow you&#8217;ve built yourself &#8211; with Giuliana&#8217;s help. That&#8217;s paradoxical in my opinion. Image processing technology has evolved too, and at a high pace &#8211; but somehow these two lines didn&#8217;t cross too much (or not at all? May one provocatively state that Color Correction has progressed in spite of, and not thanks to, technology?). How is it, in your opinion &#8211; if you agree to some extent with the above? What should we look at now, as a generation of people willing to contribute to the evolution of the applied color theory?\n    \n\n    \n      DM: One can provocatively state that Berlusconi should be the next pope, or whatever else one likes. However, both propositions are equally invalid.\n    \n\n    \n      It&#8217;s true that Photoshop has stagnated. The development of hardware has not, and that has made all the difference. I don&#8217;t have the price lists for 10 and 15 years ago, but for the book I looked at 18 years ago. At that time, RAM was 5.000 times as expensive as today. This was a bargain compared to hard disks, which were 10,000 times as expensive. I can&#8217;t figure out the difference in computing speed, but it&#8217;s at least 500x and maybe 1000x.\n    \n\n    \n      Something like the PPW sharpening action couldn&#8217;t possibly have run under these circumstances. First, we would not have the scratch space it requires, and second it would take an hour or so to sharpen a single image.\n    \n\n    \n      Similarly, the signature move of the PPW, the MMM action, makes use of a command that nobody has heard of, Equalize. People haven&#8217;t heard of even though it has existed for more than 20 years, because in the early 1990s experts like myself dismissed it as useless. And we were right&#8211;it was useless at that time, because to get any benefit requires a complicated procedure that would have brought computers of that time to their knees.\n    \n\n    \n      But probably the major impact of technology is communications. Without google, and without the means of transmitting large pictures cheaply, the development of knowledge is deterred because people interested in the subject can&#8217;t find and share information conveniently. My last three books were immeasurably better than anything previously because they were supported by international groups of beta readers. It wasn&#8217;t possible ten years ago, and my earlier books were published with many misconceptions for want of adequate feedback.\n    \n\n    \n      You yourself represent the new order. Your ideas are out there for people to look at, discuss, and improve. It&#8217;s easier than ever for users to reject vendor hype and find their own best solutions. So I expect that the field will continue to evolve rapidly, and I hope you will be one of the big reasons!\n    \n\n    \n      [Thank so much to all the friends that made this possible! Dan&#8217;s picture is © J.Scarponi]\n    &lt;/div&gt;\n    \n\n",
      tags: ["beta-reader","book","Dan Margulis","interview","Modern Photoshop Color Workflow"],
      id: 80
    });
    

    index.add({
      title: "Parametric Curves free script User Guide",
      category: ["Coding","Extensions and Scripts","Parametric Curves","Photoshop"],
      content: "\n  \n    Parametric Curves is a free Photoshop script that lets you plot mathematically defined (Javascript) Curves Adjustment layers. If you wonder what they could be for, please read my previous post called &#8220;Gradients and Parametric Curves in Photoshop&#8221;.&lt;/p&gt;\n\n    \n      Here I&#8217;ll show you the script interface and I&#8217;ll walk you through the creation of some interesting Curves, that will be the building blocks of advanced creative manipulations.\n    \n\n    \n      \n    \n\n    \n      Where do I get it?\n    \n\n    \n      Parametric Curves is available for Photoshop CS6 (Mac/Win) through Adobe Exchange &#8211; the new in-app, app-store panel made by Adobe itself. Download and install Exchange if you don&#8217;t already have it, then browse the Free extensions and look for Parametric Curves there.\n    \n\n    \n      Install Parametric Curves, and find it in the Photoshop Filter menu.\n    \n\n    \n      How does it work?\n    \n\n    \n      When you launch it, a Curves Adjustment layer is build behind the scene and the Parametric Curves window pops up: you can either pick up a ready made function from the Presets or write your own Javascript in the text area. Click Try to preview the curve, Apply to eventually confirm.\n    \n\n    \n      Interface\n    \n\n    \n      \n    \n\n    \n      1 &#8211; Curve\n    \n\n    \n      This is where the curve is graphed. By default the orientation is Light &#8211; that is to say Black (0) is bottom-left, White (255) is top-right, which is the default of RGB and Lab modes; you can switch to Pigment / Ink % (the default for Grayscale and CMYK, White is bottom-left and Black is top-right) with the Display option drop-down menu.\n    \n\n    \n      2 &#8211; Presets\n    \n\n    \n      There are some ready-made Curves you can try in the drop-down menu &#8220;Select a preset&#8230;&#8221;. When you choose one, the function is pasted in the text area (3) and the curve displayed (1).\n    \n\n    \n      You&#8217;re allowed to save your own Preset clicking the New button (you&#8217;ll be asked to enter a label for it). Add as many presets as you want &#8211; you can always delete the one you don&#8217;t like (but the Default ones) with the Remove button, or Reset them to the default group.\n    \n\n    \n      3 &#8211; Functions\n    \n\n    \n      In this text area you can write your own functions, or modify existing ones. Once you&#8217;re done, click the Try button in order to test it either as a graph in the Curve panel and as an actual Curves Adjustment Layer in the file.\n    \n\n    \n      4If you&#8217;re satisfied with the result, click the Apply button, otherwise you can tweak the function of Cancel back to Photoshop.\n    \n\n    \n      4 &#8211; Information\n    \n\n    \n      The Math Ref. button (which can be toggled on/off) shows a handy reference for the Math. Javascript Object (see below for more information):\n    \n\n    \n      \n    \n\n    \n      The Tutorial button points your browser to the Parametric Curves category page of this website, while the ? button shows copyright informations.\n    \n\n    \n      Functions Tutorial\n    \n\n    \n      This will help you starting with your own functions. Few basic information:\n    \n\n    \n      \n        The full range is 0-255, where Black = 0 and White = 255 (this is independent on the Display option).\n      \n      \n        Functions are in the form: y = f(x) and you&#8217;re supposed to write the f(x) part only: so the default Curve (a straight line 45°) is just &#8220;x&#8221;.\n      \n      \n        Functions value is automatically rounded.\n      \n      \n        Javascript syntax is allowed.\n      \n      \n        More complete Javascript reference is here.\n      \n    \n\n    \n      I&#8217;m not very good with geometry so let&#8217;s start simple:\n    \n\n    \n      \n\n      \n        x+127x-1272*xx/2\n      \n    \n\n    \n      If you need constants, they must be written as in this example: Math.PI * x + Math.LN2.\n    \n\n    \n      You can write functions as well of course:\n    \n\n    \n      \n\n      \n        Math.pow(x,2) / 7040*Math.log(x)255*Math.random()x+30*Math.sin(x/5)\n      \n    \n\n    \n      You&#8217;d better off normalizing the output to fit the 0-255 range, as in the following examples:\n    \n\n    \n      \n\n      \n        Math.sin(x)127*Math.sin(x)127*Math.sin(x) + 127127*Math.sin(2*Math.PI*x/255) + 127\n      \n    \n\n    \n      You can also use:\n    \n\n    \n      \n        The so-called ternary operator, in the form of (condition) ? (true) : (false).\n      \n      \n        The modulo operator %, for instance x%10 (which gives the rest of the division by 10 and it&#8217;s useful to repeat patterns)\n      \n      \n        Logic operators &amp;&amp; (and) || (or), besides == (equal to) and != (not equal to), &lt; and &gt;\n      \n      \n        Nest them when needed, but be careful not to write bad code!\n      \n    \n\n    \n      \n\n      \n        (x&gt;128) ? x : 255 &#8211; x4 * x%256(((x&gt;0) &amp;&amp; (x&lt;64)) || ((x&gt;128) &amp;&amp; (x&lt;191))) ? x : 255-xx / a\n      \n    \n\n    \n      Anything a bit more inspirational on the Creative side?\n    \n\n    \n      It&#8217;s a bottomless pit as soon as you start experimenting &#8211; please see this post full of examples.\n    \n\n    \n      \n    \n\n    \n      Go get it!\n    \n\n    \n      Find it on Adobe Exchange, it&#8217;s free! And if you think that after all it&#8217;s a nice piece of software, please leave a review in Exchange.&lt;/div&gt;\n\n      \n        \n\n        \n\n        \n      &lt;/div&gt;\n      \n\n",
      tags: ["Adobe Exchange","Gradient","Parametric Curves","script"],
      id: 81
    });
    

    index.add({
      title: "Gradients and Parametric Curves in Photoshop",
      category: ["Extensions and Scripts","Parametric Curves","Photoshop"],
      content: "\n  \n    \n\n    \n    I&#8217;m not a FX kind of guy, but\n    as a developer I&#8217;m fascinated by the astonishing complex results you can get with simple Gradients and a Parametric Curves in Photoshop. These designs can be used as building blocks for all kind of creative needs &#8211; whether bump maps, depth maps or displacement maps, abstract design, patterns, etc.\n  \n\n  \n    \n  \n\n  \n    Parametric Curves\n  \n\n  \n    I&#8217;ve started playing with the idea of drawing mathematically defined Curves via scripting. That is, not you adding/dragging points on a Curves window; but let ExtendScript code fetch user inputted formulas and draw the Curves accordingly. Let&#8217;s say it could be a prototype test for a future Photoshop panel.\n  \n\n  \n    So I&#8217;ve coded a simple script and, after a good dose of trial and error, I came up with some pretty interesting shapes:\n  \n\n  \n    \n\n    \n      Curves (as Adjustment Layers) I&#8217;ve drawn via scripting.\n    \n  \n\n  \n    Some of them may be replicable by hand (with a good dose of patience), others can be built via scripting only. These kind of curves are normally available in Filter Forge, a Photoshop plugin for texture creation I&#8217;ve enthusiastically written about here &#8211; but are out of reach if you don&#8217;t own it.\n  \n\n  \n    Gradients\n  \n\n  \n    The above Curves have no use on real imaging but forensic and/or scientific ones (maybe &#8211; if there&#8217;s a forensic expert out there I&#8217;m all ears! The comment section is open for you), but if you put them above a Gradient adjustment layer, the fun begins. I&#8217;ll be using 16bit files (in order to avoid posterization, some of these transformations are pretty harsh), with all of the available Gradient shapes:\n  \n\n  \n    \n\n    \n      Available gradients in Photoshop: Linear, Reflected, Radial, Diamond, Angle\n    \n  \n\n  \n    Gradient + Curves\n  \n\n  \n    Let&#8217;s start experimenting with a mix of Curves and Gradients. Be aware that you can mix multiple Curves and Gradients, and play with Blending modes too!\n  \n\n  \n    Linear\n  \n\n  \n    \n  \n\n  \n    Radial\n  \n\n  \n    \n  \n\n  \n    Angle\n  \n\n  \n    \n  \n\n  \n    Reflected\n  \n\n  \n    \n  \n\n  \n    Diamond\n  \n\n  \n    \n  \n\n  \n    Combined gradients\n  \n\n  \n    If you start adding multiple Gradients and Curves on top of each other, you may end up with some really interesting results:\n  \n\n  \n    \n  \n\n  \n    It gets addictive when you start must I confess&#8230;\n  \n\n  \n    \n  \n\n  \n    What for?\n  \n\n  \n    I&#8217;m not the creative one, I&#8217;m just owning the hardware store providing you with the tools 🙂\n  \n\n  \n    That said, as the only untalented designer available on duty at the moment, I can show you just few examples to tease your creative skills. These are just building blocks, combining them the possibilities are endless.\n  \n\n  \n    Patterns\n  \n\n  \n    This is a very simple one, textured and ready to be tiled.\n  \n\n  \n    \n\n    \n      A: Gradient + Curves. B: Perlin noise (Clouds) + Motion Blur. C: Perlin noise + Gaussian Noise. D: Combined result.\n    \n  \n\n  \n    Displacement maps\n  \n\n  \n    Pretty hidden in the menu, there&#8217;s the Distort &#8211; Displace filter. It requires you to open a second image (the Displacement Map), to be used to distort the first one according to the parameters you&#8217;ve set.\n  \n\n  \n    \n\n    \n      A: Gradient + Curves. B: Texture. C: Texture displaced. D: Combined result.\n    \n  \n\n  \n    3D Depth Maps and Bump Maps\n  \n\n  \n    Believe me, I rarely open any 3D panel in Photoshop because I&#8217;m a 2D chauvinist, so this one is really a basic example that may make the actual experts laugh, but it&#8217;s just to give you the idea of using gradient as Depth Maps.\n  \n\n  \n    \n\n    \n      From the Photoshop CS6 menu 3D &#8211; New Mesh from Layer &#8211; Depth Map to &#8211; Plane\n    \n  \n\n  \n    \n\n    \n      Another couple of Depth Map examples\n    \n  \n\n  \n    Similarly (but with a subtler effect) you can apply those gradients as actual Bump Maps in 3D materials.\n  \n\n  \n    Animations\n  \n\n  \n    All kind of weird stuff&#8230; this is just an example of what happens if you shift Gradients (on selected channels only) when one or more parametric Curves are applied. Click on the image to launch the video.\n  \n\n  \n    \n\n    \n      Few examples &#8211; scripting the gradient interaction would lead to even more interesting results\n    \n  \n\n  \n    Parametric Curves on Exchange!\n  \n\n  \n    Parametric Curves is available as a free script for Photoshop CS6 (Mac/Win) through Adobe Exchange &#8211; the new in-app, app-store panel made by Adobe itself. Download and install Exchange if you don&#8217;t already have it, then browse the Free extensions and look for Parametric Curves there. After the installation, please find it in the Filter menu. More info about the panel here. \n  \n\n",
      tags: ["3D","bump map","Curves","depth map","displacement","Gradient"],
      id: 82
    });
    

    index.add({
      title: "Presets management with DropDownList and XML in ExtendScript",
      category: ["Coding"],
      content: "\n  \n    ScriptUI windows can contain several controls (checkboxes, sliders, etc), and to setup a Preset system is a very handy way to allow users save and retrieve their own preferred configurations easily. In this tutorial I&#8217;ll show you how to implement Presets with a DropDownList menu based upon XML data.\n  \n\n  \n    \n  \n\n  \n    Starting point\n  \n\n  \n    I&#8217;ve set up a simple demo dialog using a resource string &#8211; you can use it either in InDesign or Photoshop for instance: this is the base which I&#8217;ll be using throughout the post adding functional bits of code. (I&#8217;ve used CoffeeScript, a nice language that compiles to JavaScript, to write the whole script; in this tutorial I&#8217;m going to show the JS only, but you can find the entire CoffeScript code at the bottom of the page if you&#8217;re curious).\n  \n\n  #target photoshop;\n\nvar win, windowResource;\n\nwindowResource = \"dialog {  \\\n    orientation: 'column', \\\n    alignChildren: ['fill', 'top'],  \\\n    size:[410, 210], \\\n    text: 'XML DropDownList Presets demo - www.davidebarranca.com',  \\\n    margins:15, \\\n    \\\n    controlsPanel: Panel { \\\n        orientation: 'column', \\\n        alignChildren:'right', \\\n        margins:15, \\\n        text: 'Controls', \\\n        controlsGroup: Group {  \\\n        orientation: 'row', \\\n        alignChildren:'center', \\\n        st: StaticText { text: 'Amount:' }, \\\n        mySlider: Slider { minvalue:0, maxvalue:500, value:300, size:[220,20] }, \\\n        myText: EditText { text:'300', characters:5, justify:'left'} \\\n        } \\\n    }, \\\n    presetsPanel: Panel { \\\n        orientation: 'row', \\\n        alignChildren: 'center', \\\n        text: 'Presets', \\\n        margins: 14, \\\n        presetList: DropDownList {preferredSize: [163,20] }, \\\n        saveNewPreset: Button { text: 'New', preferredSize: [44,24]}, \\\n        deletePreset: Button { text: 'Remove', preferredSize: [60,24]}, \\\n        resetPresets: Button { text: 'Reset', preferredSize: [50,24]} \\\n    }, \\\n    buttonsGroup: Group{\\\n        alignChildren: 'bottom',\\\n        cancelButton: Button { text: 'Cancel', properties:{name:'cancel'},size: [60,24], alignment:['right', 'center'] }, \\\n        applyButton: Button { text: 'Apply', properties:{name:'apply'}, size: [100,24], alignment:['right', 'center'] }, \\\n        }\\\n    }\";\n\nwin = new Window(windowResource);\n// binds mySlider.value with myText.text\nwin.controlsPanel.controlsGroup.myText.onChange = function() {\n  this.parent.mySlider.value = Number(this.text);\n};\nwin.controlsPanel.controlsGroup.mySlider.onChange = function() {\n  this.parent.myText.text = Math.ceil(this.value); }; win.show();&nbsp;\n\n  \n    This scheme lets you:\n  \n\n  \n    \n      Save new custom presets.\n    \n    \n      Delete custom presets.\n    \n    \n      Reset to the default group of presets (which are protected and cannot be deleted).\n    \n  \n\n  \n    The presets are stored as XML data in a file (in the example there&#8217;s a single value only, but they can be as many as you need)\n  \n\n  &lt;presets&gt;\n  &lt;preset default=\"true\"&gt;\n    &lt;name&gt;select...&lt;/name&gt;\n    &lt;value/&gt;\n  &lt;/preset&gt;\n  &lt;preset default=\"true\"&gt;\n    &lt;name&gt;Default value&lt;/name&gt;\n    &lt;value&gt;100&lt;/value&gt;\n  &lt;/preset&gt;\n  &lt;preset default=\"true\"&gt;\n    &lt;name&gt;Low value&lt;/name&gt;\n    &lt;value&gt;10&lt;/value&gt;\n  &lt;/preset&gt;\n  &lt;preset default=\"true\"&gt;\n    &lt;name&gt;High value&lt;/name&gt;\n    &lt;value&gt;400&lt;/value&gt;\n  &lt;/preset&gt;\n&lt;/presets&gt;\n\n  \n    The default=\"true\" attribute marks this presets as read-only (i.e. the user can&#8217;t delete them)\n  \n\n  \n    Logic\n  \n\n  \n    This is how I&#8217;ve decomposed the problem (click to open a bigger version)\n  \n\n  \n    \n  \n\n  \n    Building the Presets system\n  \n\n  \n    Basically the core is an initialization routine that reads XML data from an XML file (or creates a default one if it&#8217;s missing) and fills the DropDownList (DDL from now on) with labels, so let&#8217;s start with it.\n  \n\n  \n    1. Initialization routine\n  \n\n  \n    I&#8217;ve set few globals that will be useful for other functions too &#8211; the code is commented so it should be quite self-explanatory.\n  \n\n  // the XML object holding the XML data\nvar xmlData = null;\n\n// the array holding the preset labels, used later on\n// to check for label duplicates when adding a new preset\nvar presetNamesArray = [];\n\n// the preset file (which lives alongside this script)\nresPath = File($.fileName).parent;\nvar presetFile = new File(\"\" + resPath + \"/presets.xml\");\n\n// Initialization routine\nvar initDDL = function() {\n  var i, nameListLength;\n\n  if (!presetFile.exists) {\n    createDefaultXML();\n    // recursive call, needed to read and fill the DDL\n    initDDL();\n  }\n\n  // retrieves XML Data\n  xmlData = readXML();\n  // empties the DDL before adding new content\n  if (win.presetsPanel.presetList.items.length !== 0) {\n    win.presetsPanel.presetList.removeAll();\n  }\n\n  // the number of preset labels\n  nameListLength = xmlData.preset.name.length();\n  presetNamesArray.length = 0;\n\n  // for each preset XML element, retrieve the &lt;name&gt; label\n  // and write it in the Labels array, then add it to the DDL\n  i = 0;\n  while (i &lt; nameListLength) {\n    presetNamesArray.push(xmlData.preset.name[i].toString());\n    win.presetsPanel.presetList.add(\"item\", xmlData.preset.name[i]);\n    i++;\n  }\n\n  // set as the default the first preset\n  // which is the placeholder \"select...\"\n  win.presetsPanel.presetList.selection = win.presetsPanel.presetList.items[0];\n  return true;\n};\n\n  \n    As you&#8217;ve seen I&#8217;ve mentioned some utility functions like readXML() and writeXML(), alongside createDefaultXML(), here they are:\n  \n\n  // holds the XML object containing the\n// Default set of Presets\nvar defaultXML = &lt;presets&gt;\n\t\t   &lt;preset default=\"true\"&gt;\n\t\t   &lt;name&gt;select...&lt;/name&gt;\n\t\t   &lt;value&gt;&lt;/value&gt;\n\t        &lt;/preset&gt;\n\t        &lt;preset default=\"true\"&gt;\n\t\t   &lt;name&gt;Default value&lt;/name&gt;\n\t\t   &lt;value&gt;100&lt;/value&gt;\n\t        &lt;/preset&gt;\n\t        &lt;preset default=\"true\"&gt;\n\t\t   &lt;name&gt;Low value&lt;/name&gt;\n\t\t   &lt;value&gt;10&lt;/value&gt;\n\t        &lt;/preset&gt;\n\t        &lt;preset default=\"true\"&gt;\n\t       \t   &lt;name&gt;High value&lt;/name&gt;\n\t\t   &lt;value&gt;400&lt;/value&gt;\n                &lt;/preset&gt;\n\t     &lt;/presets&gt;;\n\n// writes an XML object to file\nwriteXML = function(xml, file) {\n  if (file == null) {\n    // global, the preset.xml file already declared\n    file = presetFile;\n  }\n  try {\n    file.open(\"w\");\n    file.write(xml);\n    file.close();\n  } catch (e) {\n    alert(\"\" + e.message + \"\\nThere are problems writing the XML file!\");\n  }\n  return true;\n};\n\n// reads an XML object from a file\nreadXML = function(file) {\n  var content;\n  if (file == null) {\n    // global, the preset.xml file already declared\n    file = presetFile;\n  }\n  try {\n    file.open('r');\n    content = file.read();\n    file.close();\n    return new XML(content);\n  } catch (e) {\n    alert(\"\" + e.message + \"\\nThere are problems reading the XML file!\");\n  }\n  return true;\n};\n\n// creates a preset.xml file\n// using the default set of presets\ncreateDefaultXML = function() {\n  // global, the preset.xml file already declared\n  if (!presetFile.exists) {\n    writeXML(defaultXML);\n    return true;\n  } else {\n    presetFile.remove();\n    // recursive call\n    createDefaultXML();\n  }\n  return true;\n};\n\n  \n    So far the script detects whether a preset.xml file exists: if it doesn&#8217;t, it creates a default one &#8211; otherwise it reads it and use it to fill the DDL.\n  \n\n  \n    2. Save a new preset\n  \n\n  \n    I&#8217;ve setup the following onClick() function which makes use of createPresetChild(), another utility that grabs values from the GUI and creates a  node from them. Which node, in turn, will be appended to the existing XML data.\n  \n\n  // used to workaround the lack of indexOf in ExtendScript\nvar __indexOf = [].indexOf || function(item) { for (var i = 0, l = this.length; i &lt; l; i++) { if (i in this &amp;&amp; this[i] === item) return i; } return -1; };\n\n// creates a XML &lt;preset&gt; node\nvar createPresetChild = function(name, value) {\n  var child;\n  // mind you, each custom node has the default attribute\n  // set to \"false\", meaning that it can be deleted\n  return child = &lt;preset default=\"false\"&gt;\n\t\t    &lt;name&gt;{name}&lt;/name&gt;\n\t\t    &lt;value&gt;{value}&lt;/value&gt;\n\t\t &lt;/preset&gt;;\n};\n\n// onClick handler for the Save New Preset button\nwin.presetsPanel.saveNewPreset.onClick = function() {\n  var child, presetName;\n\n  // ask for a preset name\n  presetName = prompt(\"Give your preset a name!\\nYou'll find it in the preset list.\", \"User Preset\", \"Save new Preset\");\n  if (presetName == null) {\n    return;\n  }\n\n  // in case the name already exists\n  if (__indexOf.call(presetNamesArray, presetName) &gt;= 0) {\n    alert(\"Duplicate name!\\nPlease find another one.\");\n    // make a recursive call to this onClick function\n    win.presetsPanel.saveNewPreset.onClick.call();\n  }\n\n  // creates a &lt;preset&gt; node with values grabbed\n  // from the window controls\n  child = createPresetChild(presetName, win.controlsPanel.controlsGroup.myText.text);\n\n  // appends the XML node to the existing XML data (global variable)\n  xmlData.appendChild(child);\n\n  // writes the XML object to the preset.xml file\n  writeXML(xmlData);\n\n  // you need to initialize again, in order to populate\n  // the DDL with new values\n  initDDL();\n\n  // selects the last preset in the DDL\n  win.presetsPanel.presetList.selection = win.presetsPanel.presetList.items[win.presetsPanel.presetList.items.length - 1];\n};\n\n  \n    Please notice the lack of indexOf in ExtendScript (the used __indexOf is courtesy of CoffeeScript)\n  \n\n  \n     3. Delete preset\n  \n\n  \n    That&#8217;s pretty straightforward, you just need to check whether the default attribute of the preset is false (which means that the preset is not protected and can be deleted). Again, each preset&#8217;s modification needs to write the changes to disk in the presets.xml file and re-initialize the DDL in order to populate the menu properly.\n  \n\n  // onClick handler for the Delete preset button\nwin.presetsPanel.deletePreset.onClick = function() {\n\n  // mind you: typeof xmlData.preset[].@default = XML\n  if (xmlData.preset[win.presetsPanel.presetList.selection.index].@default.toString() === \"true\") {\n    alert(\"Can't delete \\\"\" + xmlData.preset[win.presetsPanel.presetList.selection.index].name + \"\\\"\\nIt's part of the default set of Presets\");\n    return;\n  }\n\n  // ask for confirmation\n  if (confirm(\"Are you sure you want to delete \\\"\" + xmlData.preset[win.presetsPanel.presetList.selection.index].name + \"\\\" preset?\\nYou can't undo this.\")) {\n    // delete the &lt;preset&gt; element\n    delete xmlData.preset[win.presetsPanel.presetList.selection.index];\n  }\n\n  // as usual, write to disk and initialize again\n  writeXML(xmlData);\n  return initDDL();\n};\n\n  \n     4. Reset presets\n  \n\n  \n    Another easy task, just replace presets.xml with a default one (which is hardcoded inside the script).\n  \n\n  // onClick handler for the Reset presets button\nwin.presetsPanel.resetPresets.onClick = function() {\n  if (confirm(\"Warning\\nAre you sure you want to reset the Preset list?\", true)) {\n    createDefaultXML();\n    initDDL();\n  }\n};\n\n  \n     5. Apply preset\n  \n\n  \n    The last thing we&#8217;ve to add is a handler for the DDL onChange event &#8211; that is, the user selects the preset and window controls must be updated with values coming from it.\n  \n\n  // DDL onChange handler\nwin.presetsPanel.presetList.onChange = function() {\n  if (this.selection !== null &amp;&amp; this.selection.index !== 0) {\n    // sets GUI controls\n    win.controlsPanel.controlsGroup.myText.text = xmlData.preset[this.selection.index].value;\n    win.controlsPanel.controlsGroup.mySlider.value = Number(xmlData.preset[this.selection.index].value);\n  }\n};\n\n  \n    Completed script\n  \n\n  \n    Let&#8217;s put everything together! As follows both versions (JS and CoffeeScript). Speaking of the latter, XML management in ExtendScript made me use more than I wanted the backticks (which embed regular JS code), but that&#8217;s fine &#8211; I&#8217;m a big fan of CoffeeScript anyway 🙂\n  \n\n  \n    Javascript\n  \n\n  #target photoshop;\nvar createDefaultXML, createPresetChild, defaultXML, initDDL, presetFile, presetNamesArray, readXML, resPath, win, windowResource, writeXML, xmlData,\n  __indexOf = [].indexOf || function(item) { for (var i = 0, l = this.length; i &lt; l; i++) { if (i in this &amp;&amp; this[i] === item) return i; } return -1; };\n\nwindowResource = \"dialog {  \\\n    orientation: 'column', \\\n    alignChildren: ['fill', 'top'],  \\\n    size:[410, 210], \\\n    text: 'DropDownList Demo - www.davidebarranca.com',  \\\n    margins:15, \\\n    \\\n    controlsPanel: Panel { \\\n        orientation: 'column', \\\n        alignChildren:'right', \\\n        margins:15, \\\n        text: 'Controls', \\\n        controlsGroup: Group {  \\\n            orientation: 'row', \\\n            alignChildren:'center', \\\n            st: StaticText { text: 'Amount:' }, \\\n            mySlider: Slider { minvalue:0, maxvalue:500, value:300, size:[220,20] }, \\\n            myText: EditText { text:'300', characters:5, justify:'left'} \\\n    \t} \\\n    }, \\\n    presetsPanel: Panel { \\\n    \torientation: 'row', \\\n    \talignChildren: 'center', \\\n    \ttext: 'Presets', \\\n    \tmargins: 14, \\\n    \tpresetList: DropDownList {preferredSize: [163,20] }, \\\n    \tsaveNewPreset: Button { text: 'New', preferredSize: [44,24]}, \\\n    \tdeletePreset: Button { text: 'Remove', preferredSize: [60,24]}, \\\n    \tresetPresets: Button { text: 'Reset', preferredSize: [50,24]} \\\n    }, \\\n    buttonsGroup: Group{\\\n        alignChildren: 'bottom',\\\n        cancelButton: Button { text: 'Cancel', properties:{name:'cancel'},size: [60,24], alignment:['right', 'center'] }, \\\n        applyButton: Button { text: 'Apply', properties:{name:'apply'}, size: [100,24], alignment:['right', 'center'] }, \\\n    }\\\n}\";\n\nwin = new Window(windowResource);\nxmlData = null;\npresetNamesArray = [];\nresPath = File($.fileName).parent;\npresetFile = new File(\"\" + resPath + \"/presets.xml\");\ndefaultXML = &lt;presets&gt;\n\t\t&lt;preset default=\"true\"&gt;\n\t\t\t&lt;name&gt;select...&lt;/name&gt;\n\t\t\t&lt;value&gt;&lt;/value&gt;\n\t\t&lt;/preset&gt;\n\t\t&lt;preset default=\"true\"&gt;\n\t\t\t&lt;name&gt;Default value&lt;/name&gt;\n\t\t\t&lt;value&gt;100&lt;/value&gt;\n\t\t&lt;/preset&gt;\n\t\t&lt;preset default=\"true\"&gt;\n\t\t\t&lt;name&gt;Low value&lt;/name&gt;\n\t\t\t&lt;value&gt;10&lt;/value&gt;\n\t\t&lt;/preset&gt;\n\t\t&lt;preset default=\"true\"&gt;\n\t\t\t&lt;name&gt;High value&lt;/name&gt;\n\t\t\t&lt;value&gt;400&lt;/value&gt;\n\t\t&lt;/preset&gt;\n\t&lt;/presets&gt;;\n\nwriteXML = function(xml, file) {\n  if (file == null) {\n    file = presetFile;\n  }\n  try {\n    file.open(\"w\");\n    file.write(xml);\n    file.close();\n  } catch (e) {\n    alert(\"\" + e.message + \"\\nThere are problems writing the XML file!\");\n  }\n  return true;\n};\n\nreadXML = function(file) {\n  var content;\n  if (file == null) {\n    file = presetFile;\n  }\n  try {\n    file.open('r');\n    content = file.read();\n    file.close();\n    return new XML(content);\n  } catch (e) {\n    alert(\"\" + e.message + \"\\nThere are problems reading the XML file!\");\n  }\n  return true;\n};\n\ncreateDefaultXML = function() {\n  if (!presetFile.exists) {\n    writeXML(defaultXML);\n    void 0;\n  } else {\n    presetFile.remove();\n    createDefaultXML();\n  }\n  return true;\n};\n\ncreatePresetChild = function(name, value) {\n  var child;\n  return child = &lt;preset default=\"false\"&gt;\n\t\t\t\t&lt;name&gt;{name}&lt;/name&gt;\n\t\t\t\t&lt;value&gt;{value}&lt;/value&gt;\n\t\t\t&lt;/preset&gt;;\n};\n\ninitDDL = function() {\n  var i, nameListLength;\n  if (!presetFile.exists) {\n    createDefaultXML();\n    initDDL();\n  }\n  xmlData = readXML();\n  if (win.presetsPanel.presetList.items.length !== 0) {\n    win.presetsPanel.presetList.removeAll();\n  }\n  nameListLength = xmlData.preset.name.length();\n  presetNamesArray.length = 0;\n  i = 0;\n  while (i &lt; nameListLength) {\n    presetNamesArray.push(xmlData.preset.name[i].toString());\n    win.presetsPanel.presetList.add(\"item\", xmlData.preset.name[i]);\n    i++;\n  }\n  win.presetsPanel.presetList.selection = win.presetsPanel.presetList.items[0];\n  return true;\n};\n\nwin.controlsPanel.controlsGroup.myText.onChange = function() {\n  return this.parent.mySlider.value = Number(this.text);\n};\n\nwin.controlsPanel.controlsGroup.mySlider.onChange = function() {\n  return this.parent.myText.text = Math.ceil(this.value);\n};\n\nwin.presetsPanel.presetList.onChange = function() {\n  if (this.selection !== null &amp;&amp; this.selection.index !== 0) {\n    win.controlsPanel.controlsGroup.myText.text = xmlData.preset[this.selection.index].value;\n    win.controlsPanel.controlsGroup.mySlider.value = Number(xmlData.preset[this.selection.index].value);\n  }\n  return true;\n};\n\nwin.presetsPanel.resetPresets.onClick = function() {\n  if (confirm(\"Warning\\nAre you sure you want to reset the Preset list?\", true)) {\n    createDefaultXML();\n    return initDDL();\n  }\n};\n\nwin.presetsPanel.saveNewPreset.onClick = function() {\n  var child, presetName;\n  presetName = prompt(\"Give your preset a name!\\nYou'll find it in the preset list.\", \"User Preset\", \"Save new Preset\");\n  if (presetName == null) {\n    return;\n  }\n  if (__indexOf.call(presetNamesArray, presetName) &gt;= 0) {\n    alert(\"Duplicate name!\\nPlease find another one.\");\n    win.presetsPanel.saveNewPreset.onClick.call();\n  }\n  child = createPresetChild(presetName, win.controlsPanel.controlsGroup.myText.text);\n  xmlData.appendChild(child);\n  writeXML(xmlData);\n  initDDL();\n  return win.presetsPanel.presetList.selection = win.presetsPanel.presetList.items[win.presetsPanel.presetList.items.length - 1];\n};\n\nwin.presetsPanel.deletePreset.onClick = function() {\n  if (xmlData.preset[win.presetsPanel.presetList.selection.index].@default.toString() === \"true\") {\n    alert(\"Can't delete \\\"\" + xmlData.preset[win.presetsPanel.presetList.selection.index].name + \"\\\"\\nIt's part of the default set of Presets\");\n    return;\n  }\n  if (confirm(\"Are you sure you want to delete \\\"\" + xmlData.preset[win.presetsPanel.presetList.selection.index].name + \"\\\" preset?\\nYou can't undo this.\")) {\n    delete xmlData.preset[win.presetsPanel.presetList.selection.index];\n  }\n  writeXML(xmlData);\n  return initDDL();\n};\n\ninitDDL();\n\nwin.show();\n\n  \n     CoffeeScript\n  \n\n  `#target photoshop`\n\nwindowResource = \"dialog {  \\\n    orientation: 'column', \\\n    alignChildren: ['fill', 'top'],  \\\n    size:[410, 210], \\\n    text: 'DropDownList Demo - www.davidebarranca.com',  \\\n    margins:15, \\\n    \\\n    controlsPanel: Panel { \\\n        orientation: 'column', \\\n        alignChildren:'right', \\\n        margins:15, \\\n        text: 'Controls', \\\n        controlsGroup: Group {  \\\n            orientation: 'row', \\\n            alignChildren:'center', \\\n            st: StaticText { text: 'Amount:' }, \\\n            mySlider: Slider { minvalue:0, maxvalue:500, value:300, size:[220,20] }, \\\n            myText: EditText { text:'300', characters:5, justify:'left'} \\\n    \t} \\\n    }, \\\n    presetsPanel: Panel { \\\n    \torientation: 'row', \\\n    \talignChildren: 'center', \\\n    \ttext: 'Presets', \\\n    \tmargins: 14, \\\n    \tpresetList: DropDownList {preferredSize: [163,20] }, \\\n    \tsaveNewPreset: Button { text: 'New', preferredSize: [44,24]}, \\\n    \tdeletePreset: Button { text: 'Remove', preferredSize: [60,24]}, \\\n    \tresetPresets: Button { text: 'Reset', preferredSize: [50,24]} \\\n    }, \\\n    buttonsGroup: Group{\\\n        alignChildren: 'bottom',\\\n        cancelButton: Button { text: 'Cancel', properties:{name:'cancel'},size: [60,24], alignment:['right', 'center'] }, \\\n        applyButton: Button { text: 'Apply', properties:{name:'apply'}, size: [100,24], alignment:['right', 'center'] }, \\\n    }\\\n}\";\n\n# Create Dialog\nwin = new Window windowResource\n\n# globals\nxmlData = null\npresetNamesArray = []\nresPath = File($.fileName).parent\npresetFile = new File \"#{resPath}/presets.xml\"\ndefaultXML = `&lt;presets&gt;\n\t\t\t\t\t&lt;preset default=\"true\"&gt;\n\t\t\t\t\t\t&lt;name&gt;select...&lt;/name&gt;\n\t\t\t\t\t\t&lt;value&gt;&lt;/value&gt;\n\t\t\t\t\t&lt;/preset&gt;\n\t\t\t\t\t&lt;preset default=\"true\"&gt;\n\t\t\t\t\t\t&lt;name&gt;Default value&lt;/name&gt;\n\t\t\t\t\t\t&lt;value&gt;100&lt;/value&gt;\n\t\t\t\t\t&lt;/preset&gt;\n\t\t\t\t\t&lt;preset default=\"true\"&gt;\n\t\t\t\t\t\t&lt;name&gt;Low value&lt;/name&gt;\n\t\t\t\t\t\t&lt;value&gt;10&lt;/value&gt;\n\t\t\t\t\t&lt;/preset&gt;\n\t\t\t\t\t&lt;preset default=\"true\"&gt;\n\t\t\t\t\t\t&lt;name&gt;High value&lt;/name&gt;\n\t\t\t\t\t\t&lt;value&gt;400&lt;/value&gt;\n\t\t\t\t\t&lt;/preset&gt;\n\t\t\t\t&lt;/presets&gt;`\n\n# writes an XML object to a file\nwriteXML = (xml, file=presetFile) -&gt;\n\ttry\n\t\tfile.open \"w\"\n\t\tfile.write xml\n\t\tfile.close()\n\tcatch e\n\t\talert \"#{e.message}\\nThere are problems writing the XML file!\"\n\ttrue\n\n# reads a file (default=presets.xml) and returns an XML object\nreadXML = (file=presetFile) -&gt;\n\ttry  \n\t\tfile.open 'r'\n\t\tcontent = file.read()\n\t\tfile.close()\n\t\treturn new XML content\n\tcatch e\n\t\talert \"#{e.message}\\nThere are problems reading the XML file!\"\n\ttrue\n\n# creates a Default XML file\ncreateDefaultXML = () -&gt;\n\t# if file doesn't exist\n\tif !presetFile.exists\n\t\twriteXML defaultXML\n\t\tundefined\n\telse\n\t\tpresetFile.remove()\n\t\tcreateDefaultXML()\n\ttrue\n\n# creates and returns an XML &lt;preset&gt; element\ncreatePresetChild = (name, value) -&gt;\n\tchild = `&lt;preset default=\"false\"&gt;\n\t\t\t\t&lt;name&gt;{name}&lt;/name&gt;\n\t\t\t\t&lt;value&gt;{value}&lt;/value&gt;\n\t\t\t&lt;/preset&gt;`\n\n# Initialize DropDownList\ninitDDL = () -&gt;\n\t# if file doesn't exist\n\tif !presetFile.exists\n\t\tcreateDefaultXML()\n\t\tinitDDL()\n\n\t# file exists\n\txmlData = readXML()\n\t# clean DropDownList\n\twin.presetsPanel.presetList.removeAll() if win.presetsPanel.presetList.items.length isnt 0\n\t# how many presets?\t \n\tnameListLength = xmlData.preset.name.length()\n\t# empties the names array\n\tpresetNamesArray.length = 0\n\t# fills the DDL and the presetNamesArray\n\ti = 0\n\twhile i &lt; nameListLength\n\t\t# toString() otherwise its an array of XML objects!\n\t\tpresetNamesArray.push xmlData.preset.name[i].toString()\n\t\twin.presetsPanel.presetList.add \"item\", xmlData.preset.name[i]\n\t\ti++\n\twin.presetsPanel.presetList.selection = win.presetsPanel.presetList.items[0]\n\ttrue\n\n# binds mySlider.value and myText.text\nwin.controlsPanel.controlsGroup.myText.onChange = () -&gt;\n\t@parent.mySlider.value = Number @text\n\n# binds mySlider.value and myText.text\nwin.controlsPanel.controlsGroup.mySlider.onChange = () -&gt;\n\t@parent.myText.text = Math.ceil @value\n\nwin.presetsPanel.presetList.onChange = () -&gt;\n\tif @selection isnt null and @selection.index isnt 0\n\t\twin.controlsPanel.controlsGroup.myText.text = xmlData.preset[@selection.index].value\n\t\twin.controlsPanel.controlsGroup.mySlider.value = Number xmlData.preset[@selection.index].value\n\ttrue\n\nwin.presetsPanel.resetPresets.onClick = () -&gt;\n\tif confirm \"Warning\\nAre you sure you want to reset the Preset list?\", true\n\t\t# remove existing preset file, creates a Default XML file\n\t\tcreateDefaultXML()\n\t\t# refresh the DropDownList!\n\t\tinitDDL()\n\nwin.presetsPanel.saveNewPreset.onClick = () -&gt;\n\tpresetName = prompt \"Give your preset a name!\\nYou'll find it in the preset list.\", \"User Preset\", \"Save new Preset\"\n\tif !presetName? then return\n\tif presetName in presetNamesArray\n\t\talert \"Duplicate name!\\nPlease find another one.\"\n\t\t# recursion again?!\n\t\twin.presetsPanel.saveNewPreset.onClick.call()\n\t# create a &lt;preset&gt; xml child\n\tchild = createPresetChild presetName, win.controlsPanel.controlsGroup.myText.text\n\t# append the child to the xmlData\n\txmlData.appendChild child\n\twriteXML xmlData\n\t# re-initialize the DDL in order to make changes appear\n\tinitDDL()\n\t# select the last preset\n\twin.presetsPanel.presetList.selection = win.presetsPanel.presetList.items[win.presetsPanel.presetList.items.length - 1]\n\nwin.presetsPanel.deletePreset.onClick = () -&gt;\n\t# \"true\" is a string in the XML, not a boolean!\n\tif `xmlData.preset[win.presetsPanel.presetList.selection.index].@default.toString()` is \"true\"\n\t\talert \"Can't delete \\\"#{xmlData.preset[win.presetsPanel.presetList.selection.index].name}\\\"\\nIt's part of the default set of Presets\"\n\t\treturn\n\tif confirm \"Are you sure you want to delete \\\"#{xmlData.preset[win.presetsPanel.presetList.selection.index].name}\\\" preset?\\nYou can't undo this.\"\n\t\t`delete xmlData.preset[win.presetsPanel.presetList.selection.index]`\n\twriteXML xmlData\n\t# re-initialize the DDL in order to make changes appear\n\tinitDDL()\n\n# Init the DropDownList\ninitDDL()\n\n# Show the window\nwin.show()\n\n",
      tags: ["CoffeeScript","dropdownlist","Extendscript","presets","xml"],
      id: 83
    });
    

    index.add({
      title: "How to open and retouch Hasselblad 3F scanner files in Photoshop",
      category: ["Photoshop"],
      content: "[Updated on March 2019]\nIf you try to open in Photoshop a 3F (a file with extension .fff, the raw format for Hasselblad and Imacon scanners) coming from the Imacon/Hasselblad FlexColor software, you’re going to run into troubles (but there’s plenty of workarounds!):\n\n\n\nThat’s because the Adobe Camera Raw (ACR) that ships with CS6 expects the file to belong to a digital camera - which is not. Actually, the 3F is just a plain, bitmap TIFF file with some extra proprietary tags, that ACR can’t manage properly. This didn’t happen back with CS5, unless you’ve installed some beta version of ACR. There’s of course a workaround, but first let me review briefly why you do want to open a 3F in Photoshop bypassing ACR.\n\nWhy opening a 3F in Photoshop?\n\nIn order to retouch it! Think about the 3F as a scanner raw file - you can tweak each parameter in the Hasselblad FlexColor software and finally export a TIFF. Alas, scanned film has to be retouched: dust, scratches, blotches, hairs, chemical halos, you know what I mean. Unless you’re willing to retouch the TIFF each time you export it with different parameters from FlexColor (you aren’t), the best workflow is:\n\n\n  Open the 3F in Photoshop CS6 in order to retouch it.\n  Save the retouched 3F.\n  Open the retouched 3F in FlexColor (tweaking the parameters to your taste).\n  Export the (usually) TIFF file from FlexColor.\n  Open the TIFF in Photoshop and let the fun begin.\n\n\nWhy should I export many time a 3F, isn’t one time just enough? Fair question, but consider that:\n\n\n  People get more experienced over time, and you may want to elaborate a second time a scan you’ve acquired and processed in the past (and you’re not really satisfied with the old result).\n  You may want to sandwich two or more FlexColor versions (say, one corrected for the highlights, one for the mid-tones and shadows) in Photoshop.\n  You may be a service provider willing to return the client a flawless 3F scan.\n\n\nSo your goal is not to open a 3F with Adobe Camera Raw (which can’t export nor save a 3F), just to open it as a TIFF-like file to retouch it in Photoshop.\n\nHow to\n\nFirst, you need to install the  Imacon 3F Plug-In for Photoshop: create an account on “My Hasselblad” and find it in the download section. Move the file either in:\n\n[MAC] Photoshop CC 2019/Plug-ins/File Formats/imacon3f.plugin\n[PC] Photoshop CC 2019/Plug-ins/File Formats/Imacon 3f.8bi\n\n(or whatever Photoshop version you own). Please note that you can’t just File - Open. You must click the Options button on the bottom-left corner of the open dialog to show the options. Under Format you must select Imacon 3F; then, click one time in the file to open; finally, due to a Photoshop bug, you also need to set All Documents under Enable (by default it selects “All Readable Documents”). This is important, and in this exact sequence, otherwise it won’t work.\n\n\n\nAnd voilà it will open smoothly and easily!\n\nRetouching a 3F in Photoshop\n\nIf you’re new to 3F, I suggest you to visit the excellent Roberto Bigano’s 3F resources page first. My current workflow, when I’m in need of 3F retouching (which is basically… always), is as follows. Click on the thumbnails for a bigger version.\n\n1. Open the 3F and save a .PSB\n\nSay that you’ve a myScan.fff file. Open it in Photoshop and immediately save a copy as a .psb (Photoshop Large Format document), myScan.psb. Leave alone the original .fff file, you’ll be dealing with it again when the retouching is done.\n\n\n\n2. Retouch the PSB\n\nDuplicate the background layer, call it “Retouching” or something else.\n\n\n\nI suggest you to add an Invert adjustment layer (which turns everything into a cyan flat-land) and above it a set of Curves adjustment layer (rough curves, just to restore a decent tone and color and make the retouching easier).\n\n\n\n\n\nOne thing to keep in mind is that you’re not (at this stage) dealing with noise reduction or color correction - here you just do the always-boring, zen-like practice of manual dust and scratches removal. Clone tool, Healing Brush, time and patience will be your friends.\n\nMind you, do not make the Clone tool and the Healing brush sampling “All Layers”. Either “Current Layer” or “Current &amp; Below” will be ok - if you wonder why, try it yourself. When you’re done, remove the now useless adjustment layers and flatten the myScan.psb.\n\n\n\n3. Paste the retouched layer in the 3F\n\nOpen both the original myScan.fff and the retouched myScan.psb. Drag the Background layer from the .psb and shift-drop it (i.e. do that pressing the shift key in order to auto-align it) into the 3F. You should have two layers in the myScan.fff now: the original below and the retouched above.\n\n\n\nFlatten the myScan.fff. Leave it without any embedded ICC profile (remove it if necessary: Edit - Assign Profile - Don’t Color Manage This Document) and double check that it has a single Background layer only. Now save the file as 3F (if you don’t find the Format: Imacon 3F in the saving options the 3F plugin hasn’t been installed correctly) with a different name, say myScan_clean.fff. If you have FlexColor already opened and pointing to the folder where the files are, close and reopen it in order to let the software read them again.\n\n\n\nCaveat\n\nFlexColor preview (at least when your 3Fs are very big - say a 6x12 cm at full resolution) isn’t based upon the actual file reading, but upon an embedded low-res image. Which is not affected by your retouching any way, so the 3F will keep showing dust and scratches. If you want to make sure your retouching has been applied, check it opening the Detail window (it’s the button with the magnifying glass icon): this forces FlexColor to read and show a portion of the actual file. Anyway, when you’ll export and the TIFF, it’ll be clean!\n",
      tags: ["3F","FlexColor","Hasselblad","Imacon"],
      id: 84
    });
    

    index.add({
      title: "Introducing Filter Forge",
      category: ["Filter Forge","Photoshop"],
      content: "\n  \n    \n\n     Filter Forge is a PS plugin; actually it&#8217;s more than a PS plugin, Filter Forge is a PS plugins&#8217; container (some 9000 of them, freely available); no really, that&#8217;s not as exciting as the fact that Filter Forge lets you build your own PS filters. Visually. Oh listen: follow me in this Filter Forge introduction, I&#8217;m gonna show it to you &#8211; it&#8217;s easier than you think and a lot fun.\n  \n\n  \n    \n      \n\n      Filter Forge has been around for some years. As a technology is remarkably interesting, and now that it gets close to version 4 (currently in beta stage) I&#8217;ve decided that it was time to invest some of my money and start exploring it. Basically Filter Forge is both a Photoshop filter and a standalone image processing application (Mac/PC); it comes with several built-in algorithms (i.e. filters), but you&#8217;re allowed to download for free about ten thousands of them (not kidding: actual count is 9.388) from their website &#8211; made by a lively community of developers. Oh, and of course you&#8217;re invited to contribute, since filters building is implemented as a visual, node base operation of dragging &amp; linking components, and rewarded even with a free copy of the software. Read along.\n    \n\n    \n      Textures and Effects\n    \n\n    \n      Filter Forge biggest strength appears to be texture creation (so if you&#8217;re doing any kind of 3D modelling work you should definitely take a look at it). I&#8217;m not too much into that, so I&#8217;ll just mention that it allows procedural texture creation &#8211; which means that you sort of put down the &#8220;texture formula&#8221; and the software will create the final bitmap resolution-independently, in a seamless tiling fashion. Which is kind of cool, to put it mildly: few examples as follows (look at them and keep whispering &#8220;procedurally generated, seamless tiled, 5.000 of them&#8230;&#8220;).\n    \n\n    \n      \n\n      \n        Soft Fur (by Heliagon), Suburbia (by ThreeDee), Biologic I (by Aurum), More Rough Wood (by Sharandra).High resolution image are of course much more detailed than I&#8217;m allowed to show here!\n      \n    \n\n    \n      Each texture, in turn, has its own set of generating parameters that you can tweak in order to get different results &#8211; and several ready made presets for your convenience.\n    \n\n    \n      \n\n      \n        Yes, you can procedurally generate this one too (Swiss Cheese, by Capadilla)\n      \n    \n\n    \n      That said, it&#8217; fair to say that FF engine is astonishingly powerful. \n    \n\n    \n      \n        Mind you: I&#8217;m not gettin paid from them to soak this post with wild enthusiasm, it is entirely in-house. I&#8217;ve purchased their software (version 3 + version 4 preorder) so I&#8217;m not biased. I&#8217;ve then decided to affiliate with&#8217;em because I really believe it&#8217;s a great product, so if you consider trying Filter Forge do it following this link, thanks!\n      \n    \n\n    \n      But what interests me the most, because of my background as a PS retoucher (one life is always too short), are Effects filters &#8211; the kind of ones you&#8217;d apply to 2D pictures (contrast enhancing, artistic effects, etc). On FF website among the about 4.000 algorithms freely available there are true gems, many of which would compete with standalone plugins currently for sale.\n    \n\n    \n      \n\n      \n        Engraving (by Vladimir Gorovin), Sketchy Painting (by emme), Ice Age (by Crapadilla), Ranged Sharpen (by Skybase)\n      \n    \n\n    \n      Filter creation\n    \n\n    \n      FF licensing scheme has three different offers: Basic, Standard and Pro. With the exception of the first one, you can build your own filters and (that&#8217;s truly amazing) modify any existing filter of the library. Say that you&#8217;re crazy about the Ice Age filter, and you wonder how on earth the author has been able to came up with something that funky. Right click on the filter item, select &#8220;Edit filter&#8221; and voilà all the magic is explained (provided you can follow what happens):\n    \n\n    \n      \n    \n\n    \n      Yep, that&#8217;s what a filter looks like, you&#8217;re peeping its algorithm. It&#8217;s made with a mix of components that you drag and drop on the workspace and combine together &#8211; so that the output of the first become the input of the following ones. And so on and so on &#8211; you&#8217;ve not to worry about memory management and writing code (even if there&#8217;s a LUA based scripting engine that you can work with, customizing your own components). You can really focus on the algorithm only.\n    \n\n    \n      To read the full features list, please see Filter Forge website.\n    \n\n    \n      Filter creation example\n    \n\n    \n      I&#8217;m going to show you one of my first (very basic I know, but hey I&#8217;m just a FF beginner!) experiments with Filters creation, which took me about 10 minuts from start to finish &#8211;\n    \n\n    \n      \n        I&#8217;ve implemented Dan Margulis&#8216; &#8220;Bigger Hammer&#8221; technique, from his Picture Postcard Workflow (PPW). Dan also calls the Bigger Hammer &#8220;Shadows/Highlights on steroids&#8221;: basically we talk about a contrast normalization technique based upon an inverted overlay that boost highlights detail and help shadows too.\n      \n\n      \n        \n      \n    \n\n    \n      In Photoshop you&#8217;d do this way &#8211; the Layers palette should be pretty self-explanatory, top-down:\n    \n\n    \n      Darkening Layer: a copy of the background, Darken blending mode (opacity to taste) &#8211; use it to recover shadows darkness.\n    \n\n    \n      Overlay Layer: an inverted channel (either R, G, B) Gaussian Blurred, Overlay blending mode.\n    \n\n    \n      Background: the original image.\n    \n\n    \n      Unless you turn to scripting, to compare different channels for the overlay (say R instead of G) and blurring radii isn&#8217;t possible in real time. I&#8217;ve implemented such controls in Filter forge, so that you&#8217;re able to: switch  the channel for the inverted overlay (R, G or B); modify the Gaussian Blur radius; modify the Overlay layer and Darkening layer opacity (i.e both the overall effect&#8217;s and the Shadow recovering strength). As a plus, an overlay usually affect color as well &#8211; I&#8217;ve implemented it so that Luminosity only is altered. Anyway, here it goes.\n    \n\n    \n      1. Setting up the project\n    \n\n    \n      First thing I create a blank filter (Filter &#8211; New):\n    \n\n    \n      \n    \n\n    \n      As you see, the Result component (what you&#8217;ll get from running the filter in PS) shows a red alert: it has no input yet. Let&#8217;s create one dragging from the Components column on the right, under External group, an Image component on the workspace; linking it to the Result. You do a link dragging the green output arrow of a component on to the green input point of another.\n    \n\n    \n      \n    \n\n    \n      This is a dummy filter that does nothing, i.e. input is equal to output (but it works!). Notice that being italian I&#8217;ve selected a more appropriate placeholder image. Let&#8217;s start adding interesting pieces.\n    \n\n    \n      2. Extracting Luminosity\n    \n\n    \n      One important feature is to leave color alone working with a luminosity channel. FF implements RGB, Lab, HLS, HSB, and HSY colorspaces. Apparently, HSY lightness (Y) channel takes human perception of color into account: it looks promising, so I&#8217;ve added three Extract HSY components and one Assemble HSY to split and recombine channels, linking them as you see in the screenshot &#8211; from now on I&#8217;ll be dealing with the Y channel only.\n    \n\n    \n      \n    \n\n    \n      3. Extracting R, G, B channels\n    \n\n    \n      I now need some channels to be used for the inverted layer blending (in overlay). I&#8217;ll stick to R, G, B for the clarity&#8217;s sake, but nothing would prevent me implementing Lab and HSY as well. So here they are three Extract RGB components hardcoded to output the desired channels (inverted, see the checkbox on the left column).\n    \n\n    \n      \n    \n\n    \n      4. Building a Switch\n    \n\n    \n      FF implements Controls, which are a way to let the user, uhm, control the filter&#8217;s values. Alas, there&#8217;s no built in Radio-button set, so as a workaround I&#8217;ve to use a Switch component linked with an IntSlider. The Slider ranges 1-2-3, which enables the R, G, B positions of the Switch (i.e. 1=R, 2=G, 3=B).\n    \n\n    \n      \n    \n\n    \n      5. Adding the Blur\n    \n\n    \n      Under the Processing group you can find built-in components for traditional image processing algorithm, such as Minimum, Maximum, Blend, Blur, High Pass, etc. I&#8217;ve added a Blur component with a Slider control, remapping it to a maximum value of 40 (I won&#8217;t get into remapping and percent values here). The slider will control the amount of blurring applied to the overlay channel.\n    \n\n    \n      \n    \n\n    \n      6. Overlay Blend\n    \n\n    \n      FF comes with a Blend component that supports common PS blending modes and opacity. I&#8217;ve linked the original Y channel (as the background) to the blurred inverted channel (as the foreground), setting the blend to Overlay. Mind you, I&#8217;ve added a Slider control to tweak the opacity (that is: the effect&#8217;s strength). So far the big picture is as follows:\n    \n\n    \n      \n    \n\n    \n      7. Shadow darkness\n    \n\n    \n      I&#8217;ve added a second Blend, using a second copy of the original Y in Darken (with a Slider controlling the opacity) to mimic the Darkening layer (as in the PS Layers palette screenshot depicts). This time the Y is the foreground, while the result of the overlay blend of the previous step is the Background:\n    \n\n    \n      \n    \n\n    \n      8. Linking the Result\n    \n\n    \n      It&#8217;s time to un-link the Assemble HSY component from the Y channel coming from the Extract HSY, substituting it with the output of the chain of components ending with the shadow darkness blend. Now the filter is done! The complete pipeline is as follows:\n    \n\n    \n      \n    \n\n    \n      You can now save it as a filter, adding info (such as instruction, author, description) and tags. FF allows you a before/after draggable split view, and the controls I&#8217;ve been adding are now automatically gathered in the Settings tab, ready to be modified on the fly:\n    \n\n    \n      \n    \n\n    \n      Conclusions\n    \n\n    \n      \n\n      \n      \n      I&#8217;m really excited to start experimenting with Filter Forge: its powerful processing engine and the peculiar way you build filters with it lead to remarkable results.\n    \n\n    \n      For instance, I&#8217;ve been able to replicate exactly my dear Photoshop Extension ALCE (Advanced Local Contrast Enhancer) and it took me, not kidding, about 5 minutes (which is kind of sad, if I think about all the PS extension development time in Extendscript + Actionscript).\n    \n\n    \n      Of course there&#8217;s room for improvement (on controls, for instance) &#8211; I&#8217;m confident that new version will keep advancing. As a developing platform is really amazing. The only actual big flaw (at least it&#8217;s a flaw from my own personal point of view) is that Filter Forge business model is build around the idea that the software value is boosted by the huge amount of free resources that come with it &#8211; that is, 9.388 filters created by skilled developers from all over the world. Moreover, you can open, study and tweak each one of them &#8211; and that&#8217;s definitely the better way to learn. Alas this business model collides with the possibility to compile your own filters into a binary form to sell them as standalone PS plugins. That&#8217;s not allowed: both for licensing reasons and because that way anyone could just grab one of the 9.388, pack and sell it (not fair, indeed).\n    \n\n    \n      It has to be acknowledged that Filter Forge has several interesting rewarding policies towards filters authors and contributors (a credits scoring system they use to assign free licenses) so they pursue their business model coherently &#8211; kudos to them!\n    \n\n    \n      Please visit Filter Forge for a full list of features, available filters and more!\n    \n  \n\n",
      tags: ["Bigger Hammer","effects","Filter Forge","PPW","textures"],
      id: 85
    });
    

    index.add({
      title: "Double USM #3: Examples",
      category: ["Double USM","Extensions and Scripts","Photoshop"],
      content: "\n  \n    Double USM is a brand new Sharpening script for Photoshop; in this third post of the series I&#8217;ll show you examples for traditional sharpening, HiRaLoAm (High Radius Low Amount) and mixed sharpening.\n  \n\n  \n    \n  \n\n  \n    \n      \n        UPDATE\n      &lt;/p&gt;\n    \n\n    \n      \n        This is a 3 part series on the Double USM Photoshop plugin. Even if the version used in this article is older than the current one, the sharpening concepts still apply! Find the new version announcement here\n      \n\n      \n        \n          Introduction (sharpening basics, Double USM)\n        \n        \n          Features (interface, functionality and Batch processing)\n        \n        \n          Examples (case studies) &lt;- you&#8217;re here!\n        \n      \n\n      \n        Double USM v2 is available for sale on my website CC-Extensions. &lt;/div&gt; &lt;/div&gt;\n\n        \n          Warning!\n        \n\n        \n          If you&#8217;re not familiar with Sharpening or want to recap the basics, I&#8217;ve covered the topic extensively here. We&#8217;re going to explore three kind of sharpening you can apply with Double USM: traditional (fine detail), HiRaLoAm (shape and 3D look), mixed (a combination of the former ones). I&#8217;m going to show an example for each case, but there&#8217;s of course a lot of room for your personal experimentation, which I strongly encourage (you can read a step-by-step Sharpening Case Study I&#8217;ve published some time ago as an example of a possible advanced application).\n        \n\n        \n          Mind you: I&#8217;ll be a bit heavy-handed in the examples &#8211; I&#8217;d like you to spot halos easily; moreover, we&#8217;ll be studying not only &#8220;how much&#8221; sharpening to apply (which can be a matter of taste), but specifically &#8220;which kind&#8220;. That is to say, the Dark / Light Halos ratios, both in terms of Amount and Radius. Don&#8217;t mind too much absolute values (why 345% Amount and not 350%?), but instead think to the relation between Dark and Light sharpening, this way: Light Halos are, say, one third smaller (Radius) and a half weaker (Amount) than Dark Halos.\n        \n\n        \n          I&#8217;m interested in finding the &#8220;right kind&#8221; of sharpening; as once my Maestro Dan Margulis during one of is italian trip said &#8220;one liter of Grappa is too much of a good idea&#8221;, so if you think it&#8217;s over-sharpened, just reduce the opacity of the layer to your taste and it&#8217;ll be OK.\n        \n\n        \n          Traditional Sharpening\n        \n\n        \n          This is what we&#8217;re mostly used to: high frequency detail boosting (i.e. small features and borders). Standard sharpening values include high Amounts (up to 500% if you&#8217;re heavy-handed) and low Radii (it depends on the image size and the output &#8211; display, print/press &#8211; but we talk about values ranging from 0.2/0.4px for the web, up to about 2.0px or more). Threshold is really up to you, and must be evaluate on each picture: sometimes you&#8217;d be better off masking the sharpening level with some kind of luminosity mask or border mask, instead to tweaking the Threshold.\n        \n\n        \n          \n\n          \n            Still-life shot of a Manfrotto leather photo bag by Roberto Bigano\n          \n        \n\n        \n          This first picture comes as a pretty high resolution file &#8211; it&#8217;s been captured using an Hasselblad 528 Multi Shot (four shots recording real non-interpolated color) with a HC4-120 Macro lens.\n        \n\n        \n          \n        \n\n        \n          Roberto masters a particular technique called Focus Stacking (that is, he takes several multi-shot captures focusing at progressively distant planes, then blends the files into a amazingly detailed picture).  Anyway, at left there is a 100% crop of a detail so you can get the idea of the image size and quality. I&#8217;m going to downsample the file to 50% so that the values refers to a picture sized: 2720 x 2040 px. Let&#8217;s pretend we need to sharpen it for a printed ad.\n        \n\n        \n          We can easily predict that a traditional sharpening (high Amount, Low Radius would emphasize too much the fabric texture, which highlights are going to &#8220;eat&#8221; detail, resulting in a polka-dots-like white noise, a bad scenario. On the other hand, sheet-fed presses and high quality coated papers can stand quite a bit of sharpening, so we&#8217;d like to boost the existing detail &#8211; it&#8217;s a finely crafted technical product after all, and the texture should be prominent in my personal opinion.\n        \n\n        \n          So, as follows there&#8217;s a comparison between the original file, the Photoshop&#8217;s UnSharp Mask filter (Amount 300%, Radius 1.5px) and the Double USM default (which Dark Halos are the very same 300% and 1.5px, but Light Halos are 50% thinner and 50% weaker &#8211; that is to say: Amount 150% and Radius 0.7)\n        \n\n        \n          [widgetkit id=1591]\n        \n\n        \n          HiRaLoAm\n        \n\n        \n          This is how Dan Margulis first called a sharpening applied with High Radius, Low Amount (the reverse of traditional sharpening). Instead of enhancing high fine detail, a high Radius (in the range of about 15/50px or more, depending on the image size and the subject&#8217;s features) affects wider frequencies and basically shapes the subject: it makes it appear more three-dimensional.\n        \n\n        \n          We&#8217;ll be using the following image, a Texas Tea hybrid rose (see the caption for full details &#8211; the original file is about 4400x3600px).\n        \n\n        \n          \n\n          \n            Plaublel 4&#215;5 camera with Sinar Vario film holder and Schneider Symmar Macro 180 lens.Fujichrome Velvia50 6x7cm, scanner Hasselblad Flextight X5 &#8211; © Roberto Bigano\n          \n        \n\n        \n          If you&#8217;re not familiar with HiRaLoAm, practice with it this way:\n        \n\n        \n          \n            Open a picture and duplicate the background layer (precautionary measure).\n          \n          \n            Run Photoshop&#8217;s own UnSharp Mask filter and pump up the Amount slider to the max (500%), leave Threshold alone and start raising the Radius up to very large numbers (say 15, 35, 70 pixels or more).\n          \n        \n\n        \n          Oh yes, the picture looks like crap but it&#8217;s OK:\n        \n\n        \n          \n\n          \n            Testing high Radii at crazy Amount &#8211; just to assess the right Radius for HiRaLoAm\n          \n        \n\n        \n          You see that, even though the Amount is absurd, HiRaLoAm shapes the yellow rose&#8217;s volume &#8211; which Radius does the job best in your opinion?\n        \n\n        \n          (I haven&#8217;t bothered to fade the UnSharp Mask filter to Luminosity with &#8220;Edit -&gt; Fade&#8230;&#8221; menu item &#8211; in order to avoid chromatic aberration as you see &#8211; we don&#8217;t care about them now)\n        \n\n        \n          \n            Tweak Threshold if necessary, to suppress noise in areas where it may be a problem.\n          \n          \n            Finally, reduce Amount to pleasing levels &#8211; something like 30% up to 70% (again, these are just suggested values).\n          \n        \n\n        \n          Double USM has the power to let you control both Light and Dark shaping Halos separately (Amount and Radius), so you&#8217;ve unprecedented  control on the 3D effect you&#8217;re after. Please see in the following comparison the original image, Dark Halos only (available in Double USM checking the Dark preview radio-button), Light Halos only (ibid.) and the combination of Double USM Dark + Light Halos (the final result).\n        \n\n        \n          [widgetkit id=1608]\n        \n\n        \n          Applied values on the original sized image are: Dark Halos (Amount: 40%, Radius: 60px), Light Halos (Amount 80%, Radius 80px), Threshold zero. So this time Dark Halos are half weaker and three quarters the size of Light Halos.\n        \n\n        \n          Mixed Sharpening\n        \n\n        \n          So far we&#8217;ve seen how to tweak Dark and Light Halos in traditional sharpening to boost fine detail, and how to shape a picture with broader halos in HiRaLoAm to get a three-dimensional effect.\n        \n\n        \n          Besides advanced topics (masking and iterative sharpening for instance), an easy yet very effective technique that Double USM only allows is:\n        \n\n        \n          \n            Use Dark Halos at high Amount and low Radius to boost the structure as you&#8217;d do with traditional sharpening.\n          \n          \n            Use Light Halos to add volume, with low Amount and High Radius as you&#8217;d do with HiRaLoAm.\n          \n        \n\n        \n          The following is just one example, and it comes from Roberto Bigano&#8217;s Plastic Girls project (a 30 years long study on mannequins around the world). This &#8220;girl&#8221; has been shot with a Canon G12 compact camera, which at 80 ISO delivers an amazing quality, and it&#8217;s just a low-res crop 1500px wide.\n        \n\n        \n          Applied Double USM values are as follows: Dark Halos (Amount: 500%, Radius: 0.5px), Light Halos (Amount 40%, Radius 40px), Threshold (3 levels).\n        \n\n        \n          [widgetkit id= 1615]\n        \n\n        \n          Experiment!\n        \n\n        \n          Sharpening is a topic with quite many ramification &#8211; so there&#8217;s a lot of room for personal research; your skills will grow as you get more experienced. Have a look to this step-by-step Sharpening Case Study I&#8217;ve published some time ago, then start experimenting on your own.\n        \n\n        \n          \n            \n              UPDATE\n            &lt;/p&gt;\n          \n\n          \n            \n              This is a 3 part series on the Double USM Photoshop plugin. Even if the version used in this article is older than the current one, the sharpening concepts still apply! Find the new version announcement here\n            \n\n            \n              \n                Introduction (sharpening basics, Double USM)\n              \n              \n                Features (interface, functionality and Batch processing)\n              \n              \n                Examples (case studies) &lt;- you&#8217;re here!\n              \n            \n\n            \n              Double USM v2 is available for sale on my website CC-Extensions. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; \n              \n\n",
      tags: ["Double USM","halos","sharpening","unsharp mask"],
      id: 86
    });
    

    index.add({
      title: "Double USM #2: Features",
      category: ["Double USM","Extensions and Scripts","Photoshop"],
      content: "\n  \n    Double USM is the brand new Sharpening script for Photoshop that I&#8217;ve coded; in this second post of the series I&#8217;ll cover the user interface, functionality and how to record it into an Action for batch processing.\n  \n\n  \n    \n  \n\n  \n    \n      \n        UPDATE\n      &lt;/p&gt;\n    \n\n    \n      \n        This is a 3 part series on the Double USM Photoshop plugin. Even if the version used in this article is older than the current one, the sharpening concepts still apply! Find the new version announcement here\n      \n\n      \n        \n          Introduction (sharpening basics, Double USM)\n        \n        \n          Features (interface, functionality and Batch processing) &lt;- you&#8217;re here!\n        \n        \n          Examples (case studies)\n        \n      \n\n      \n        Double USM v2 is available for sale on my website CC-Extensions. &lt;/div&gt; &lt;/div&gt;\n\n        \n          If you&#8217;re not familiar with Sharpening or want to recap the basics, I&#8217;ve covered the topic extensively here.\n        \n\n        \n          Interface\n        \n\n        \n          First, access Double USM from the Photoshop&#8217;s Filter menu. The User Interface It is quite straightforward, have a look at it:\n        \n\n        \n          \n        \n\n        \n          Basically the upper part of the panel is where sharpening controls belong (1, 2, 3), while in the lower part you can find preview tools (4, 5) and confirmation buttons (6). Details as follows:\n        \n\n        \n          \n            Dark Group: these two sliders let you control Amount and Radius for the sharpening&#8217;s Dark Halos only (equivalent of the Photoshop&#8217;s UnSharp Mask Filter, but darkening only). Input in the text field is allowed too.\n          \n          \n            Light Group: same as above, Amount and Radius but for the sharpening&#8217;s Light Halos only (text input allowed).\n          \n          \n            Threshold: equivalent of the UnSharp Mask Filter &#8211; a single control for both Dark and Light Group.\n          \n          \n            Zoom: Zoom In, Zoom Out and Fit to Screen buttons. Resize the preview accordingly and inform you of the current zoom level in percent.\n          \n          \n            Preview: radio buttons that set the preview either On (you see the effect of both Dark and Light sharpening Halos), Off (unsharpened original), Dark (Dark Halos only), Light (Light Halos only).\n          \n          \n            Confirmation buttons: Cancel the dialog and switch back to the original image, or Apply the sharpening effect.\n          \n        &lt;/div&gt;\n\n        \n          Functionality\n        \n\n        \n          Double USM runs on both on 8bit and 16bit files, in Grayscale, RGB, Lab and CMYK working spaces. Open a picture, find Double USM in the Photoshop&#8217;s Filter menu:\n        \n\n        \n          \n        \n\n        \n          Mind you, you&#8217;ve got to select a bitmap layer or a Smart Object in the Layers palette (that is: not a text layer, not a vector shape, just something with raster pixels in it &#8211; if you&#8217;re not sure what this mean, you have see in the Layers&#8217; palette layer thumbnail your picture, and that&#8217;s ok).\n        \n\n        \n          Now Photoshop goes fullscreen to hide disturbing interface elements and a small progress bar window pops up: Double USM must calculate a bunch of Smart Objects with Smart Filters and blending modes in order to operate properly (if you&#8217;re curious, the routine is publicly available, I&#8217;ve written extensively about it and published a 5 posts series called Decomposing Sharpening) &#8211; it should be pretty fast, but it eventually depends on your computer performance.\n        \n\n        \n          When Double USM is done with its calculation the main window appears (you can drag it along the screen freely) and you can start tweaking the values.\n        \n\n        \n          \n        \n\n        \n          Use the Zoom buttons to reach the zoom level you&#8217;re most comfortable with when evaluating sharpening, pan the image (the hand tool is selected by default) to inspect the result in significant areas. You can either drag sliders or input values directly in the text field (press tab to switch between them). The routine&#8217;s update takes about half of a second to complete (may be even faster on some computers), so please wait the preview&#8217;s refresh of one control, before moving to another one.\n        \n\n        \n          The Preview radio buttons are crucial to evaluate the sharpening effect. You can switch it On and Off, but you&#8217;re allowed to inspect Light Halos only (Light radio button) or Dark Halos only (Dark radio button): this is particularly useful because you can concentrate on the Dark/Light values separately, then judge their combination (preview: On) before clicking the Apply button.\n        \n\n        \n          \n        \n\n        \n          Mind you: the preview setting that is active when you press Apply will determine the final result. That is to say: if the preview is set to Light when clicking Apply, only Light Halos will be applied &#8211; conversely, if the preview is set to Dark, only Dark Halos will survive. So, unless you want it expressly, remember to switch the preview to On before clicking Apply.\n        \n\n        \n          As you see in the Layer&#8217;s palette screenshot, the resulting elaboration of the Double USM routine belongs to a new layer (i.e. the original layer is left untouched), named according to the used parameters. In the example, &#8220;Double USM: D(300, 1.5), L(150, 0.7), T(0)&#8221; means that I&#8217;ve applied Amount 300% and Radius 1.5px for the Dark Halos, Amount 150% and Radius 0.7px for the Light Halos, and Threshold has been left to its default, zero. In case you want to apply Double USM to a Smart Object (yes you can), the resulting layer will be a new, raster one anyway &#8211; scripts can&#8217;t be tied as smart filters (this is a technical limit of the platform).\n        \n\n        \n          Lastly, I&#8217;d like to mention that no matter whether the layer you&#8217;re applying Double USM to is beneath other layers (adjustments, texts, vectors, whatever), the preview and the result will be based on that very layer only &#8211; this is consistent with Photoshop&#8217;s Filters general behavior.\n        \n\n        \n          Actions\n        \n\n        \n          Double USM can be recorded into Actions, so that you&#8217;re allowed to apply it on batch via the Photoshop&#8217;s File &#8211; Automate &#8211; Batch command. This is particularly appropriate when you&#8217;ve to process blindly large amount of images with output sharpening for instance (before sending them to some kind of printing process).\n        \n\n        \n          Just create a new Action and, while recording, run Double USM. You can check later in the Actions palette the used parameters.\n        \n\n        \n          That&#8217;s basically it for Double USM interface, functionality and the Actions feature. The next post will be about real world examples &#8211; traditional, HiRaLoAm and mixed sharpening.\n        \n\n        \n          \n            \n              UPDATE\n            &lt;/p&gt;\n          \n\n          \n            \n              This is a 3 part series on the Double USM Photoshop plugin. Even if the version used in this article is older than the current one, the sharpening concepts still apply! Find the new version announcement here\n            \n\n            \n              \n                Introduction (sharpening basics, Double USM)\n              \n              \n                Features (interface, functionality and Batch processing) &lt;- you&#8217;re here!\n              \n              \n                Examples (case studies)\n              \n            \n\n            \n              Double USM v2 is available for sale on my website CC-Extensions. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; \n            \n\n",
      tags: ["batch","Double USM","halos","sharpening","unsharp mask"],
      id: 87
    });
    

    index.add({
      title: "Double USM Photoshop Sharpening Script #1: Introduction",
      category: ["Double USM","Extensions and Scripts","Photoshop"],
      content: "\n  \n    Double USM is the brand new Sharpening script for Photoshop that I&#8217;ve coded, let me introduce it to you in a three posts series! Post #1, this one, I&#8217;ll be discussing sharpening basics and how Double USM is different. Post #2 is about the interface, functionality and batch processing. In post #3 I&#8217;ll show you real world examples, for traditional, HiRaLoAm and mixed sharpening.\n  \n\n  \n    \n  \n\n  \n    \n      \n        UPDATE\n      &lt;/p&gt;\n    \n\n    \n      \n        This is a 3 part series on the Double USM Photoshop plugin. Even if the version used in this article is older than the current one, the sharpening concepts still apply! Find the new version announcement here\n      \n\n      \n        \n          Introduction (sharpening basics, Double USM) &lt;- you&#8217;re here\n        \n        \n          Features (interface, functionality and Batch processing)\n        \n        \n          Examples (case studies)\n        \n      \n\n      \n        Double USM v2 is available for sale on my website CC-Extensions. &lt;/div&gt; &lt;/div&gt;\n\n        \n          Why bother with Sharpening at all?\n        \n\n        \n          If you look away from the monitor for a moment, you&#8217;ll see the world you live in as a source of an apparently endless detail; things around you can be probed up to the microscopic level (provided you&#8217;ve got a superhero eyesight) and still reveal new features. Which is not a big news, as it&#8217;s not a big news to discover that digital captures of the world (either a landscape, a portrait, a still life&#8230;) you&#8217;ve taken with some expensive equipment don&#8217;t contain such an infinite detail &#8211; just a portion of it. It&#8217;s not CSI, zoom in 10.000% on your 16bit RAW file and you&#8217;ll be staring just at a huge, single pixel.\n        \n\n        \n          The fact you&#8217;re not allowed by the laws of the universe to capture but a portion of the world&#8217;s infinite detail, results in a softer picture &#8211; if you compare what you perceive out there in the wild, with your digital images. Softness you&#8217;ve got to compensate for, especially if pictures are going to be printed (that is, they&#8217;ll undergo another reduction of information levels available).\n        \n\n        \n          Luckily, our perception is easily deceived by appearances &#8211; if you inject your pictures with a little bit of the right drug, they suddenly look &#8220;right&#8221;, or &#8220;better&#8221;. That drug is called sharpening.\n        \n\n        \n          Halos\n        \n\n        \n          How is it that we&#8217;re tricked into perceiving things as more detailed? Short answer: by means of halos around borders, that&#8217;s the little magic. Long answer would mention things like &#8220;retina&#8217;s receptive fields&#8220;, &#8220;lateral inhibition&#8221; and &#8220;Mach bands&#8221; &#8211; but I won&#8217;t go any further here, even though they&#8217;re fascinating topics.\n        \n\n        \n           So: a border is found where two differently perceived areas meet: it may be one darker the other lighter, different color, tone, etc. What sharpening as we know it in the Photoshop&#8217;s implementation (aka Gaussian Sharpening) does, is to create two tiny gradients around borders. A darker one spreading inside the darker area, a lighter one spreading inside the lighter area. If those artifacts are small enough to be noticed as just artifacts, our perception blends them and we get the impression of more detail &#8211; we&#8217;re that easy as humans, you see.\n        \n\n        \n          \n        \n\n        \n          UnSharp Mask filter\n        \n\n        \n          Photoshop owns one since version 1.0. UnSharpMask (USM from now on) lets you specify three parameters to describe the sharpening artifacts (that is: halos) features.\n        \n\n        \n          \n            How strong are they (Amount)\n          \n          \n            How widespread are they (Radius)\n          \n          \n            Should they be applied everywhere (Threshold)?\n          \n        \n\n        \n          The three are independent controls: as far as so called &#8220;traditional sharpening&#8221; is involved, you usually set large Amounts and relatively small Radii &#8211; as in the screenshot, which is a reasonable starting point for web-targeted images; while the contrary may give interesting results as well (read along about HiRaLoAm).\n        \n\n        \n          Find below the controls&#8217; effect on a synthetic test file.\n        \n\n        \n          \n\n          \n            Amount: the Halos&#8217; strength\n          \n        \n\n        \n          \n\n          \n            Radius: the Halos&#8217; thickness\n          \n        \n\n        \n          \n\n          \n            Threshold: the Halos&#8217;&#8230; threshold\n          \n        \n\n        \n          I must mention one thing: we all know what oversharpening means &#8211; it&#8217;s a delicate topic, photographers go nuts when retouchers oversharpen their images &#8211; i.e. they are strong handed with USM filter.\n        \n\n        \n          \n\n          \n            What do you think about, is this picture oversharpened?\n          \n        \n\n        \n          The psychological threshold (how much USM is too much?) is to some extent a subjective choice. As a retoucher, I admit I&#8217;ve kind of a higher tolerance (I know for instance that printed images require, and can stand, much more USM that you&#8217;d consider fair just looking at them in a monitor).\n        \n\n        \n          So what&#8217;s wrong with USM?\n        \n\n        \n          My color correction maestro Dan Margulis did study the topic of sharpening (among the rest) extensively, and came up with some experimental discoveries worth mentioning. What is that we&#8217;re bothered the most with, when we look at oversharpened pictures? Halos of course, but he&#8217;s been the first one to ask the simple question: &#8220;Ok, but which kind?&#8221;.\n        \n\n        \n          Sharpening introduces at the same time Light Halos and Dark Halos &#8211; do we respond to them in the same way?\n        \n\n        \n          Take a look at the following picture.\n        \n\n        \n          \n\n          \n            A: original image; B: sharpened image (Light + Dark Halos); C: Light Halos only; D: Dark Halos only.\n          \n        \n\n        \n          We may agree that the original version is way too soft, while the sharpened&#8217;s gone too far. But&#8230; what about the rest? Light Halos, as well, may seem too much, while we may be perfectly fine with Dark halos.\n        \n\n        \n          That is to say: we&#8217;re easily disturbed by Light halos, while Dark ones are less annoying. Whether it is their strength (Amount), or their size (Radius) that bother us, we&#8217;re perhaps not sure about &#8211; but Light Halos should be toned down, somehow. So far, an easy and well known way to do this was to apply USM on a layer, duplicate it, set one layer to Darken (Dark Halos only) and the other to Lighten (Light Halos only) and lower the latter&#8217;s opacity. Which is somehow equivalent to lower the Amount of the Light Halos altogether.\n        \n\n        \n          But what about if Light Halos could be made smaller (Radius) too?\n        \n\n        \n          \n\n          \n            E: image sharpened traditionally. F: sharpening where Light Halos are half the strength of, and two third thinner than, the Dark Halos. (heavy handed correction just to show you more clearly the difference!)\n          \n        \n\n        \n          The picture will benefit from a sharpening that is more robust on the dark side (stronger in Amount, wider in Radius), and less on the light side (weaker Amount, narrower Radius), as you can see in the above comparison.\n        \n\n        \n          \n\n          \n            Double USM user interface: Amount and Radius controls for both Dark and Light Halos\n          \n        \n\n        \n          Double USM\n        \n\n        \n          Since drum scanners&#8217; softwares did have this feature decades ago, why not implementing it back in Photoshop now? Double USM lets you tweak both Amount and Radius for Dark and Light halos separately, at the same time.\n        \n\n        \n          A double set of sliders can be useful not only for traditional sharpening, but it gives unprecedented control also for HiRaLoAm (as Dan Margulis calls it: sharpening with unusual High Radius and Low Amount, used as a local contrast enhancer), or as a tool for mixed sharpening (Dark Halos: traditional, Light Halos: HiRaLoAm).\n        \n\n        \n          You may be tempted to do the trick with Darken and Lighten layers but save your time: alas it can&#8217;t work, as I&#8217;ve demonstrated here.\n        \n\n        \n          \n            \n              UPDATE\n            &lt;/p&gt;\n          \n\n          \n            \n              This is a 3 part series on the Double USM Photoshop plugin. Even if the version used in this article is older than the current one, the sharpening concepts still apply! Find the new version announcement here\n            \n\n            \n              \n                Introduction (sharpening basics, Double USM) &lt;- you&#8217;re here\n              \n              \n                Features (interface, functionality and Batch processing)\n              \n              \n                Examples (case studies)\n              \n            \n\n            \n              Double USM v2 is available for sale on my website CC-Extensions. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; \n            \n\n",
      tags: ["Double USM","halos","sharpening","unsharp mask"],
      id: 88
    });
    

    index.add({
      title: "Selling digital books with Apple: iBooksAuthor, InDesign, Digital Publishing Suite",
      category: ["Adobe DPS","Digital Publishing","iBooksAuthor"],
      content: "\n  \n    If you&#8217;re willing to produce digital books to be sold by Apple, be aware that each one of the available tools will shape your content and how it is going to be delivered in a very peculiar way. This post will cover InDesign, iBooksAuthor and the Adobe Digital Publishing Suite targeting Apple iBookStore and AppStore with ePub, iBooks and Apps. Which one does fit your needs the best? I&#8217;ll try to show you Pro and Cons from a point of view that is halfway between strategic and technical.\n\n     \n  \n\n  \n    Digital Publishing is a hot topic, for a number of good reasons I&#8217;m not going into here. But when it comes to actually producing the content things get a bit fuzzy, at least this was my personal impression as I approached the subject. What format? What software? What device? What store?\n  \n\n  \n    If you narrow the available combinations defining Apple as the seller of your digital content (which is just a single constraint among the many ones you can choose from &#8211; a very reasonable one I&#8217;d say), the degree of freedom dramatically drops: not enough to avoid confusion and to protect you from unexpected issues.\n  \n\n  \n    Apple iBookStore\n  \n\n  \n    It is where you&#8217;d assume your digital books to be sold. People who&#8217;ll get them must have, and read them with, an Apple mobile device (iPhone, iPod Touch or iPad); no, you can not consume any content on your laptop or desktop computer if you&#8217;ve bought it from Apple as a digital book. That&#8217;s the way it is, so far. Amazon Kindle for instance is different &#8211; the reader software can be installed on a number of different devices (including the iPad and your personal computer too).\n  \n\n  \n    File Formats\n  \n\n  \n    iBookStore accepts just two file formats for publication: ePub and iBooks.\n  \n\n  \n    \n  \n\n  \n    ePub\n  \n\n  \n    It is a standard format based on XML. So to speak, it&#8217;s perfect for text only books because you can format your content&#8217;s layout only up to a very, very basic level: that is, Title is bigger, Body is smaller, Captions are all caps and so on. If you can change the Font size in the reader (bigger, smaller) it&#8217;s an ePub. ePub books don&#8217;t have a fixed number of pages (it depends on the Font size), images interrupt the text flow, forget about page layout: if you&#8217;re old enough to remember what HTML looked like in the early times of the Internet you know modern ePub. But you get page whirls! 🙂\n  \n\n  \n    Nonetheless many lavishly illustrated books end up as ePubs (I&#8217;ve bought Jeff Schewe&#8217;s &#8220;The Digital Negative&#8221; for instance), the difference between ePub Vs. traditional PDF is&#8230; huge to put it mildly &#8211; I can say it because Peachpit press lets you download both formats.\n  \n\n  \n    \n  \n\n  \n    If you want to inspect an .ePub file, rename it as .zip and unzip it (for some reason OSX default Archive Utility doesn&#8217;t work &#8211; you can use the free Stuffit Expander). You&#8217;ll end up with a folders&#8217; hierarchy with a bunch stuff with, among the rest, many HTML file, JPG images and CSS.\n  \n\n  \n    \n  \n\n  \n    If it fits your needs, you can output an ePub with, for instance, Adobe InDesign, Apple Pages or even Microsoft Word &#8211; the first one being maybe a bit oversized for the job, but you may happen to already have some content in its format ready to be converted. I&#8217;ve been told from a friend of mine that runs a publishing firm in the UK (Hersilia Press) that some &#8220;post production&#8221; work (either in the HTML or CSS) is needed on the final ePub to fix small formatting issues. DRM aside, you can read an ePub that is not protected (i.e. while you&#8217;re composing it) on your PC/Mac with Adobe Digital Editions.\n  \n\n  \n    iBooks and iBooksAuthor\n  \n\n  \n    It is a proprietary format made by Apple to overcome ePub limited support of multimedia and customized layout. That is, in iBooks you can set the page layout inasmuch as you like, manage interactive content with widgets (Gallery, Movies, Interactive Images, 3D objects, etc) or write your own widget in HTML + JS. The final result on the iPad looks very like what you&#8217;d expect a real page to be, plus rich and interactive content.\n  \n\n  \n    Under the hood iBooks are very like ePub files with proprietary tags (yes, you can rename as .zip and unzip them to inspect&#8217;em: HTML is there as well), but they give you amazing flexibility compared to ePub.\n  \n\n  \n    \n  \n\n  \n    The sole choice to build iBooks is the free Apple&#8217;s iBooksAuthor (that I&#8217;ve personally covered here and here) which is a Mac only software. Speaking of which, it is very much following the style of other Apple releases such as Pages; it&#8217;s still a pretty young piece of software (especially compared to the ubiquitous InDesign) and there&#8217;s room for improvement. But it actually does what it promises, and there are several examples of spectacular books built with it in the Store.\n  \n\n  \n    Many InDesign professionals look down on iBooksAuthor as if it were nothing but a toy; some others say that if a toy is able to fit the needs of a large number of users that find InDesign oversized for their job (in terms of price and learning curve) and let them get where they want to get, toys are wonderful things.\n  \n\n  \n    Adobe Digital Publishing Suite\n  \n\n  \n    This is the new player in the arena, that Adobe threw in with a lot of (deserved) expectations. Basically you work with InDesign to compose your content, provided you&#8217;ve set the project accordingly (which allows you to use all kinds of interactivity options and support for multimedia); then you gather all your content in a .folio file that eventually DPS App Builder packs into an iPad app. Being the result an app and not a book, rather than to the iBookStore you submit it to the App Store (as you&#8217;d do for, say, a game). This bypasses completely the ePub/iBooks dilemma and makes you deliver high quality products in a straightforward way right to the iPad starting from a well known and established software such as InDesign.\n  \n\n  \n    Many if not all magazines (just to mention one class of top-users) have embraced DPS, also because it integrates perfectly with their existing workflow, and allows them to produce content once and deploy it almost effortlessly to different platforms (print, web, mobile).\n  \n\n  \n     You need to enroll in Apple&#8217;s iOS Developer Program (some extra $99), learn to manage few kinds of certificates, and you&#8217;re better off subscribing to the Adobe Creative Cloud (all the Adobe applications for an annual fee of $600 &#8211; rather than buying single DPS licenses), that is for sure the cheaper way to access DPS and publish your content as a freelance digital book author (more info about Single Edition here). I should also mention that DPS App Builder is Mac only, and you&#8217;re allowed to target also Android and Kindle if you can afford a $500 per month subscription Adobe asks for the DPS Pro edition.\n  \n\n  \n    The process of app building is performed remotely (that is, you upload your Folio to the Adobe servers); if you&#8217;ve some programming background, it&#8217;s just like writing your own code and send it remotely for compilation. DPS returns you with a compiled app, that, as far as I know, you can&#8217;t inspect any way.\n  \n\n  \n    \n  \n\n  \n    Apps can&#8217;t be books in disguise!\n  \n\n  \n    Whether it&#8217;s a protectionist effort or not we can&#8217;t say, but the fact is that Apple policies allows them to reject apps that&#8230; aren&#8217;t &#8220;app-like enough&#8221;! That is, if your DPS made stuff looks too much like a regular digital book, well, Apple rejects it because you should have been building it as an actual digital book (either ePub or iBooks), and submit it to the iBookStore &#8211; not to the App Store!\n  \n\n  \n    You could be surprised at first, but conceptually it makes sense; and after all it&#8217;s their stores and they set up the policies they like the most. Yet it is still unclear what level of&#8230; extra interactivity makes a book eligible to be packed into an app (for sure you don&#8217;t want to reshape your book into a circus just to be able to use a cool tool such as the DPS to get to the iPad).\n  \n\n  \n    Conclusions\n  \n\n  \n    We&#8217;re in the early stages of (another) digital revolution &#8211; things are pretty liquid at the time, and Standards are evolving. Yet, provided that we choose to sell our digital books through Apple (either iBookStore or AppStore), the available options drop to a very small number, and in my personal opinion there&#8217;s still a lot of room for improvement &#8211; for us as freelance publishers. The two big players in the field are of course Adobe and Apple, but their interests overlap in some ways (and this ends up as not being rather beneficial for us).\n  \n\n  \n    Both provide users with softwares: while Apple ones are quite cheap (Pages) or even free (iBooksAuthor) because it&#8217;s not there that they mainly look for profits, Adobe&#8217;s core business is content production tools, and such tools are priced accordingly &#8211; as it is fair to expect. But is it fair to directly compare InDesign/DPS and iBooksAuthor? Not so, in my opinion, since they target two very distinct audiences: chances are that InDesign (a well established and mature piece of software) is going to be used by those who already knows it and/or use it in their production environment &#8211; coupled with the new great possibilities that DPS provides. While iBooksAuthor follows Apple&#8217;s strategy and is willing to be a simpler tool &#8220;for the rest of us&#8221;. Do you need DPS extra features, may be the question to ask yourself, or more appropriately: &#8220;Can you afford the price and learning curve of Adobe&#8217;s platform?&#8221;, and &#8220;Does the final product of the DPS (an App to be sold in the AppStore, rather than a digital book to be sold in the iBookStore) fit your project?&#8221; Apple&#8217;s app rejection because they basically can&#8217;t be books in disguise (and this happens) may become a problem.\n  \n\n  \n    My personal impression is that, as users, we&#8217;re given tools and options (targeting Apple digital stores) that leave a big, empty gap:\n  \n\n  \n    \n      ePub is a relatively &#8220;cheap&#8221; format and suits projects only up to a graphically very very basic level (no true multimedia support, no text layout but very essential one, among the rest). I&#8217;d say &#8220;text only&#8221;, but is widely used for complex books as well &#8211; with disgusting results, must I say. You can build them with pretty much anything &#8211; even if manual tweaking is needed afterwards.\n    \n    \n      iBooks made with iBooksAuthor should cover a wide spectrum of needs: you&#8217;ve got interactivity, fancy and customized layouts, all kind of media supports, you can even code your own widgets. Yet the software (although promising) is pretty young and somehow limited (in the book structure for instance): but it&#8217;s free and easy to learn &#8211; compared to InDesign, it&#8217;s a piece of cake. It delivers what it promises &#8211; but you&#8217;re within an Apple proprietary, non-standard format, you can&#8217;t use anything else but iBooksAuthor, which is a Mac only application.\n    \n    \n      Adobe InDesign coupled with Digital Publishing Suite (DPS) lays down the biggest bet: bypassing ebooks format altogether (you&#8217;re building Apps) the possibilities are endless: rich content, high-end interactivity, and above all it fits publishers&#8217; existing workflow letting them deploy the same content to different media (print, web, mobile). Such bonanza comes with a price, both in terms of money (Adobe Creative Cloud and Apple Developer program subscription fees) and time (learning curve: although Adobe markets it as a relatively easy system, there&#8217;s nothing worse that final products built with complex softwares used by unexperienced operators). Plus, there the Apple variable: if your App looks too much like a book, Apple rejects it because as we&#8217;ve seen apps can&#8217;t be books in disguise.\n    \n  \n\n  \n    The picture as I&#8217;ve drawn it (simplified, maybe) describes two solid alternatives (ePub and Apps), being the former maybe&#8230; too much low-end, and the latter maybe too much high-end in my humble opinion. ePub is frankly unsuited for anything but text only unless you&#8217;re fatally inclined to ugliness, and Apps are overkill for just books &#8211; unless you&#8217;re National Geographic or a similar big publishing firm. The middle way (iBooks) should be the right one for the rest of us (with all its Pro &amp; Cons), and we definitely should be the majority. Yet, as it happens in business and especially with Apple, it really depends on how much a platform/system is widespread, used and well established. Digital history is plenty of promising attempts that have been pruned because they couldn&#8217;t manage to get where they were targeted to.\n  \n\n",
      tags: ["Adobe","app","App Store","Apple","Digital Publishing Suite","ePub","iBooks","iBooksAuthor","iBookStore","InDesign"],
      id: 89
    });
    

    index.add({
      title: "CS Extensions: Photoshop CS6 Extensions and Scripts website",
      category: ["Coding","Extensions and Scripts","Photoshop"],
      content: "\n  \n    \n      \n      Are you looking for Photoshop Extensions? I&#8217;m pleased to announce that\n      CS Extensions, a brand new website entirely dedicated to Adobe Photoshop CS6 script and panels, is finally online (a project that kept me quite busy in the last two months).\n    \n\n    \n      \n\n       CS Extensions collects both paid and free 5 star extensions that my talented friend Giuly &#8220;Cromaline&#8221; Abbiati and I have been building recently. Few highlights:&lt;/p&gt;\n\n      \n        \n          Sneak peek of Double USM, my new Photoshop extension about advanced sharpening techniques.\n        \n        \n          Adobe Exchange ready (the new in-app, app-store for Creative Suite Extensions).\n        \n        \n          Plenty of learning resources links and products specification.\n        \n        \n          Updated news about the Extensions ecosystem (releases, updates, platform bugs, etc.)\n        \n      \n\n      \n        &lt;/span&gt; Have a look at CS Extensions&lt;/a&gt; to find out more!&lt;/p&gt;\n\n        \n          I&#8217;m still tweaking it so there&#8217;s room for improvement. Leave a feedback there if you&#8217;re willing to make your voice heard! &lt;/div&gt; &lt;/div&gt; \n          \n\n",
      tags: ["Adobe Exchange","extensions","photographers","Photoshop CS6","plugins","retouchers"],
      id: 90
    });
    

    index.add({
      title: "Capture One 7 DB &#8211; Lens Correction bug",
      category: ["Photography Post-Production"],
      content: "\n  \n    The recently released Capture One Pro 7.0 software (from PhaseOne) shows a bug when dealing with Lens Correction &#8211; particularly when you couple Sharpness Falloff with Distortion Correction only when the Process engine is the latest, version 7 (at least with raws coming from P1 digital backs &#8211; I&#8217;ve tested this on IQ180).\n  \n\n  \n    This has been confirmed by the (incredibly fast, kudos to them!) Phase One tech support, so be aware of it. You need to inspect your images (the exported TIFF) looking for artifacts such as the following white stripes that appear near the corners, up to a third of the image&#8217;s width.\n  \n\n  \n    The issue may or may not shows even in the Capture One preview, depending on the OpenCL settings and your video card (but who cares, the important thing is that the exported files are affected).\n  \n\n  \n    Waiting for a bugfix update!\n  \n\n  \n    \n  \n\n",
      tags: ["bug","Capture One","issue","Lens Correction","Phase One"],
      id: 91
    });
    

    index.add({
      title: "Action recordable scripts in Photoshop",
      category: ["Coding","ExtendScript / Javascript"],
      content: "\n  \n    Scripts, by default, leave traces in the History palette; a common practice is to pack and hide into a labeled, single history step some code (either few commands or entire scripts) this way:\n  \n\n  var lotOfCode = function() {\n  // ... perform needed tasks\n}\n\n// suspendHistory()\n// String: Label that will appear in the History panel\n// String: commands to be executed\napp.activeDocument.suspendHistory(\"Secret stuff happening...\", \"lotOfCode();\" );\n\n  \n    You can record such a script into an action (using the File &#8211; Scripts &#8211; Browse&#8230; menu item), and play it later.\n  \n\n  \n    Things get more complicate when your script requires some kind of user interaction, say a GUI that asks for an input value. Take the following as an example (this will be the starting point that I&#8217;ll be elaborating throughout this post):\n  \n\n  // GBlur.jsx\n\n#target photoshop\nvar Globals, ParamHolder, RecordableGB, rGB;\n\n// store useful values here\nGlobals = {\n  defaultRadius: 10,\n  stringTitle: \"Recordable Gaussian Blur\",\n};\n\n// main object\nRecordableGB = function() {\n  var actualRoutine, win;\n  win = null;\n  this.CreateDialog = function() {\n    // String containing info needed to build the ScriptUI window\n    windowResource = \"dialog { \\\n\t\t\tvaluePanel: Panel { \\\n\t\t\t\torientation: 'column', \\\n\t\t\t\talignChildren: ['fill', 'top'], \\\n\t\t\t\ttext: 'Radius', \\\n\t\t\t\tvalueText: EditText {}, \\\n\t\t\t\tbuttonsGroup: Group { \\\n\t\t\t\t\tcancelButton: Button {text: 'cancel', properties: {name: 'cancel'}}, \\\n\t\t\t\t\tokButton: Button {text: 'Ok', properties: {name: 'ok'}} \\\n\t\t\t\t} \\\n\t\t\t} \\\n\t\t}\"\n    win = new Window(windowResource);\n    // further formatting\n    win.text = Globals.stringTitle;\n    win.valuePanel.valueText.active = true;\n    // button callbacks\n    win.valuePanel.buttonsGroup.cancelButton.onClick = function() {\n      return win.close();\n    };\n    win.valuePanel.buttonsGroup.okButton.onClick = function() {\n      actualRoutine(Number(win.valuePanel.valueText.text));\n      win.close();\n    };\n  };\n  this.runDialog = function() {\n    app.bringToFront();\n    win.center();\n    win.show();\n  };\n  // routine's core - just a Gaussian Blur as an example\n  actualRoutine = function(radius) {\n    return app.activeDocument.activeLayer.applyGaussianBlur(radius);\n  };\n};\n\n// main program\nrGB = new RecordableGB();\nrGB.CreateDialog();\nrGB.runDialog();\n\n  \n    It doesn&#8217;t do anything exciting &#8211; it just pops up a ScriptUI modal window asking for a value that will be used as a radius in pixels for a Gaussian Blur filter.\n  \n\n  \n    But&#8230; when you try to embed this into an action (say you&#8217;ve input 32 as the value while recording it), the action won&#8217;t remember the used value. Instead, it will keep popping up the GUI asking for a number, each time you play it. Not exactly what you wanted.\n  \n\n  \n    In order to build an &#8220;action-aware&#8221;, user-input required, recordable script you need to implement a specific logic, that also requires a set of specific instructions: I&#8217;ll be using this very code for the GUI, adding and commenting the bits of code required to make it fully working into Actions.\n  \n\n  \n    Javascript Resource\n  \n\n  \n    Section 3 of the Photoshop CS6 Javascript Reference deals with the Javascript Resource, a set of instruction that includes, among the rest:\n  \n\n  \n    \n      the ability to specify a menu the script appears in as a command [that is, a new item in the Filter / Automate / Help menu]\n    \n    \n      a terminology resource so the script can function with the Action Manager, which allows your script to record and be automated by scripting parameters [that is: Actions];\n    \n  \n\n  \n    Third feature should be the possibility to group commands within menus, but it seems to be buggy. I won&#8217;t duplicate the Reference here, let me just report a comment from the Adobe&#8217;s Fit Image.jsx script:\n  \n\n  // Special properties for a JavaScript to enable it to behave like an automation plug-in,\n// the variable name must be exactly as the following example and the variables\n// must be defined in the top 1000 characters of the file\n\n  \n    That said, the needed code for my example is as follows:\n  \n\n  /*\n@@@BUILDINFO@@@ RecordableGBlur.jsx 1.0\n*/\n/*\n// BEGIN__HARVEST_EXCEPTION_ZSTRING\n&lt;javascriptresource&gt;\n&lt;name&gt;$$$/RBG/recordableGBlur=Recordable Gaussian Blur...&lt;/name&gt;\n&lt;menu&gt;filter&lt;/menu&gt;\n&lt;enableinfo&gt;true&lt;/enableinfo&gt;\n&lt;eventid&gt;07d2f0b1-653d-11e0-ae3e-0800200c9a66&lt;/eventid&gt;\n&lt;terminology&gt;&lt;![CDATA[&lt;&lt;  /Version 1\n                          /Events &lt;&lt;\n                            /07d2f0b1-653d-11e0-ae3e-0800200c9a66 [($$$/RBG/recordableGBlur=Recordable Gaussian Blur)\n                            /imageReference &lt;&lt;\n                              /radius [($$$/Actions/Key/GaussianBlur/Radius=Radius) /uint]\n                            &gt;&gt;]\n                          &gt;&gt;\n                      &gt;&gt; ]]&gt;&lt;/terminology&gt;\n&lt;/javascriptresource&gt;\n// END__HARVEST_EXCEPTION_ZSTRING\n*/\n\n  \n    Things to notice (besides the fact that everything is enclosed in comment marks):\n  \n\n  \n    [Line 7] The Label as it will appear in the menu ($$$ should refer to localized strings &#8211; even though if no correspondence is found, it keeps using what&#8217;s provided; RBG is a custom string, add your own initial for instance) [Line 8] The menu the script will appear listed into (Filter in the example) [Line 9] A conditional that is evaluated in order to determine when to list the script in the menu (for instance only if the document is CMYK, etc) &#8211; in the example set to true, i.e. always. [Line 10] A unique string that is associated to your script in the Photoshop registry. You can throw in whatever you like, but in order to be sure it won&#8217;t be duplicated, a UUID is recommended &#8211; find a UUID generator here. [Line 13] The event as it will be listed in the recorded Action, with its associated string/UUID. [Line 15] The parameter(s) that will be listed within, and used in, the recorded Action. In the example the only parameter is the radius &#8211; which happens to be included in the localized strings set (otherwise I would have used a customized string as in line 7 that will not be translated).\n  \n\n  \n    Basically, the code says: please, add this script as a new item in the Filter&#8217;s menu and get ready to use the provided labels as the script and parameter&#8217;s name within an Action. But it&#8217;s not enough, we&#8217;re just at the beginning.\n  \n\n  \n     Script&#8217;s logic\n  \n\n  \n    In meta-language the main routine has to:\n  \n\n  /*\nIf the script is called from the menu\n    pop-up the dialog\n    read the param values from the GUI\n    run the routine\nelse\n    read the param values from the Action\n    run the routine\nif the routine is done\n    save the parameters\n*/\n\n  \n    That is, the GUI doesn&#8217;t pop-up when the script is called by an action &#8211; it&#8217;s the action itself that has to pass the parameters (actually, you can make an action display the dialogs, toggling the second column of icons in the Actions panel). This translates into actual code as follows:\n  \n\n  Globals = {\n  defaultRadius: 10,\n  stringTitle: \"Recordable Gaussian Blur\",\n  isCancelled = true,\n};\n\nvar ParamHolder = function() {\n  this.radius = undefined;\n};\nparamHolder = new ParamHolder();\n\nrGB = new RecordableGB();\n\nif (app.playbackDisplayDialogs === DialogModes.ALL) {\n  rGB.CreateDialog();\n  rGB.runDialog();\n} else {\n  rGB.initParameters();\n  rGB.actualRoutine(paramHolder);\n}\n\nif (!Globals.isCancelled) {\n  rGB.saveOffParameters(paramHolder);\n}\n\n  \n    The highlighted lines are new, compared to the starting script.\n  \n\n  \n    \n      In the Globals object I define a new isCancelled key &#8211; which is a toggle that&#8217;s switched to false when the routine is finished; otherwise I assume that the script isn&#8217;t done yet.\n    \n    \n      I instantiate a paramHolder object, which contains a radius property that I&#8217;ll be using to store the value from the GUI. Of course in this example it is just a single parameter, but they can be as many as you need.\n    \n    \n      I&#8217;m checking the app.playbackDisplayDialog property: it controls whether to display the dialog (and what kind of dialogs are allowed) when the script is called by an action. If it&#8217;s equal to DialogModes.ALL it means that either the Action is set to play with all the dialogs on, or the call comes from the menu item; as a result, the GUI must pop up.\n    \n    \n      The .initParameters() function retrieves the parameter from the Action and assign it to the paramHolder object &#8211; which is what the .actualRoutine() is then fed with.\n    \n    \n      When the .actualRoutine() is done, it switches to false the isCancelled key, so that the script saves the paramHolder object (the radius&#8217; container, in the example). If the Action is recording, the parameter is saved inside the Action.\n    \n  \n\n  \n    Write the Parameter from the Action\n  \n\n  \n    The script communicates with Actions by means of Action Descriptors, special objects that store key-value pairs, used to drive Photoshop at a lower level: for instance when DOM commands aren&#8217;t specified (the Action terms doesn&#8217;t refer to recordable/playable actions, afaik). The so-called Action Manager code is kind of the Photoshop&#8217;s voodoo equivalent &#8211; yet when you start to scratch the surface of such an unfriendly mechanism, it&#8217;s less scaring than it appears.\n  \n\n  \n    Anyway: the key point is to pass a descriptor to app.playbackParameters (which is what is going to be stored within the Action itself in order to be played later). This is done in Adobe&#8217;s own scripts by means of the objectToDescriptor() function:\n  \n\n  RecordableGB = function() {\n// ...\n// ... same code as before\n\n  this.saveOffParameters = function(paramObject) {\n    var desc;\n    desc = objectToDescriptor(paramObject, Globals.stringMessage);\n    app.playbackParameters = desc;\n  };\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// Function: objectToDescriptor\n// Usage: create an ActionDescriptor from a JavaScript Object\n// Input: JavaScript Object (o)\n//        object unique string (s)\n//        Pre process converter (f)\n// Return: ActionDescriptor\n// NOTE: Only boolean, string, number and UnitValue are supported, use a pre processor\n//       to convert (f) other types to one of these forms.\n// REUSE: This routine is used in other scripts. Please update those if you\n//        modify. I am not using include or eval statements as I want these\n//        scripts self contained.\n///////////////////////////////////////////////////////////////////////////////\nfunction objectToDescriptor (o, s, f) {\n   if (undefined != f) {\n      o = f(o);\n   }\n\n   var d = new ActionDescriptor;\n   var l = o.reflect.properties.length;\n   d.putString( app.charIDToTypeID( 'Msge' ), s );\n   for (var i = 0; i &lt; l; i++ ) {\n      var k = o.reflect.properties[i].toString();\n      if (k == \"__proto__\" || k == \"__count__\" || k == \"__class__\" || k == \"reflect\")\n         continue;\n      var v = o[ k ];\n      k = app.stringIDToTypeID(k);\n      switch ( typeof(v) ) {\n         case \"boolean\":\n            d.putBoolean(k, v);\n            break;\n         case \"string\":\n            d.putString(k, v);\n            break;\n         case \"number\":\n            d.putDouble(k, v);\n            break;\n         default:\n         {\n            if ( v instanceof UnitValue ) {\n               var uc = new Object;\n               uc[\"px\"] = charIDToTypeID(\"#Pxl\"); // pixelsUnit\n               uc[\"%\"] = charIDToTypeID(\"#Prc\"); // unitPercent\n               d.putUnitDouble(k, uc[v.type], v.value);\n            } else {\n               throw( new Error(\"Unsupported type in objectToDescriptor \" + typeof(v) ) );\n            }\n         }\n      }\n   }\n    return d;\n}\n\n  \n    As you see from the commented code, you feed the objectToDescriptor() function with an object to be converted (in the example: paramHolder) and a String (in the example, coming from an updated version of the Globals object, &#8220;Recordable Gaussian Blur Action Settings&#8221;), which looks to me as a key that you&#8217;ll be using to retrieve the object back from the descriptor in the next step.\n  \n\n  \n    Read the Parameter from the Action\n  \n\n  \n    I&#8217;ll make use of another Adobe made function, descriptorToObject(), to extract paramHolder from the stored descriptor (which is represented by the app.playbackParameters):\n  \n\n  RecordableGB = function() {\n// ...\n// ... same code as before\n\n  this.initParameters = function() {\n    var isFromAction;\n    isFromAction = !!app.playbackParameters.count; // boolean casting\n    if (isFromAction) {\n      descriptorToObject(paramHolder, app.playbackParameters, Globals.stringMessage);\n    }\n  }\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// Function: descriptorToObject\n// Usage: update a JavaScript Object from an ActionDescriptor\n// Input: JavaScript Object (o), current object to update (output)\n//        Photoshop ActionDescriptor (d), descriptor to pull new params for object from\n//        object unique string (s)\n//        JavaScript Function (f), post process converter utility to convert\n// Return: Nothing, update is applied to passed in JavaScript Object (o)\n// NOTE: Only boolean, string, number and UnitValue are supported, use a post processor\n//       to convert (f) other types to one of these forms.\n// REUSE: This routine is used in other scripts. Please update those if you\n//        modify. I am not using include or eval statements as I want these\n//        scripts self contained.\n///////////////////////////////////////////////////////////////////////////////\n\nfunction descriptorToObject (o, d, s, f) {\n   var l = d.count;\n   if (l) {\n       var keyMessage = app.charIDToTypeID( 'Msge' );\n        if ( d.hasKey(keyMessage) &amp;&amp; ( s != d.getString(keyMessage) )) return;\n   }\n   for (var i = 0; i &lt; l; i++ ) {\n      var k = d.getKey(i); // i + 1 ?\n      var t = d.getType(k);\n      strk = app.typeIDToStringID(k);\n      switch (t) {\n         case DescValueType.BOOLEANTYPE:\n            o[strk] = d.getBoolean(k);\n            break;\n         case DescValueType.STRINGTYPE:\n            o[strk] = d.getString(k);\n            break;\n         case DescValueType.DOUBLETYPE:\n            o[strk] = d.getDouble(k);\n            break;\n         case DescValueType.UNITDOUBLE:\n            {\n            var uc = new Object;\n            uc[charIDToTypeID(\"#Rlt\")] = \"px\"; // unitDistance\n            uc[charIDToTypeID(\"#Prc\")] = \"%\"; // unitPercent\n            uc[charIDToTypeID(\"#Pxl\")] = \"px\"; // unitPixels\n            var ut = d.getUnitDoubleType(k);\n            var uv = d.getUnitDoubleValue(k);\n            o[strk] = new UnitValue( uv, uc[ut] );\n            }\n            break;\n         case DescValueType.INTEGERTYPE:\n         case DescValueType.ALIASTYPE:\n         case DescValueType.CLASSTYPE:\n         case DescValueType.ENUMERATEDTYPE:\n         case DescValueType.LISTTYPE:\n         case DescValueType.OBJECTTYPE:\n         case DescValueType.RAWTYPE:\n         case DescValueType.REFERENCETYPE:\n         default:\n            throw( new Error(\"Unsupported type in descriptorToObject \" + t ) );\n      }\n   }\n   if (undefined != f) {\n      o = f(o);\n   }\n}\n\n  \n    descriptorToObject() works in a peculiar way, since it takes as a parameter an instance of the object (paramHolder) that he&#8217;s going to update &#8211; with the information extracted from the other parameter (the actual descriptor).\n  \n\n  \n    You see that, in the initParameters() function I define an isFromAction boolean, casting the app.playbackParameters.count property to Boolean. If it&#8217;s zero (i.e. false &#8211; the descriptor has no keys) the script isn&#8217;t run from an Action. In this very case, the count is equal to two, and if you want to know what the keys look like you can:\n  \n\n  var descParameters = app.playbackParameters.count;\nfor (var i = 0; i&lt; descParameters; i++) {\n  alert(\"Key[\" + i + \"]: \" + app.typeIDToStringID(app.playbackParameters.getKey(i)) + \"; \")\n}\n\n  \n    Mind you, the javascriptresource is used in the process to determine the label that will appear in the Actions palette &#8211; so they must match.\n  \n\n  \n    Final script\n  \n\n  \n    The completed example&#8217;s code is as follows (please find a note just below it regarding line 99):\n  \n\n  // Written by Davide Barranca based on Adobe's FitImage\n/*\n@@@BUILDINFO@@@ RecordableGBlur.jsx 1.0\n*/\n/*\n// BEGIN__HARVEST_EXCEPTION_ZSTRING\n&lt;javascriptresource&gt;\n&lt;name&gt;$$$/RBG/recordableGBlur=Recordable Gaussian Blur...&lt;/name&gt;\n&lt;menu&gt;filter&lt;/menu&gt;\n&lt;enableinfo&gt;true&lt;/enableinfo&gt;\n&lt;eventid&gt;07d2f0b1-653d-11e0-ae3e-0800200c9a66&lt;/eventid&gt;\n&lt;terminology&gt;&lt;![CDATA[&lt;&lt;  /Version 1\n                          /Events &lt;&lt;\n                            /07d2f0b1-653d-11e0-ae3e-0800200c9a66 [($$$/RBG/recordableGBlur=Recordable Gaussian Blur)\n                            /imageReference &lt;&lt;\n                              /radius [($$$/Actions/Key/GaussianBlur/Radius=Radius) /uint]\n                            &gt;&gt;]\n                          &gt;&gt;\n                      &gt;&gt; ]]&gt;&lt;/terminology&gt;\n&lt;/javascriptresource&gt;\n// END__HARVEST_EXCEPTION_ZSTRING\n*/;\n\n#target photoshop\nvar Globals, ParamHolder, RecordableGB, paramHolder, rGB;\n\nParamHolder = function() {\n  return this.radius = Globals.defaultRadius;\n};\n\nRecordableGB = function() {\n  var actualRoutine, win;\n  win = null;\n  this.actualRoutine = function(param) {\n    app.activeDocument.suspendHistory(\"Recordable Gaussian Blur\", \"app.activeDocument.activeLayer.applyGaussianBlur(param.radius)\");\n    Globals.isCancelled = false;\n    return false;\n  };\n  actualRoutine = this.actualRoutine;\n  this.CreateDialog = function() {\n    var windowResource;\n    windowResource = \"dialog {\t\t\tvaluePanel: Panel {\t\t\t\torientation: 'column',\t\t\t\talignChildren: ['fill', 'top'],\t\t\t\ttext: 'Radius',\t\t\t\tvalueText: EditText {},\t\t\t\tbuttonsGroup: Group {\t\t\t\t\tcancelButton: Button {text: 'cancel', properties: {name: 'cancel'}},\t\t\t\t\tokButton: Button {text: 'Ok', properties: {name: 'ok'}}\t\t\t\t}\t\t\t}\t\t}\";\n    win = new Window(windowResource);\n    win.text = Globals.stringTitle;\n    win.valuePanel.valueText.active = true;\n    win.valuePanel.buttonsGroup.cancelButton.onClick = function() {\n      return win.close();\n    };\n    win.valuePanel.buttonsGroup.okButton.onClick = function() {\n      paramHolder.radius = Number(win.valuePanel.valueText.text);\n      actualRoutine(paramHolder);\n      win.close();\n    };\n  };\n  this.runDialog = function() {\n    app.bringToFront();\n    win.center();\n    win.show();\n  };\n  this.initParameters = function() {\n    var isFromAction;\n    isFromAction = !!app.playbackParameters.count;\n    if (isFromAction) {\n      descriptorToObject(paramHolder, app.playbackParameters, Globals.stringMessage);\n    }\n  };\n  this.saveOffParameters = function(paramObject) {\n    var desc;\n    desc = objectToDescriptor(paramObject, Globals.stringMessage);\n    app.playbackParameters = desc;\n  };\n};\n\nGlobals = {\n  scriptUUID: \"07d2f0b1-653d-11e0-ae3e-0800200c9a66\",\n  defaultRadius: 30,\n  stringRadius: localize(\"$$$/Actions/Key/GaussianBlur/Radius=Radius\"),\n  stringTitle: \"Recordable Gaussian Blur\",\n  stringMessage: \"Recordable Gaussian Blur Action Settings\",\n  isCancelled: true\n};\n\nparamHolder = new ParamHolder();\n\nrGB = new RecordableGB();\n\nif (app.playbackDisplayDialogs === DialogModes.ALL) {\n  rGB.CreateDialog();\n  rGB.runDialog();\n} else {\n  rGB.initParameters();\n  rGB.actualRoutine(paramHolder);\n}\n\nif (!Globals.isCancelled) {\n  rGB.saveOffParameters(paramHolder);\n}\n\nisCancelled ? 'cancel' : undefined;\n\n///////////////////////////////////////////////////////////////////////////////\n// Function: objectToDescriptor\n// Usage: create an ActionDescriptor from a JavaScript Object\n// Input: JavaScript Object (o)\n//        object unique string (s)\n//        Pre process converter (f)\n// Return: ActionDescriptor\n// NOTE: Only boolean, string, number and UnitValue are supported, use a pre processor\n//       to convert (f) other types to one of these forms.\n// REUSE: This routine is used in other scripts. Please update those if you\n//        modify. I am not using include or eval statements as I want these\n//        scripts self contained.\n///////////////////////////////////////////////////////////////////////////////\nfunction objectToDescriptor (o, s, f) {\n   if (undefined != f) {\n      o = f(o);\n   }\n\n   var d = new ActionDescriptor;\n   var l = o.reflect.properties.length;\n   d.putString( app.charIDToTypeID( 'Msge' ), s );\n   for (var i = 0; i &lt; l; i++ ) {\n      var k = o.reflect.properties[i].toString();\n      if (k == \"__proto__\" || k == \"__count__\" || k == \"__class__\" || k == \"reflect\")\n         continue;\n      var v = o[ k ];\n      k = app.stringIDToTypeID(k);\n      switch ( typeof(v) ) {\n         case \"boolean\":\n            d.putBoolean(k, v);\n            break;\n         case \"string\":\n            d.putString(k, v);\n            break;\n         case \"number\":\n            d.putDouble(k, v);\n            break;\n         default:\n         {\n            if ( v instanceof UnitValue ) {\n               var uc = new Object;\n               uc[\"px\"] = charIDToTypeID(\"#Pxl\"); // pixelsUnit\n               uc[\"%\"] = charIDToTypeID(\"#Prc\"); // unitPercent\n               d.putUnitDouble(k, uc[v.type], v.value);\n            } else {\n               throw( new Error(\"Unsupported type in objectToDescriptor \" + typeof(v) ) );\n            }\n         }\n      }\n   }\n    return d;\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// Function: descriptorToObject\n// Usage: update a JavaScript Object from an ActionDescriptor\n// Input: JavaScript Object (o), current object to update (output)\n//        Photoshop ActionDescriptor (d), descriptor to pull new params for object from\n//        object unique string (s)\n//        JavaScript Function (f), post process converter utility to convert\n// Return: Nothing, update is applied to passed in JavaScript Object (o)\n// NOTE: Only boolean, string, number and UnitValue are supported, use a post processor\n//       to convert (f) other types to one of these forms.\n// REUSE: This routine is used in other scripts. Please update those if you\n//        modify. I am not using include or eval statements as I want these\n//        scripts self contained.\n///////////////////////////////////////////////////////////////////////////////\n\nfunction descriptorToObject (o, d, s, f) {\n   var l = d.count;\n   if (l) {\n       var keyMessage = app.charIDToTypeID( 'Msge' );\n        if ( d.hasKey(keyMessage) &amp;&amp; ( s != d.getString(keyMessage) )) return;\n   }\n   for (var i = 0; i &lt; l; i++ ) {\n      var k = d.getKey(i); // i + 1 ?\n      var t = d.getType(k);\n      strk = app.typeIDToStringID(k);\n      switch (t) {\n         case DescValueType.BOOLEANTYPE:\n            o[strk] = d.getBoolean(k);\n            break;\n         case DescValueType.STRINGTYPE:\n            o[strk] = d.getString(k);\n            break;\n         case DescValueType.DOUBLETYPE:\n            o[strk] = d.getDouble(k);\n            break;\n         case DescValueType.UNITDOUBLE:\n            {\n            var uc = new Object;\n            uc[charIDToTypeID(\"#Rlt\")] = \"px\"; // unitDistance\n            uc[charIDToTypeID(\"#Prc\")] = \"%\"; // unitPercent\n            uc[charIDToTypeID(\"#Pxl\")] = \"px\"; // unitPixels\n            var ut = d.getUnitDoubleType(k);\n            var uv = d.getUnitDoubleValue(k);\n            o[strk] = new UnitValue( uv, uc[ut] );\n            }\n            break;\n         case DescValueType.INTEGERTYPE:\n         case DescValueType.ALIASTYPE:\n         case DescValueType.CLASSTYPE:\n         case DescValueType.ENUMERATEDTYPE:\n         case DescValueType.LISTTYPE:\n         case DescValueType.OBJECTTYPE:\n         case DescValueType.RAWTYPE:\n         case DescValueType.REFERENCETYPE:\n         default:\n            throw( new Error(\"Unsupported type in descriptorToObject \" + t ) );\n      }\n   }\n   if (undefined != f) {\n      o = f(o);\n   }\n};\n\n  \n    Regarding line 99, the always helpful Mike Hale from Ps-scripts pointed out the following:\n  \n\n  \n    \n      it&#8217;s important that the function return the un-localized string &#8216;cancel&#8217; if there is an error so an action that is recording the script will know something went wrong and not store the step in the action. It should return undefined if everything went ok and the step should be recorded.\n    \n  \n\n  \n    Code Protection\n  \n\n  \n    Be aware that if you&#8217;re willing to protect your work via binary encoding, you must leave alone everything from the beginning to the #target photoshop (included). Basically you have to:\n  \n\n  \n    \n      Put the code (without the &lt;javascriptresource&gt;) on a temporary file, and export it (File &#8211; Export to Binary&#8230;)\n    \n    \n      Open the resulting .jsxbin and add a backslash to each line ending:\n    \n  \n\n  // From this:\n\n@JSXBIN@ES@2.0@MyBbyBnACM2jKFbyBn0AGO2jLFby2jMFn0ABJ2jMFnASzBjPBGEVzBjGCfIRBVBfG\nffnffACzChBhdDjzJjVjOjEjFjGjJjOjFjEEfVCfInnnJ2jPFnASzBjEFAEjzQiBjDjUjJjPjOiEjFj\nTjDjSjJjQjUjPjSGfntnftJ2jQFnASzBjMHBXzGjMjFjOjHjUjIIfXzKjQjSjPjQjFjSjUjJjFjTJfX\nzHjSjFjGjMjFjDjUKfVBfGnftJ2jRFnAEXzJjQjVjUiTjUjSjJjOjHLfVFfARCEXzOjDjIjBjSiJiEi\n//....\n\n// To This:\n\n@JSXBIN@ES@2.0@MyBbyBnACM2jKFbyBn0AGO2jLFby2jMFn0ABJ2jMFnASzBjPBGEVzBjGCfIRBVBfG\\\nffnffACzChBhdDjzJjVjOjEjFjGjJjOjFjEEfVCfInnnJ2jPFnASzBjEFAEjzQiBjDjUjJjPjOiEjFj\\\nTjDjSjJjQjUjPjSGfntnftJ2jQFnASzBjMHBXzGjMjFjOjHjUjIIfXzKjQjSjPjQjFjSjUjJjFjTJfX\\\nzHjSjFjGjMjFjDjUKfVBfGnftJ2jRFnAEXzJjQjVjUiTjUjSjJjOjHLfVFfARCEXzOjDjIjBjSiJiEi\\\n//... etc.\n\n  \n    \n       Copy the binary file&#8217;s content and put everything inside an eval(\"...\") function, then add before it the &lt;javascriptresource&gt;:\n    \n  \n\n  // Written by Davide Barranca based on Adobe's FitImage\n/*\n@@@BUILDINFO@@@ RecordableGBlur.jsx 1.0\n*/\n/*\n// BEGIN__HARVEST_EXCEPTION_ZSTRING\n&lt;javascriptresource&gt;\n&lt;name&gt;$$$/RBG/recordableGBlur=Recordable Gaussian Blur...&lt;/name&gt;\n&lt;menu&gt;filter&lt;/menu&gt;\n&lt;enableinfo&gt;true&lt;/enableinfo&gt;\n&lt;eventid&gt;07d2f0b1-653d-11e0-ae3e-0800200c9a66&lt;/eventid&gt;\n&lt;terminology&gt;&lt;![CDATA[&lt;&lt;  /Version 1\n                          /Events &lt;&lt;\n                            /07d2f0b1-653d-11e0-ae3e-0800200c9a66 [($$$/RBG/recordableGBlur=Recordable Gaussian Blur)\n                            /imageReference &lt;&lt;\n                              /radius [($$$/Actions/Key/GaussianBlur/Radius=Radius) /uint]\n                            &gt;&gt;]\n                          &gt;&gt;\n                      &gt;&gt; ]]&gt;&lt;/terminology&gt;\n&lt;/javascriptresource&gt;\n// END__HARVEST_EXCEPTION_ZSTRING\n*/;\n\n#target photoshop\n\neval(\"@JSXBIN@ES@2.0@MyBbyBnACM2jKFbyBn0AGO2jLFby2jMFn0ABJ2jMFnASzBjPBGEVzBjGCfIRBVBfG\\\nffnffACzChBhdDjzJjVjOjEjFjGjJjOjFjEEfVCfInnnJ2jPFnASzBjEFAEjzQiBjDjUjJjPjOiEjFj\\\nTjDjSjJjQjUjPjSGfntnftJ2jQFnASzBjMHBXzGjMjFjOjHjUjIIfXzKjQjSjPjQjFjSjUjJjFjTJfX\\\nzHjSjFjGjMjFjDjUKfVBfGnftJ2jRFnAEXzJjQjVjUiTjUjSjJjOjHLfVFfARCEXzOjDjIjBjSiJiEi\\\n// ...\n// ...\nB4J0AiAiR4K0AiAjJ4M0AiAjI4N0AiAjC4O0AiAnO4P0AiAnG4Q0AiAnJ4R0AiA2OB4S0AiA2hDB4T0\\\nAiAia4U0AiAiS4V0AiAnS4W0AiAmQ4X0AiAlf4Y0AiAiH4Z0AiAiI4ga0AiA2kQB4gb0AiAiY4gc0Ai\\\nAiX4ge0AiAiW4gf0AiAjG4hA0AiAiZ4hB0AiAiF4hD0AiAie4hC0AiAAhEARByB\");\n\n  \n    [I&#8217;d like to thank Mike Hale and Xbytor from ps-scripts.com who helped me understanding how this stuff works!]\n  \n\n  \n    &nbsp;\n  \n\n",
      tags: ["Action","GUI","recordable","script","tutorial"],
      id: 92
    });
    

    index.add({
      title: "ScriptUI &#8211; BridgeTalk persistent Window examples",
      category: ["Coding","ExtendScript / Javascript"],
      content: "\n  \n    In a previous post I&#8217;ve shown how to use either app.refresh() or the waitForRedraw() function in a loop in order to keep alive and idle a palette Window in Photoshop (since, unlike other CS apps like InDesign or the ExtendScript ToolKit, aka ESTK itself, palettes&#8217; lifespan ends when there&#8217;s no more code to execute).\n  \n\n  \n    I&#8217;ve recently found that BridgeTalk can provide a good alternative since, quoting Adobe&#8217;s Bob Stucky, it &#8220;uses a persistent engine to process BT messages&#8220;. I&#8217;ve had no luck following the forum&#8217;s snippet, so (thanks also to some code kindly made available by Kasyan Servetsky) I&#8217;ve ended up with working examples that I&#8217;m going to share here.\n  \n\n  \n    BridgeTalk basics\n  \n\n  \n    The Javascript Tools Guide (installed by default alongside the ESTK) is quite straightforward:\n  \n\n  \n    \n      The Adobe scripting environment provides an interapplication messaging framework, a way for to send and receive information and scripts from one Adobe application to another.\n    \n  \n\n  \n    The interesting part is that one app can message itself: that is, to be the sender and the receiver at the same time. The communication system includes a small set of cross-DOM function (general ones: open, print, close, etc. or application specific: photomerge, etc), but you can control an application in its native DOM language via messages deployed by means of BridgeTalk objects. A simple example is as follows:\n  \n\n  // the BridgeTalk Object\nvar bt = new BridgeTalk();\n\n// the communication target\nbt.target = \"photoshop\";\n\n// The script to be executed as a String\nvar message = \"alert('Hello Photoshop')\";\n\n// assign to the object's body the message\nbt.body = message;\n\n// send the message to the target app\nbt.send(); // an alert will pop-up in Photoshop\n\n  \n    Things can be more sophisticated, but this main concept holds always  true. I&#8217;ll provide you with few examples, following different strategies.\n  \n\n  \n    Example #1 &#8211; Strings\n  \n\n  \n    Using a window resource string to create the &#8216;palette&#8216; and the .toString() function on the object.\n  \n\n  #target photoshop\nfunction WinObject() {\n  // Long resource String for 'palette' Window\n  var windowResource = \"palette {          orientation: 'column',         alignChildren: ['fill', 'top'],          preferredSize:[300, 130],         text: 'ScriptUI Window - palette',          margins:15,                 sliderPanel: Panel {             orientation: 'row',             alignChildren: 'right',             margins:15,             text: ' PANEL ',             st: StaticText { text: 'Value:' },             sl: Slider { minvalue: 1, maxvalue: 100, value: 30, size:[220,20] },             te: EditText { text: '30', characters: 5, justify: 'left'}             }                 bottomGroup: Group{             cd: Checkbox { text:'Checkbox value', value: true },             cancelButton: Button { text: 'Cancel', properties:{name:'cancel'}, size: [120,24], alignment:['right', 'center'] },             applyButton: Button { text: 'Apply', properties:{name:'ok'}, size: [120,24], alignment:['right', 'center'] },         }    }\";\n  var win = new Window(windowResource);\n\n  win.bottomGroup.cancelButton.onClick = function() { win.close() };\n  win.bottomGroup.applyButton.onClick = function() { win.close() };\n\n  // Show the Window\n  win.show();\n};\n\n// String message for BridgeTalk\nvar message = WinObject.toString();\n\n// construct an anonymous instance and add it to the string\nmessage += \"\\nnew WinObject();\"\n// $.writeln(message); // check it in the ESTK Console, just in case\n\nvar bt = new BridgeTalk();\nbt.target = \"photoshop\";\nbt.body = message;\nbt.send();\n\n  \n    Everything&#8217;s been packed into the WinObject &#8211; which .toString() function exactly replicate in the message string (you must add there the call to create an anonymous instance, though).\n  \n\n  \n  \n\n  \n    Example #2 &#8211; Binary Strings\n  \n\n  \n    Let&#8217;s say we&#8217;ve this usual script:\n  \n\n  function winObject() {\n  // Long resource String for 'palette' Window\n  var windowResource = \"palette {          orientation: 'column',         alignChildren: ['fill', 'top'],          preferredSize:[300, 130],         text: 'ScriptUI Window - palette',          margins:15,                 sliderPanel: Panel {             orientation: 'row',             alignChildren: 'right',             margins:15,             text: ' PANEL ',             st: StaticText { text: 'Value:' },             sl: Slider { minvalue: 1, maxvalue: 100, value: 30, size:[220,20] },             te: EditText { text: '30', characters: 5, justify: 'left'}             }                 bottomGroup: Group{             cd: Checkbox { text:'Checkbox value', value: true },             cancelButton: Button { text: 'Cancel', properties:{name:'cancel'}, size: [120,24], alignment:['right', 'center'] },             applyButton: Button { text: 'Apply', properties:{name:'ok'}, size: [120,24], alignment:['right', 'center'] },         }    }\";\n  var win = new Window(windowResource);\n  win.bottomGroup.cancelButton.onClick = function() { win.close() };\n  win.bottomGroup.applyButton.onClick = function() { win.close() };\n  win.show();\n};\nwinObject()\n\n  \n    In ESTK do File &#8211; Export as Binary in a temp file, which will look like:\n  \n\n  @JSXBIN@ES@2.0@MyBbyBnABMAbyBn0AFJCnASzOjXjJjOjEjPjXiSjFjTjPjVjSjDjFBAne2kVDjQjB\njMjFjUjUjFhAjbhAhAhAhAhAhAhAhAhAhAjPjSjJjFjOjUjBjUjJjPjOhahAhHjDjPjMjVjNjOhHhMh\nAhAhAhAhAhAhAhAhAjBjMjJjHjOiDjIjJjMjEjSjFjOhahAibhHjGjJjMjMhHhMhAhHjUjPjQhHidhM\nhAhAhAhAhAhAhAhAhAhAjQjSjFjGjFjSjSjFjEiTjJjajFhaibhThQhQhMhAhRhThQidhMhAhAhAhAh\n// ... etc. etc.\n\n  \n    Add a backslash at each line&#8217;s end:\n  \n\n  @JSXBIN@ES@2.0@MyBbyBnABMAbyBn0AFJCnASzOjXjJjOjEjPjXiSjFjTjPjVjSjDjFBAne2kVDjQjB\\\njMjFjUjUjFhAjbhAhAhAhAhAhAhAhAhAhAjPjSjJjFjOjUjBjUjJjPjOhahAhHjDjPjMjVjNjOhHhMh\\\nAhAhAhAhAhAhAhAhAjBjMjJjHjOiDjIjJjMjEjSjFjOhahAibhHjGjJjMjMhHhMhAhHjUjPjQhHidhM\\\nhAhAhAhAhAhAhAhAhAhAjQjSjFjGjFjSjSjFjEiTjJjajFhaibhThQhQhMhAhRhThQidhMhAhAhAhAh\\\n// ... etc. etc.\n\n  \n    and use it as a message String for the BridgeTalk body:\n  \n\n  var message = \"@JSXBIN@ES@2.0@MyBbyBnABMAbyBn0AFJCnASzOjXjJjOjEjPjXiSjFjTjPjVjSjDjFBAne2kVDjQjB\\\njMjFjUjUjFhAjbhAhAhAhAhAhAhAhAhAhAjPjSjJjFjOjUjBjUjJjPjOhahAhHjDjPjMjVjNjOhHhMh\\\nAhAhAhAhAhAhAhAhAjBjMjJjHjOiDjIjJjMjEjSjFjOhahAibhHjGjJjMjMhHhMhAhHjUjPjQhHidhM\\\n// ...\n// ... etc. etc. etc.\n// ...\nEnAEXzFjDjMjPjTjFHfjCfnf0DzAICEnfJFnABXEfXzLjBjQjQjMjZiCjVjUjUjPjOJfXGfVCfBNyBn\\\nAMFbyBn0ABJFnAEXHfjCfnf0DICFnfJGnAEXzEjTjIjPjXKfVCfBnfACB40BiAC4B0AiAACAzJjXjJj\\\nOiPjCjKjFjDjULAHBJInAEjLfnf0DIByB\";\n\nvar bt = new BridgeTalk();\nbt.target = \"photoshop\";\nbt.body = message;\nbt.send();\n\n  \n    This way you&#8217;re able to pack different stuff &#8211; an entire script &#8211; into the string.\n  \n\n  \n     Example #3 &#8211; External scripts\n  \n\n  \n    You can load as a string an external file (either binary or not). In the following code, both files are in the same folder:\n  \n\n  var currentPath = (new File($.fileName)).path // retrieve the current script path\nvar scriptToLoad = new File (currentPath + \"/paletteWindow.jsx\") // the script to load\ntry {\n    if (!scriptToLoad.exists) { throw new Error(\"script not found!\"); }\n    scriptToLoad.open (\"r\"); // read only\n    var message = scriptToLoad.read();\n    scriptToLoad.close()\n} catch (error) {\n    alert(\"Error parsing the file: \" + error.description);\n}\nvar bt = new BridgeTalk();\nbt.target = \"photoshop\";\nbt.body = message;\nbt.send();\n\n  \n    Caveats\n  \n\n  \n    \n      For some reason that I miss, if you don&#8217;t have any button&#8217;s callback (or at least a window.onClose() function, even an empty one) in the Window, the palette won&#8217;t display.\n    \n    \n      Be aware that window resource strings can&#8217;t contain backslashes \\ as you&#8217;d write normally (see the code below). This applies to images embedded as strings as well &#8211; they don&#8217;t return any error yet prevent the window from displaying.\n    \n  \n\n  windowResource = \"palette {  \\\n        orientation: 'column', \\\n        alignChildren: ['fill', 'top'],  \\\n        preferredSize:[300, 130], \\\n        text: 'ScriptUI Window - palette',  \\\n        margins:15, \\\n        \\\n        sliderPanel: Panel { \\\n            orientation: 'row', \\\n            alignChildren: 'right', \\\n            margins:15, \\\n            text: ' PANEL ', \\\n            st: StaticText { text: 'Value:' }, \\\n            sl: Slider { minvalue: 1, maxvalue: 100, value: 30, size:[220,20] }, \\\n            te: EditText { text: '30', characters: 5, justify: 'left'} \\\n            } \\\n        \\\n        bottomGroup: Group{ \\\n            cd: Checkbox { text:'Checkbox value', value: true }, \\\n            cancelButton: Button { text: 'Cancel', properties:{name:'cancel'}, size: [120,24], alignment:['right', 'center'] }, \\\n            applyButton: Button { text: 'Apply', properties:{name:'ok'}, size: [120,24], alignment:['right', 'center'] }, \\\n        }\\\n    }\"\n\n  \n    \n       The way you write a function results in different .toString() outputs, for instance:\n    \n  \n\n  // Function Declaration\nfunction foo()  { alert(\"foo\") }\nfoo.toString();\n// function foo()  { alert(\"foo\") }\nfoo.toSource();\n// (function foo()  { alert(\"foo\") })\n\n// Function Expression\nvar bar = function() { alert(\"bar\") }\nbar.toString();\n// function () { alert(\"bar\") }\nbar.toSource();\n// (function() { alert(\"bar\") })\n\n// Named Function\nvar baz = function baz() { alert(\"baz\") }\nbaz.toString();\n// function baz() { alert(\"baz\") }\n\n  \n    When using Function Declaration / Named function the name is correctly parsed, while in Function Expressions the function assigned to the variable is anonymous and can&#8217;t return its name (Coffeescript for instance uses Function Expressions by default):\n  \n\n  var qux = function() { alert(\"qux\") }\n$.writeln(qux.name); // anonymous\n\n  \n    As a consequence, you&#8217;ve to explicitly make a correct message:\n  \n\n  // custom function\n// the parameter is a String\nvar stringifyFunction = function (fn) {\n  var functionBody = eval(fn);\n  return fn + \" = \" + functionBody;\n}\n\n// the Function to send\nvar foo = function () { alert(\"foo\") }\n\n// BridgeTalk obj\nvar bt = new BridgeTalk();\nbt.target = \"photoshop\";\nvar message = stringifyFunction (\"foo\"); // \"foo\" as a String\nmessage += \"foo()\"; // to call it\nbt.body = message;\n// $.writeln(message)\nbt.send();\n\n  \n    Following the above pattern, things get slightly more verbose if you need prototypes.\n  \n\n  \n    Conclusions\n  \n\n  \n    The BridgeTalk generated &#8216;palette&#8216; Window is finally a true, non-modal, idle Window object in Photoshop, no matter whether PC or Mac (the workarounds I&#8217;ve shown in a previous post work only partially on PC).\n  \n\n  \n    My suggestion, if you need such a feature, is to implement in an early stage of the script coding the BridgeTalk communication structure &#8211; since in a quite complex project I&#8217;m spending my time on (which started way before I knew about this technique), I&#8217;m still unable to make it work. The debugging is&#8230; quite complicate!\n  \n\n",
      tags: ["BridgeTalk","Extendscript","palette","ScriptUI","tutorial","Window"],
      id: 93
    });
    

    index.add({
      title: "ScriptUI Window in Photoshop – Palette vs. Dialog",
      category: ["Scripting"],
      content: "ExtendScript - one of the scripting languages supported by Creative Suite applications, and the only one cross-platform - has several extra features compared to Javascript. Besides Filesystem access (a notable addition), there’s the ScriptUI component: which allows you to add graphic user interfaces (GUI) to scripts. Photoshop support is dated to CS2, according to the unofficial (yet essential) ScriptUI for Dummies guide made by Peter Kahrel. As it happens often in scripting CS applications, the implementation of some feature is version and platform dependent - i.e. PS CS5 on PC is different from PS CS6 on Mac. In this post I’m going to focus on one particular aspect of ScriptUI, namely the Window object.\n\nTwo of a kind\n\nWindows can be either dialogs or palettes (at least in Photoshop). They look and are substantially different.\n\nPalettes\n\nOpen the ExtendScript ToolKit (ESTK) and run the following code:\n\n#target estoolkit\nvar windowResource = \"palette {  \\\norientation: 'column', \\\nalignChildren: ['fill', 'top'],  \\\npreferredSize:[300, 130], \\\ntext: 'ScriptUI Window - palette',  \\\nmargins:15, \\\n\\\nsliderPanel: Panel { \\\n  orientation: 'row', \\\n  alignChildren: 'right', \\\n  margins:15, \\\n  text: ' PANEL ', \\\n  st: StaticText { text: 'Value:' }, \\\n  sl: Slider { minvalue: 1, maxvalue: 100, value: 30, size:[220,20] }, \\\n  te: EditText { text: '30', characters: 5, justify: 'left'} \\\n}, \\\n\\\nbottomGroup: Group{ \\\n  cd: Checkbox { text:'Checkbox value', value: true }, \\\n  cancelButton: Button { text: 'Cancel', properties:{name:'cancel'}, size: [120,24], alignment:['right', 'center'] }, \\\n  applyButton: Button { text: 'Apply', properties:{name:'ok'}, size: [120,24], alignment:['right', 'center'] }, \\\n}\\\n}\";\n\nvar win = new Window(windowResource);\nwin.bottomGroup.cancelButton.onClick = function() {\n  return win.close();\n};\nwin.bottomGroup.applyButton.onClick = function() {\n  return win.close();\n};\nwin.show();\n\nThe result (it runs in ESTK, since the first line is the preprocessing directive #target estoolkit) is as follows:\n\n\n\nThe above is a palette, a non-modal window. Something that lets you interact either with the window itself (clicking buttons, dragging sliders, etc) and with the host program - here the ESTK (so you can select, say, menu items, other panels, etc.). It’s the Javascript equivalent of Panels and third party Extensions - they show up and wait there, letting you work with Photoshop.\n\nDialogs\n\nIn ESTK run the following code:\n\n#target estoolkit\nvar windowResource = \"dialog {  \\\norientation: 'column', \\\nalignChildren: ['fill', 'top'],  \\\npreferredSize:[300, 130], \\\ntext: 'ScriptUI Window - dialog',  \\\nmargins:15, \\\n\\\nsliderPanel: Panel { \\\n  orientation: 'row', \\\n  alignChildren: 'right', \\\n  margins:15, \\\n  text: ' PANEL ', \\\n  st: StaticText { text: 'Value:' }, \\\n  sl: Slider { minvalue: 1, maxvalue: 100, value: 30, size:[220,20] }, \\\n  te: EditText { text: '30', characters: 5, justify: 'left'} \\\n}, \\\n\\\nbottomGroup: Group{ \\\n  cd: Checkbox { text:'Checkbox value', value: true }, \\\n  cancelButton: Button { text: 'Cancel', properties:{name:'cancel'}, size: [120,24], alignment:['right', 'center'] }, \\\n  applyButton: Button { text: 'Apply', properties:{name:'ok'}, size: [120,24], alignment:['right', 'center'] }, \\\n}\\\n}\"\n\nwin = new Window(windowResource);\n\nvar win.bottomGroup.cancelButton.onClick = function() {\n  return win.close();\n};\nwin.bottomGroup.applyButton.onClick = function() {\n  return win.close();\n};\n\nwin.show();\n\nCompared to the first script, there’s little difference (the windowResource string contains dialog instead of palette) and the Window looks like this:\n\n\n\nWe’re still within ESTK: the red, closing button is disappeared, the titlebar is bigger, but the main difference is that dialogs are modal: that is, the focus is on them and you can not interact with the host application. If you try to select, say, a menu item the program complains with a beep.\n\nPhotoshop support\n\nIn the ScriptUI for Dummies guide it’s asserted that Photoshop doesn’t support palette windows. While the more hours I spend debugging unwanted behaviors the more I tend to agree with Peter Kahrel, I would personally express the statement as follows:\n\n\n  Photoshop supports both dialog and palette Windows; while the former kind’s behavior is consistent with other CS applications, palette implementation in PS is peculiar and version/platform dependent.\n\n\nPS dialogs implementation\n\nIt’s basically what you’d expect - a modal window (which aspect depends on the PS version and platform). Yet the behavior is exactly the same no matter what is the combination, PC/Mac - CS5/CS6. In order to test it, just change the first line of the two scripts above:\n\n#target photoshop\n\n\n\t\n\tFirst row: Mac CS5, Mac CS6. Second row: PC CS5, PC CS6.\n\n\nOne thing to notice is that PC versions have a red X closing button, while Mac versions do not (for some reason, CS5 rendering of the slider bar is thicker compared to CS6).\n\nPS palettes implementation\n\nHere things get “interesting”. First, if you run the palette example with  #target photoshop you’re not going to see very much. The Window that you expect to see just flashes briefly and disappear. This is what will happen to every Snp*.jsx examples provided alongside with the ESTK application, not just my code. The answer lies in the fact that palettes keep showing only if there’s something going on in the script, they can not stay idle waiting for the user to do something like modal dialogs do.\n\n// palette same as before\nvar windowResource = \"palette { /* etc... */}\";\nvar win = new Window(windowResource);\nwin.show();\n\napp.documents.add(); // adds a new document\napp.activeDocument.activeLayer.applyAddNoise (400, NoiseDistribution.GAUSSIAN, true)\nfor (i = 0; i &lt; 3; i++) { // Blurs it 3 times\n    app.activeDocument.activeLayer.applyGaussianBlur(1);\n    $.sleep (2000); // waits 2 seconds\n    app.refresh(); // refreshes PS\n}\n\nIn the above example, after the win.show() the script is busy creating a new document, applying and blurring some noise just to kill some time. In the meantime, the palette window shows and keep showing; when the script is done with the task, the palette is automatically closed with no need of user interaction whatsoever, nor explicit win.close(). In order to keep Photoshop busy, I’ve found two working alternatives (because a straight $.sleep() loop makes PS quite non-responsive) involving the following functions:\n\n// According to the PS Javascript Reference:\n// \"Pauses the script while the application refreshes.\n// Use to slow down execution and show the results to\n// the user as the script runs.\n// Use carefully;\n// your script runs much more slowly when using this method.\"\napp.refresh();\n\n// According to one example's comment in the\n// Photoshop Javascript Reference:\n// \"A helper function for debugging\n// It also helps the user see what is going on\"\nvar waitForRedraw = function() {\n  var d;\n  d = new ActionDescriptor();\n  d.putEnumerated(app.stringIDtoTypeID('state'),\n  app.stringIDtoTypeID('state'),\n  app.stringIDtoTypeID('redrawComplete'));\n  return executeAction(app.stringIDtoTypeID('wait'), d, DialogModes.NO);\n};\n\nThe comments in the code should be self-explanatory. As far as I know, waitForRedraw() should work also in earlier PS versions - it’s not documented, it’s just used in one Reference’s demo scripts.\n\nPS palette working example\n\nOne possible way to implement working palette Windows in Photoshop has the following logic:\n\nvar isDone, s2t, waitForRedraw, win, windowResource;\n\n// Shortcut function\ns2t = function(stringID) {\n  return app.stringIDToTypeID(stringID);\n};\n\nwaitForRedraw = function() {\n  var d;\n  d = new ActionDescriptor();\n  d.putEnumerated(s2t('state'), s2t('state'), s2t('redrawComplete'));\n  return executeAction(s2t('wait'), d, DialogModes.NO);\n};\n\n//sentinel variable\nisDone = false;\n\n// palette same as before\nwindowResource = \"palette { /* etc... */}\";\n\nwin = new Window(windowResource);\n\n// Button listeners\nwin.bottomGroup.cancelButton.onClick = function() {\n  return isDone = true;\n};\nwin.bottomGroup.applyButton.onClick = function() {\n  return isDone = true;\n};\n\n// don't forget this one!\nwin.onClose = function() {\n  return isDone = true;\n};\n\nwin.show();\n\nwhile (isDone === false) {\n  app.refresh(); // or, alternatively, waitForRedraw();\n}\n\nThe key point is the isDone variable, which is set false at the beginning. After the Window is shown, a while loop keeps checking for this sentinel value and call app.refresh(), or alternatively waitForRedraw() in order to keep the palette showing. Instead of an explicit call to win.close(), it’s a better option to attach to the buttons’ onClick() functions a sentinel value’s switch to true. This way the while loop breaks and the Window automatically closes.\nMind you, it’s crucial to set isDone = true even in the win.onClose() function, otherwise Photoshop will keep evaluating the variable even after the Window has been closed clicking the red dismiss button (see following screenshot) - resulting in some degree of non-responsiveness.\n\n\n\t\n\tFirst row: Mac CS5, Mac CS6. Second row: PC CS5, PC CS6.\n\n\nFew cosmetic differences are present this time too, including the position of the dismiss button (top-left for Mac, top-right for PC). This button is taken into account with the onClose() function as shown in the code example.\n\nPlatform specific behaviors (aka bugs)\n\nThings are getting weird if you start to compare Mac and PC implementations of ScriptUI windows in Photoshop CS5 and CS6 (the tests on the PC side have been made on a virtualized Window7 system running in Parallels Desktop). It happens that, depending on how you mix platform, version and Window type, you end up with different behaviors (modal and non-modal), as follows:\n\n\n\nWhile dialog’s behavior is nicely uniform between Mac and PC, CS5 and CS6; palettes are a fiasco. Mac behavior is always non-modal (as it should be - palettes are non-modal by definition), while PC’s one depends on the Photoshop version and function used. Basically, only CS5 with app.refresh() works as it’s supposed to do, while in CS6 both functions fail. From my personal standpoint, this is a bug - and I haven’t found any workaround yet.\n\nConclusions\n\nTo sum up:\n\n\n  Palettes are defined within Creative Suite applications as non-modal Windows that can wait idle.\n  The Photoshop implementation misses entirely the “can wait idle” part - palettes keep showing only if there’s some code running in background.\n  There’s a workaround to make palettes behave as they’re supposed to do, involving either app.refresh() or waitForRedraw() loops and a sentinel variable.\n  On Mac the workaround, no matter on CS5 and CS6 or what function is used, makes the palettes non-modal (as they’re supposed to be by definition)\n  On PC, only app.refresh() in CS5 returns a non-modal Window - CS6 ones are always modal.\n\n\nWhich is quite a pain in the neck, especially if you need your palettes to work as they’re supposed to do in a platform and version independent way.\n",
      tags: ["ScriptUI"],
      id: 94
    });
    

    index.add({
      title: "CryptoJS Tutorial For Dummies",
      category: ["Scripting"],
      content: "[Updated October 16th 2012 with corrected information from CryptoJS author Jeff Mott - look for the UPDATED tag below]\nThe Google Code Crypto-JS page titles “JavaScript implementations of standard and secure cryptographic algorithms” and that’s exactly what it’s all about. I strongly suggest you to read the QuickStart Guide: this CryptoJS Tutorial for Dummies is just made with comments in the margin of it - underlining important stuff I didn’t notice when I first approached it. The official documentation is precise yet kind of succint - it makes sense to those who master the topic, but it may disorient the newcomers (like me when I get there for the first time). I’m just on my way learning both cryptography theory and this nice JS library, the following is only a collection of basic advices and personal notes. Please forgive me for using a very… non-specialized jargon.\n\n\n  Caveat: I use to write code in Extendscript - which is an Adobe made superset of Javascript adding nice extras, such as: file-system access, user interface development tools, external communication, preprocessing directives, XML integration, etc. I say so because it seems that my mindset is a bit shifted from the one of the pure Javascript developer (for instance, I’ve not to worry about privacy issues arising from readable code - my .jsx files are binary encrypted by default when I export them from the ESTK - ExtendScript ToolKit). I use to write Coffeescript, a nice smart language that compiles to Javascript, but I won’t us it here. Lot of extra resources on the Coffeescript language here.\n\n\nFiles\n\nFirst, download the CryptoJS package (3.0.2 at the time of this post). It contains two folders:\n\n\n  components - with both minified and commented JS files.\n  rollups - minified files (one for each algorithm) bundled with core code.\n\n\nComponents files have dependencies: you have to link at least core.js, while rollups are quite self contained. In Extendscript, save a test.jsx file alongside (or within) the CryptoJS folder and set the preprocessing directives:\n\n#include \"components/core-min.js\"\n#include \"rollups/sha1.js\"\n#include \"rollups/aes.js\"\n#include \"components/enc-base64-min.js\"\n#include \"components/enc-utf16-min.js\"\n\nThe #include (redundant here) are the equivalent of the following tag in HTML documents:\n\n&lt;script src=\"http://crypto-js.googlecode.com/svn/tags/3.0.2/build/rollups/aes.js\"&gt;&lt;/script&gt;\n\nBe aware that you need to save the file at least once otherwise the include can’t resolve the path.\n\nWord Array and Encodings\n\nCryptoJS makes large use of Word Array - that is, arrays of 32-bits words (instances of the CryptoJS.lib.WordArray); few useful functions:\n\n// Creates a word array filled with random bytes.\n// @param {number} nBytes The number of random bytes to generate.\nvar wordArray = CryptoJS.lib.WordArray.random(16);\n\n// Converts a String to word array\nvar words = CryptoJS.enc.Utf16.parse('Hello, World!');\n// 00480065006c006c006f002c00200057006f0072006c00640021\n\n// Reverses the word array to a readable String\nvar utf8 = CryptoJS.enc.Utf16.stringify(words); // Hello World\n\nMind you, quoting the Guide: “When you use a WordArray object in a string context”, that is, an alert box or the Console, “it’s automatically converted to a hex string”. CryptoJS can manage different encodings, such as Base64, Hex, Latin1, UTF-8 and UTF-16. If you wonder how data would look like converted to them, paste this script in ESTK (Adobe’s ExtendScriptToolKit):\n\n#include \"components/enc-base64-min.js\"\n#include \"components/enc-utf16-min.js\"\n\n// Save at least once in the appropriate folder in order to\n// help the #include to resolve the path\n\nvar wordArray = CryptoJS.lib.WordArray.random(32);\nalert(\"Random WordArray\" +\"\\\\nHex: \"+ wordArray + \"\\\\n\\\\nBase 64: \" + CryptoJS.enc.Base64.stringify(wordArray) +\"\\\\n\\\\nLatin 1: \" + CryptoJS.enc.Latin1.stringify(wordArray) + \"\\\\n\\\\nUTF-8: \" + CryptoJS.enc.Utf8.stringify(wordArray) + \"\\\\n\\\\nUTF-16: \" + CryptoJS.enc.Utf16.stringify(wordArray))\n\nThe output is as follows - I’ve used an Alert box because ESTK’s $.writeln() (as I suspect both console.log() and debug(), depending on the tool you use to run your JS code) can’t output but Hex and Base64:\n\n\n\nConversion Functions\n\nInclude both enc-base64-min.js and enc-utf16-min.js in your code. The functions are in the form:\n\nCryptoJS.enc./Encoding/.parse();\n// interprets the param as /Encoded/\t\t\t\t\t\t\t\t\t\t\t\t// and converts it to Word Array\n\nCryptoJS.enc./Encoding/.stringify();\n// interprets the param as Word Array\t\t\t\t\t\t\t\t\t\t\t\t// and converts it to the /Encoded/ String\n\nvar wordArray = CryptoJS.enc.Utf16.parse('Hello, World!'); // Word Array\nvar utf16 = CryptoJS.enc.Utf16.stringify(wordArray); // String\n\nFew examples as follows:\n\n// Encodings:\nvar words  = CryptoJS.enc.Base64.parse('SGVsbG8sIFdvcmxkIQ==');\nvar base64 = CryptoJS.enc.Base64.stringify(words);\n\nvar words = CryptoJS.enc.Latin1.parse('Hello, World!');\nvar latin1 = CryptoJS.enc.Latin1.stringify(words);\n\nvar words = CryptoJS.enc.Hex.parse('48656c6c6f2c20576f726c6421');\nvar hex = CryptoJS.enc.Hex.stringify(words);\n\nvar words = CryptoJS.enc.Utf8.parse('♦')\nvar utf8  = CryptoJS.enc.Utf8.stringify(words);\n\nvar words = CryptoJS.enc.Utf16.parse('Hello, World!');\nvar utf16 = CryptoJS.enc.Utf16.stringify(words);\n\nHash Functions\n\nSo to speak, hashers are functions that take an input (no matter how large) and maps it to a fixed size, smaller one (the hash, or checksum). You can’t convert a hash back to the original input, yet you can check if the original data has been corrupted comparing the hashes. CryptoJS implements MD5, SHA-1 (used by Git) and its variant (2, 224, 384, 256 and 512).\n\nvar hash = CryptoJS.SHA1(\"Message\");\n// c4b0858dd7f14b154cac443b659bf9def034e01f\n\nThe input \"Message\" can either be a WordArray or a String (which will automatically be converted to the former, encoded UTF-8). Then you may:\n\n// CONVERT WordArray to Latin1 or Base64 - better off using alert()!\nvar hash_Latin1 = hash.toString(CryptoJS.enc.Latin1);\nvar hash_Base64 = hash.toString(CryptoJS.enc.Base64);\n\nMind you the two forms are equivalent:\n\nvar hash_Base64 = hash.toString(CryptoJS.enc.Base64);\nvar hash_Base64 = CryptoJS.enc.Base64.stringify(hash);\n\nCiphers\n\nCryptoJS implements several Cipher Algorithms - in the following example AES:\n\n#include \"rollups/aes.js\"\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\n$.writeln(encrypted);\n// AABsAABkAABiAAAAAAAAAABNAABlAABPAAC0AABHAAA=\n\nvar decrypted = CryptoJS.AES.decrypt(encrypted, \"Secret Passphrase\");\n$.writeln(decrypted);\n// 4d657373616765\n\nThe encryption results in a Base64 string, while the decrypted string is Hex. To get back the “Message” you need to:\n\n$.writeln(decrypted.toString(CryptoJS.enc.Utf8));\n// Message\n\nIf you run the encryption code several times, you’ll notice that the result will change:\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\n$.writeln(encrypted);\n// AABsAABkAAA0AAAAAAAAAABfAADSAAC5AABcAACjAAA=\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\n$.writeln(encrypted);\n// AABsAABkAAA8AAAAAAAAAAA1AACIAAChAAAqAADMAAA=\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\n$.writeln(encrypted);\n// AABsAABkAAArAAAAAAAAAAA4AABGAAAkAACpAAB5AAA=\n\nyet each one of them will always decrypt to “Message”. How come?\n\nKey, Salt and Initialization Vector\n\nI’m not a cryptography expert - my raw understanding of the matter (after digging Wikipedia and StackOverflow) is as follows. Human memorizable passphrase are known to be bad ones. In order to make them more secure you can add a bunch of random bits (the salt) so that the actual key = function(Salt, Passphrase). An effect of Salt is that the same passphrase doesn’t always produce the same key. IV (initialization vector) is used, similarly, to ensure that the same plaintext (“Message”) doesn’t return the same ciphertext. It appears that Salt is used with passphrase to generate a key for encryption, then the resulting encryption is processed with IV. So to speak, the encryption = function(plaintext, passphrase, salt, IV). So when you write:\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\n\nCryptoJS randomly generates for you what it need. [UPDATED] Alternatively, you can specify:\n\nvar key = CryptoJS.lib.WordArray.random(16);\nvar iv  = CryptoJS.lib.WordArray.random(16);\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", key, { iv: iv });\nvar decrypted = CryptoJS.AES.decrypt(encrypted, key, { iv: iv });\n\nInput and output\n\nThe encrypt function takes a plaintext input as a String or WordArray (the “Message”), and either a similar passphrase or Hex Key and IV. [UPDATED] It’s important to reaffirm that, if you use a String as a passphrase, CryptoJS uses it to generate a random key and IV:\n\nvar key = CryptoJS.enc.Hex.stringify(CryptoJS.lib.WordArray.random(16));\nvar iv  = CryptoJS.enc.Hex.stringify(CryptoJS.lib.WordArray.random(16));\n// WRONG!!\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", key, {iv: iv});\n\nThe code above is not proper, for two reasons:\n\n\n  Key and IV are Strings, not Word Arrays!\n  Consequently, the key is used as a String passphrase from which to derive a random actual key + IV pair - so they’re not the key and iv variables you’ve declared.\n\n\nWhen it comes to the output, things are a bit different because if you:\n\nalert (typeof encrypted); // object\n\nActually, the encryption output is an object called CipherParams, and you can access its properties:\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\n\nalert(encrypted); // U2FsdGVkX1+iX5Ey7GqLND5UFUoV0b7rUJ2eEvHkYqA=\nalert(encrypted.key); // 74eb593087a982e2a6f5dded54ecd96d1fd0f3d44a58728cdcd40c55227522223\nalert(encrypted.iv); // 7781157e2629b094f0e3dd48c4d786115\nalert(encrypted.salt); // 7a25f9132ec6a8b34\nalert(encrypted.ciphertext); // 73e54154a15d1beeb509d9e12f1e462a0\n\nI’ve had some troubles understanding why they use this object as the vector of encrypted data - the pack of bits you and the other guy oversea securely swap. It looks like you’re putting your treasure map into a casket and ship it with the keys to open it (I would have used the ciphertext only). It puzzled me because you can successfully write:\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\nvar decrypted = CryptoJS.AES.decrypt(encrypted, encrypted.key, { iv: encrypted.iv});\n\n[UPDATED] The explanation came directly from CryptoJS author Jeff Moss who wrote me:\n\n\n  Although the key is a property in the CipherParams object, the key is not included when that CipherParams object is serialized to a string. By default, CipherParams objects are serialized using a format from OpenSSL. Just do encrypted.toString(), and you can safely send that to the other side of the ocean.\n\n\nSo the alert(encrypted); hex string you see in the last but one code block is definitely safe to use and share:\n\nvar encrypted = CryptoJS.AES.encrypt(\"Message\", \"Secret Passphrase\");\nvar decrypted = CryptoJS.AES.decrypt(\"U2FsdGVkX1+iX5Ey7GqLND5UFUoV0b7rUJ2eEvHkYqA=\", \"Secret Passphrase\") // or just encrypted.toString()\n\nThere’s definitely more to dig in this great CryptoJS library, I’m looking forward to explore it! This should be just enough to let you start implementing cryptography in your own projects.\n",
      tags: ["Crypto-JS"],
      id: 95
    });
    

    index.add({
      title: "Hahnemühle PhotoRag Baryta paper issues",
      category: ["Printing"],
      content: "\n  \n    \n\n    \n\n    \n    \n\n    \n    &lt;/p&gt;\n\n    \n      \n      &lt;/p&gt;\n\n      \n        \n      \n\n      \n        This is what you get right in the middle of a 64 inches (162 cm) roll of Hahnemuhle Photo Rag Baryta, one of the most Expensive Archival Cotton Rag Papers for Inkjet on the market. Now try to print a 64 x 85 inches (162 x 215 cm) image on it and guess how many square feet of paper you will be recycling in the process. That&#8217;s why we never buy sheet boxes anymore! &lt;/div&gt;\n\n        \n          Quality control is gone! What a shame. And I&#8217;m not talking about a single roll &#8211; I&#8217;ve been working with a client who bought a 30 rolls strong stock and we&#8217;ve been using more than a half so far. (I&#8217;ve been using Hahnemuhle for years and things weren&#8217;t that much different in the past).&lt;/p&gt;\n\n          \n            \n          \n\n          \n            Mind you, if such an expensive (and large sized) paper is targeted to high-level users, it should be obvious that the very same high-level users can&#8217;t afford to leave such surface imperfections in the framed piece of work. What kind of imperfections? We&#8217;ve got bumps like the one depicted above, caused by solid bits of who-knows-what embedded in the rag paper base &#8211; depending on the kind of impurity they look darker or lighter; black dots scattered here and there, a pleasure to work with when you print skies or similar large patches of flat colors; curly red hair like this one on the left, trapped between the paper base and the coating (where do they come from, I can&#8217;t say); wax-like stains half of a centimeter wide, almost invisible on the virgin paper unless you inspect it under incident light &#8211; but resulting in matte blemishes on the printed surface. &lt;/div&gt;\n\n            \n              What can you do? My personal suggestions:\n            \n\n            \n              \n                Avoid Hahnemuhle when possible, buy another paper brand. It will save you a lot of money, time, and energy otherwise spent getting upset.\n              \n              \n                Cut the imperfections out, enclose the paper box/roll sticker showing the production lot (kind of a serial number that should indicate when they&#8217;ve manufactured the paper) and mail everything to your Hahnemuhle reseller asking for refund. Depending on your country, require a signed return receipt (so that they can&#8217;t pretend the envelope has been lost &#8211; it happened to me).\n              \n              \n                Let your voice be heard loud: complain online in forums and blogs. This is way more effective than dropping an email to some Hahnemuhle marketing representative. Believe it or not, few years ago when we protested because the fourth consecutive 64 inches roll we opened in a series had a big fingerprint in the very same position and a dark, rubbing stain on one side (first 20 cm of paper) we got an email back where they assured that Hahnemuhle workers always wear white cotton gloves and we should keep calm.\n              \n              \n                End your existing stock, sigh with relief and buy another brand.\n              \n            \n\n            \n              [These are my own personal opinions, springing from +10 years working in the business of large format printing in Italy for fine-art photographers who exhibit in my country and abroad &#8211; I&#8217;m not related neither to Hahnemuhle nor to any other paper manufacturer except as a final user] &lt;/div&gt; &lt;/div&gt; \n              \n\n",
      tags: ["archival","cotton","Hahnemuhle @en","inkjet","issues","paper","rag","surface"],
      id: 96
    });
    

    index.add({
      title: "Decomposing Sharpening #5 Arithmetic matters",
      category: ["Decomposing Sharpening","Photoshop"],
      content: "\n  \n    The previous post of the series (#4 The Lab way) introduced a different strategy for splitting in an updatable fashion Dark and Light halos of the Unsharp Mask filter (USM) via Smart Objects (SO) and Smart Filters (SF). Basically, I&#8217;ve been manipulating a Subtraction between USM and the original image (Grayscale, RGB and CMYK files) or using the High Pass filter to work around the lack of the Subtraction blending mode in Lab colorspace.\n  \n\n  \n    Guilty omissions\n  \n\n  \n    I must tell you that I&#8217;ve hit the post #4 &#8220;Publish&#8221; button with a bit of an uncomfortable feeling. I was, deliberately, passing over one important detail when I wrote the formula:\n  \n\n  \n    \n      Amount % = 2*Base + 4*Booster\n    \n  \n\n  \n    Which regulates the total amount (in percent) of the USM on the following setup &#8211; where Base and Booster are the High Pass SO and Curves adjustment layers:\n  \n\n  \n    \n  \n\n  \n    The missing information is that you can not tweak the Base opacity, otherwise the result isn&#8217;t USM-like anymore. If you switch off the Booster layer and lower the opacity of the High Pass (Base) layer only to, say, 50% this doesn&#8217;t equal at all an USM of amount 2*50 = 100%:\n  \n\n  \n    \n  \n\n  \n    So the Base opacity must stick to 100% &#8211; which binds the original setup (the one above is just a simplified example, please find the details in the previous post) to work as a split Dark/Light halos USM simulator for Amounts in the range 200% &#8211; 600% only (no 100%, no 50%, etc). Just a negligible detail.\n  \n\n  \n     … At least as long as you don&#8217;t throw a new player in that layers crowded arena: that would act as an inverted Booster, something like a Dimmer.\n  \n\n  \n    We&#8217;re talking about an adjustment layer: either a flat, horizontal Curves passing through the middle point (128,128) or better a Layers adjustment with both Output Levels at 128 &#8211; which you must clip to the High Pass SO. As a result, the adjustment makes everything as a nice 50% gray. Theoretically, you should use 16 bits files in order to avoid visible posterization, yet I&#8217;m biased to thing that 8 or 16 wouldn&#8217;t make any remarkable difference: synthetic images are one thing, real world pictures another one (and we&#8217;re talking about posterization in a mostly 50% gray layer with blurred borders here and there).\n  \n\n  \n    When you need USM Amounts lower than 200%, switch off the Booster, turn on the Dimmer, modulating its opacity. Conversely, for Amounts higher than 200%, turn the Dimmer off, the Booster on and tune its opacity as you&#8217;d do usually. The math that regulates the Dimmer is easy:\n  \n\n  \n    \n      USM Amount % = 200 &#8211; 2*Dimmer opacity\n    \n  \n\n  \n    Back at the time of post #4 writing I didn&#8217;t know how to solve the problem, the solution came out experimenting later. So you may ask Davide, why don&#8217;t you just add this piece of information alongside the original article? The answer lies in the original motivations of the whole analysis: to build a tool for Photoshop (via scripting). As a little researcher of image related topics, I should be fine, happy and done &#8211; the problem&#8217;s fix has been expressed and it works. As a programmer, to implement such a stack of layers is far from being an elegant solution &#8211; and I&#8217;m the laziest around when it comes to write new code.\n  \n\n  \n    Subtraction, from a different point of view\n  \n\n  \n    It&#8217;s been playing around with blending modes math (which I&#8217;ll be talking about in the next and last post of the series possibly &#8211; notes, errata, variations, etc) that I bumped into the obvious.\n  \n\n  \n    \n      a &#8211; b = a + (-b)\n    \n  \n\n  \n    They call it the &#8220;aha moment&#8220;. Translated in the image arithmetic that matters for us, it means that the following layers setups are just equivalent.\n  \n\n  \n    \n  \n\n  \n    Which also means that post #4 about the Lab way and the first part of this very one are a just an interesting academic digression. I&#8217;m not going to implement it, relying to this (more efficient? less efficient? surely easier to code) setup:\n  \n\n  \n    \n  \n\n  \n    In other terms (just in case you have&#8217;t &#8220;aha-ed&#8221; yet) I can easily bypass the subtraction (forbidden in Lab) with an inversion (Invert adjustment layer &#8211; one or two depending on the cases) followed by an addition (Linear Dodge blending mode). A good news for Davide the scripter.\n  \n\n  \n    If your eyes are as sharp as your post-produced pictures, you&#8217;d notice I&#8217;m still omitting one detail. The Grayscale/RGB/CMYK routine involves two other blending modes that are forbidden in Lab too, namely Darken and Lighten. Their cousins Darker Color and Lighter Color will work there; the difference basically is that Darker/Lighten work on a channel basis, while Darker/Lighter Color work on the composite. You can visualize the difference here:\n  \n\n  \n    \n\n    \n      From left to right: Original (base). Gradient (blend). Blend in Darken. Blend in Darker Color\n    \n  \n\n  \n    Yet, if you resort to the old trick of the Advanced Blending Options, deselecting the a,b channels of the blend layer&#8230; and since I want the halos to be applied in Luminosity only, you get reasonably close to the equivalent of Darken/Lighten anyway.\n  \n\n  \n    This post should end the series: given a problem, I&#8217;ve found and discussed in depth the solutions (even those that have been proved to be less efficient). Yet a closing post will follow, with errata, variations, experiments, and thorough extra examination of some of the topics that have been shown here &#8211; we need to decompress somehow, I guess :-). Thanks for having followed so far!\n  \n\n  \n\n  \n    The Decomposing Sharpening series has been written as a research project for my script DoubleUSM: multi-radius sharpening. You might be interested also in Fixel Detailizer 2PS: multi-frequency contrast booster.\n  \n\n",
      tags: ["blending modes","Lab","sharpening","smart filter","smart object"],
      id: 97
    });
    

    index.add({
      title: "Decomposing Sharpening #4 The Lab way",
      category: ["Decomposing Sharpening","Photoshop"],
      content: "\n  \n    If you&#8217;ve followed this series, you know that I&#8217;m involved in a scripting project for Photoshop that deals with sharpening, splitting Dark and Light Halos control. So far, I&#8217;ve been able to build Layer Sets that mix Smart Objects (SO), Smart Filters (SF) and several Blending modes with good results &#8211; at least if I restrict myself to RGB and CMYK documents.\n  \n\n  \n    \n  \n\n  \n    Because when you approach Lab with that very workflow in mind, you hit a notable wall. That is to say, Subtract blending mode is not allowed: no Subtraction, no party.\n  \n\n  \n    In the Lab colorspace, Blending modes act in peculiar ways (some would say: arbitrary). For instance, if you set a layer to Multiply, only the L channel is actually multiplied, while a and b behave as in Overlay. Why? Go figure. For the same reason (that&#8217;s my own personal opinion) Difference, Exclusion, Subtract, Divide, not to mention Darken and Color Burn, Lighten and Color Dodge are grayed out.\n  \n\n  \n    One option when you feel stuck in a dead end is to ask for help to friends &#8211; better if they&#8217;re talented, with a solid scientific background, Photoshop experts and coding wizards. Most of the following content is here because of Jacob Rus&#8216; kind suggestions and comments. Luckily he also knows how to support the afflicted\n  \n\n  \n    \n      [Davide] I&#8217;ve found out that my biggest efforts have been spent dealing with unexpected behaviors [Jacob] This is called &#8220;programming&#8221; 😉\n    \n  \n\n  \n    So: a different plan is needed, back to the basics.\n  \n\n  \n    Work around Subtraction\n  \n\n  \n    If you recall post #1 of the series, when I&#8217;ve introduced the concept of the Halo Map I&#8217;ve briefly cited High Pass as one of the ways to extract detail up to a certain frequency threshold into a separate layer. High Pass is deeply linked to Gaussian Blur, being nothing but a Subtraction between the original image and a blurred one (with Offset 128).\n  \n\n  \n    You&#8217;ve read at least one interesting word: subtraction &#8211; yes, we&#8217;ll be using High Pass to workaround the Subtraction lack as a blending mode in Lab. To refine the correct setup let&#8217;s switch to grayscale for a quick review of the concepts I&#8217;ve shown throughout the past posts of the series.\n  \n\n  \n    Unsharp Mask uses Gaussian Blur\n  \n\n  \n    Indeed.\n  \n\n  \n    \n  \n\n  \n    High Pass is made with Gaussian Blur\n  \n\n  \n    That is, High Pass = Original &#8211; Blurred. You can get there in several ways, using Calculations, Blending modes or (as in the following screenshot) with Apply Image &#8211; I&#8217;ve applied the Original in Subtract, Offset 128 on a Gaussian Blurred layer.\n  \n\n  \n    \n  \n\n  \n     So Unsharp Mask can use High Pass\n  \n\n  \n    Which is not a very big news, but few have ever shown the exact setup that replicate USM precisely &#8211; and of course I&#8217;m going to do it here with SO and SF, splitting Dark and Light Halos.\n  \n\n  \n    Using Linear Light makes the High Pass layer look like USM: \n  \n\n  \n    \n  \n\n  \n    Question: how much USM is this? Besides the fact that Radius is equal to 20px (the same as the High Pass filter), what about the Amount? Let&#8217;s have a look to Linear Light blending mode:\n  \n\n  \n    \n      [Linear Light] if (Base &gt; ½) = Base + 2*(Blend-½) [Linear Light] if (Base &lt; ½) = Base + 2*Blend &#8211; 1\n    \n  \n\n  \n    That is to say:\n  \n\n  \n    \n      [Linear Light] = Base + 2*Blend &#8211; 1\n    \n  \n\n  \n    In other words, as Jacob&#8217;s suggested me, &#8220;Linear Light mode just (if you define the midpoint of any channel to be zero) adds twice the top layer to the bottom layer&#8220;. Which should suggest that the above Amount for the High Pass sharpening is equal to 200%.\n  \n\n  \n    How would you modulate the High Pass layer in order to get to 500%? A neat trick is to use Adjustment Layers in a similar way as we&#8217;ve been doing to stack blending modes, this time clipping a blank Curves layer (Linear Light too) on top of the High Pass. It will act as a blending multiplier (mind you: not Multiply as the blending mode, but like a Linear Light booster):\n  \n\n  \n    \n  \n\n  \n     The following formula regulates the interaction between the two Linear Light layers:\n  \n\n  \n    \n      Amount % = 2*Base + 4*Booster\n    \n  \n\n  \n    Where Base and Booster are the layer opacites of High Pass and Curves respectively. In other words the above screenshot shows a USM of Radius 20px and Amount 600% (2*100 + 4*100, pretty cool). Amount 500% requires 75% Booster opacity, Amount 300% requires 25% opacity and so on.\n  \n\n  \n    Splitting Halos\n  \n\n  \n    The business of splitting Dark and Light sides of USM here is pretty simple: you just need a way to obliterate darker-than-Gray 50% or lighter-than-Gray 50% shades from the High Pass layer. Which you can do with Curves or Layers adjustment layers, as follows the two options for Dark Halos:\n  \n\n  \n    \n  \n\n  \n     And the same two options for Light Halos:\n  \n\n  \n    \n  \n\n  \n     Is there anything missing?\n  \n\n  \n    Of course there is, namely the third (often less crucial) parameter: Threshold. In order to implement it, I&#8217;m going to leave this gray flatland and switch to color mountains &#8211; where my friend Santo shows his brand new trekking outfit (it was pretty cold in the Alps that day in spite of the sun &#8211; our wives may confirm). Let&#8217;s merge everything I&#8217;ve shown so far (SO, SF, High Pass, split halos, etc):\n  \n\n  \n    \n  \n\n  \n    Since we&#8217;re in Lab, Linear Light will act wildly on chromatic channels (a, b) too, so you&#8217;d be better switching them off in the two High Pass SO Advanced Blending dialog (right click the SO name and choose Blending Options):\n  \n\n  \n    \n  \n\n  \n    \n  \n\n  \n    Threshold rolls in as a new Layers adjustment layer, right above the two High Pass SO. You can use Curves as well, but I prefer Levels because by default they come ranged 0-255 like the actual Threshold is (by the way, while the Amount is expressed in percent, Radius in pixels, Threshold&#8217;s units are Levels, which is quite the case here).\n  \n\n  \n    In order to nicely visualize how Threshold affects the USM, switch off one side of the Halos (say, the Light Halos High Pass SO) and keep only the Background layer, the Dark Halos High Pass SO and its clipped friends on top. Switch the SO blending mode back to Normal. You should be looking at a gray, high pass stuff with dark halos on it.\n  \n\n  \n    Click on the Threshold named Layers adjustment and start to move the white Input cursor to the left (the value starts from 255 and lowers). You&#8217;ll see how the dark halos in the High Pass layer are slowly eroded by the threshold &#8211; that is, only strong darker halos resist, while the rest goes to 50% Gray &#8211; this is another way to look at what happens inside USM.\n  \n\n  \n    \n  \n\n  \n    You&#8217;ll move the White cursor left in the Dark Halos Threshold, the Black cursor right in the Light Halos Threshold Layers adjustment.\n  \n\n  \n    The final setup for Lab colorspace is eventually as follows:\n  \n\n  \n    \n  \n\n  \n    This is the end of our journey decomposing the Unsharp Mask filter: it is something I do because I like it! I&#8217;m at work on a Photoshop script (on this very topic); yet I hope that to break the toy, look in its belly and put together the pieces in new ways have been an interesting learning experience for you &#8211; as it&#8217;s been for me as well writing about it.\n  \n\n  \n    A last, fifth post will follow (and definitely conclude the series) with notes, errata, variations, additions and what else, goodies perhaps. Thanks for having followed so far, you&#8217;ve earned the right to feel like a Sharpening geek now! 😉\n  \n\n  \n\n  \n    The Decomposing Sharpening series has been written as a research project for my script DoubleUSM: multi-radius sharpening. You might be interested also in Fixel Detailizer 2PS: multi-frequency contrast booster.\n  \n\n",
      tags: ["blending modes","Lab","sharpening","smart filter","smart object"],
      id: 98
    });
    

    index.add({
      title: "Decomposing Sharpening #3 Workaround",
      category: ["Decomposing Sharpening","Photoshop"],
      content: "\n  \n    In the previous post I&#8217;ve approached appealing yet wrong solutions to the problem of splitting in a scriptable-friendly way Dark and Light halos of the Unsharp Mask filter (USM) using Smart Objects (SO). I&#8217;ve got to use Smart Filters (SF), which parameters can be easily modified anytime. To find a working answer to the problem (which is just a pretext for going into the topic of sharpening and blending modes), let&#8217;s go back to the basic of Halo Maps creation.\n  \n\n  \n    \n  \n\n  \n    If you&#8217;ve followed the first post of the series, this is what a Halo Map for USM Dark halos look like:\n  \n\n  \n    \n\n    \n      My friend Santo is back &#8211; here with his peculiar Dark Halos map (USM 500%, 3px) coming from a Grayscale version\n    \n  \n\n  \n    As a reminder, it&#8217;s been created fading to Darken the USM applied to a duplicate of the original, then subtracting the original layer from it. The appropriate Blending Mode to simulate USM is Linear Burn. How would you get there with SO, avoiding Calculations? Let&#8217;s split the problem into smaller pieces.\n  \n\n  \n    Fade to Darken / Lighten\n  \n\n  \n    Duplicate the Original layer, call it &#8220;USM &#8211; Darken&#8220;, then apply USM as a SF. We&#8217;ve a couple of options here, either:\n  \n\n  \n    \n      fade the SF in Darken\n    \n    \n      fade the SF in Luminosity and set the SO to Darken\n    \n  \n\n  \n    \n  \n\n  \n    Pick up the one you like the most (I prefer the latter, that avoids chromatic shifts and it&#8217;s a nice way). Nothing new so far.\n  \n\n  \n    Subtract\n  \n\n  \n    Photoshop doesn&#8217;t allow you to stack blending modes &#8211; that is, you can&#8217;t set a layer to Overlay, then set the composite result to Luminosity, then set the composite result to Darken. Something you would write:\n  \n\n  \n    \n      Darken( Luminosity( Overlay (layer) ) )\n    \n  \n\n  \n    A trick that doesn&#8217;t work (it would be really nice if it would) is to use nested Layer Sets (aka Groups), changing the blending modes of each one. No luck, try yourself if you will. Why do I want such a &#8220;double&#8221; blending? Because I&#8217;d like to use Subtract on a SO that already has its own blending mode. A neat solution is to use Adjustment Layers as a support &#8211; for instance if you want to Subtract to layers:\n  \n\n  \n    \n      Put the two layers one on top of the other in the Layers palette\n    \n    \n      Create a Curves Adjustment Layer (or Levels, it&#8217;s just the same) right in between. Leave it as it is.\n    \n    \n      Set it to Subtraction blending mode\n    \n    \n      Clip the upper layer to it\n    \n  \n\n  \n    \n  \n\n  \n    Pretty nice &#8211; the only limitation is that you can&#8217;t stack more than two blending modes (it would be a really cool feature if Photoshop supported nested, multiple clipping! But it doesn&#8217;t). Actually, I&#8217;m performing a triple blending (SF faded to Luminosity, SO set to Luminosity, clipped to Subtraction) but this is cheating because a SF is involved ;-). So, we&#8217;ve subtracted the faded-to-Luminosity, Darken USM and the original layer, let&#8217;s go ahead.\n  \n\n  \n    Invert\n  \n\n  \n    The Halo Map is almost done, we need to Invert it &#8211; and an Adjustment Layer will come handy. Almost done!\n  \n\n  \n    \n  \n\n  \n    Linear Burn\n  \n\n  \n    How would you set all that to Linear Burn as requested? Think about it: we already have a bunch of interacting Blending modes. The solution lies in the use of Layer Sets, with a caveat.\n  \n\n  \n    If you just group the Subtract Curve, the clipped SO and the Invert Adjustment Layer you&#8217;ll see that no matter what group&#8217;s Blending mode you set, the result doesn&#8217;t change. In order for a Layer Set to effectively use its content for blending, it must have a bitmap base: that is, it can&#8217;t be based only upon an Adjustment Layer with clipped stuff on, it plus extra adjustments on top. To make it work, duplicate the Original layer and put it inside the Layer Set, so that you end up with the following setup:\n  \n\n  \n    \n  \n\n  \n    Top-down:\n  \n\n  \n    \n      Linear Burn Layer Set, containing&#8230; \n        \n          Invert Adjustment Layer \n            \n              SO, set to Darken, with a USM SF faded to Luminosity &#8211; clipped to…\n            \n          \n        \n\n        \n          Curves Adjustment Layer set to Subtract\n        \n        \n          a duplicate of the original layer\n        \n      \n    \n\n    \n      the Original, unsharpened layer\n    \n  \n\n  \n    Does this make sense to you? Within this setup, you can change the USM Dark Amount/Radius on the fly, tweaking the SF (double click on it and change the values)\n  \n\n  \n    Halfway!\n  \n\n  \n    We&#8217;re just in the middle of the task &#8211; we still need to create the Light Halos Map. It should be easy now, though! (I&#8217;m lying of course ;-)) One thing that you should keep in mind when trying to replicate this setup on your own before reading along, is that you don&#8217;t just need to fade to Lighten (instead of Darken).\n  \n\n  \n    Think about how Subtraction works, say you&#8217;ve a Base layer and a Blend one on top, set to (wonder what) Subtraction. The actual operation carried along is:\n  \n\n  \n    \n      Base &#8211; Blend\n    \n  \n\n  \n    and not the reverse. If you go back to a simpler grayscale image, you&#8217;ll see that in order to extract the Light Halos the position of the layers in the subtraction must change, otherwise you&#8217;ll get a totally black layer:\n  \n\n  \n    \n  \n\n  \n    Eventually, your final setup will look like as follows:\n  \n\n  \n    \n\n    \n      Santo is over-sharpened on purpose &#8211; just in case you&#8217;re wondering\n    \n  \n\n  \n    Phew. We made it! This is a working configuration that let you split Dark and Light Halos of USM in a non-destructive, updatable way (my script is based on this one), at least for RGB and CMYK documents. Yes, because in the Lab colorspace, it doesn&#8217;t work at all (why? Try yourself and find this out if you don&#8217;t imagine the why). Lab requires a completely different approach, that I will explore in the next post of the series.\n  \n\n  \n\n  \n    The Decomposing Sharpening series has been written as a research project for my script DoubleUSM: multi-radius sharpening. You might be interested also in Fixel Detailizer 2PS: multi-frequency contrast booster.\n  \n\n",
      tags: ["blending modes","sharpening","smart filter","smart object"],
      id: 99
    });
    

    index.add({
      title: "Decomposing Sharpening #2 Mistakes",
      category: ["Decomposing Sharpening","Photoshop"],
      content: "\n  \n    In the previous post I cited a tool for Photoshop I&#8217;m working on &#8211; which is currently on its beta cycle, thanks to the people of Colortheory. Basically, it&#8217;s a Photoshop script that lets you control separately both Amount and Radius for Unsharp Mask filter&#8217;s Dark and Light halos: that is, you can apply USM 500% and 1.5px for Dark Halos and 300%, 0.9px for Light ones, tweaking sliders on a single GUI. \n\n     Being it a Script and not a Filter means that I have to drive Photoshop behind the scenes while you play with the interface (I don&#8217;t have pixel access, it would require a programming knowledge exceeding my skills). In other words I must find a way to replicate with layers and filters that very effect; not only, the scheme update must be easy, because I can&#8217;t afford the whole processing to be time consuming (otherwise the live-update performance of the tool would degrade).\n  \n\n  \n    That is to say: I won&#8217;t follow the steps outlined in the post #1 of the series &#8211; producing two halo maps via Calculations, which would require a new calculation each time the user changes a single value. I need on-the-fly updatable parameters &#8211; I&#8217;m afraid I need Smart Objects (SO). I&#8217;m not particularly fond of SO &#8211; yet they provide Smart Filters (SF) which fit perfectly my requirements: SF are Filters applied non-permanently to a SO, i.e. you can decide to change their parameters anytime, or switch them on/off.\n  \n\n  \n    Given this framework, how would you deal with SO and SF to split USM dark and light halos? There are at least two easy and intuitive ways that may came to your Photoshop-driven mind (they came to mine, too) &#8211; yet they&#8217;re wrong; this post is just about this &#8211; how not to be fooled by appealing, easy paths.\n  \n\n  \n    Blending modes: Darken / Lighten\n  \n\n  \n    Let me use this exciting picture as an example:\n  \n\n  \n    \n  \n\n  \n    Reminder: to convert a layer in a Smart Object, right click on the layer&#8217;s name in the Layers palette and choose Convert to Smart Object. Be aware that if you right click on the layer&#8217;s icon the option does not show (there must be good reasons that I miss for that). Alternatively, via menu: Layer &#8211; Smart Objects &#8211; Convert to Smart Object.\n  \n\n  \n    That said, if you duplicate the Background layer, convert it to SO, apply the Unsharp Mask Filter (USM, say 500% 10px, we&#8217;ll always leave Threshold untouched) and set the SO blending mode to Lighten, this is what you get:\n  \n\n  \n    \n  \n\n  \n    Same concept but a different USM (500%, 30px) blended in Darken:\n  \n\n  \n    \n  \n\n  \n    So what would we get if we mix the two?\n  \n\n  \n    \n  \n\n  \n    Exactly what you would have expected &#8211; that&#8217;s it, we&#8217;re done splitting dark and light halos. With a caveat: what if we reverse the layer&#8217;s order (Lighten top, Darken bottom)?\n  \n\n  \n    \n  \n\n  \n    Uhm, it doesn&#8217;t work anymore. So the trick is to keep the SO with the bigger USM Radius on top of the other &#8211; easy peasy.\n  \n\n  \n    Alas, I should have been more careful, because I&#8217;ve based a good deal of coding time on this paradigm (two SO in Darken / Lighten) just to discover that it is flawed. What a unhappy business! Not to mention what my wife has been repeating me (she used to work as a programmer): &#8220;Did you test it? You did it not enough. Test it more or you&#8217;ll waste your time later. Don&#8217;t complain, I told you this before: test it!&#8221; She&#8217;s been in charge of testing, as you may guess.\n  \n\n  \n    But&#8230; why it doesn&#8217;t work? The example I&#8217;ve showed is misleading because as a test it is not exhaustive one: I&#8217;ve changed one parameter only &#8211; the Radius, keeping the Amount full blast at 500% What happens for instance if I lower back the Amount of Dark halos to 100%? No matter how I would set the layers order, the result does not match my expectations:\n  \n\n  \n    \n  \n\n  \n    First row (Lighten on top) is obviously flawed because it erases the Darken effect &#8211; the dark halo is wrong. Middle row (Darken on top) is not correct too, because the light halo is slightly darkened by the dark halo: it&#8217;s subtle because I&#8217;ve used a low Amount. Check out the last row to compare them with the right effect. Back to square one.\n  \n\n  \n    Multiple Smart Filters\n  \n\n  \n    A single SO can support more than one SF (how many, frankly I don&#8217;t know). A nice idea could be to apply two rounds of USM as SF to the same SO, fading them to Darken / Lighten. To fade a SF, click on the &#8220;lines-and-arrows&#8221; icon at the right of the &#8220;Unsharp Mask&#8221; label of the Smart Filter.\n  \n\n  \n    \n  \n\n  \n    There&#8217;s a detail that prevent this one to work, though. SFs don&#8217;t operate in parallel, but in sequence: that is, the first USM round acts on the original SO (as expected); the second USM round operates on the result of the first SF, i.e. upon an already sharpened version. No matter how you set the SF stack order (in the following image: first Darken top, second Lighten top), the result is wrong.\n  \n\n  \n    \n  \n\n  \n    So what?\n  \n\n  \n    As a consequence of these experiments and in a bad mood for the time wasted coding a flawed paradigm (lesson learned: test extensively! Don&#8217;t get excited when things apparently work!), I&#8217;ve had to resort to some&#8230; trickier solutions &#8211; which I&#8217;ll show you in the next post of the series.\n  \n\n  \n\n  \n    The Decomposing Sharpening series has been written as a research project for my script DoubleUSM: multi-radius sharpening. You might be interested also in Fixel Detailizer 2PS: multi-frequency contrast booster.\n  \n\n",
      tags: ["blending modes","sharpening","smart filter","smart object"],
      id: 100
    });
    

    index.add({
      title: "Decomposing Sharpening #1 Introduction",
      category: ["Decomposing Sharpening","Photoshop"],
      content: "\n  \n    I&#8217;m currently working on a project of mine (Photoshop script) that involves sharpening. This has been giving me the opportunity to err (quite a bit) and discover some interesting combinations that may lead to new tools. In this first post of a small multipart series, I&#8217;ll review basic concepts like Halo Maps, Subtractions and Blending Modes applied to sharpening.\n  \n\n  \n    I&#8217;ve always been a great fan of Dan Margulis&#8217; PPW (Picture Postcard Workflow) panel, coded by my dear friend and talented scripter Giuly Abbiati. If you don&#8217;t know it, I&#8217;m talking about a Photoshop extension filled up with great tools that make Dan&#8217;s color correction workflow really speedy and automated &#8211; provided that you know the theory behind it. Over the years Margulis has freely distributed updated actions that target very specific topics &#8211; of course sharpening is one of them and this is what I&#8217;d like to go into.\n  \n\n  \n    The last evolution stage of such action (at the time of this post in September 2012, but new versions may appear for this is a very active project) provides you with several layers &#8211; each one aiming to a peculiar target: among the rest there are two separate high-frequency sharpening layers, one for Light Halos and one for Dark Halos. If you&#8217;re interested in the exact way they&#8217;re produced, applied and masked within the context of the PPW workflow, please refer to Dan&#8217;s original action and follow each step.\n  \n\n  \n    Let&#8217;s first approach the idea of creating a Halo Map &#8211; that is, a single layer that contains just what is needed in order to sharpen and is applied on top of the original image with an appropriate blending mode. The purpose of such layer is to separate the effect (a detail enhancing filter) from its substrate (the original image).\n  \n\n  \n    A trivial and well known Halo Map is easily built with the High Pass filter:\n  \n\n  \n    \n\n    \n      Please welcome my friend Santo &#8211; here showing his trekking outfit and the result of the High Pass filter\n    \n  \n\n  \n    Switching the blending mode to some contrast enhancing one (such Overlay, Soft Light, Hard Light, etc.).\n  \n\n  \n    Strangely enough, at least in my personal opinion, High Pass is mainly used for HiRaLoAm (High-Radius-Low-Amount) sharpening &#8211; that is, not targeting high frequency detail but as a local contrast enhancer (speaking of which, let me remind you ALCE as my favorite tool ;-)) Bizarre, because there is nothing to prevent you using High Pass to create a traditional sharpening layer.\n  \n\n  \n    In a nutshell, the High Pass filter is a quick way to cut every image frequency up to the radius you&#8217;ve set: that is, radius 10px replaces with a nice 50% gray everything in the image but frequencies higher than 10px &#8211; if this sounds mysterious, dig the topic here. I&#8217;m sure you&#8217;re willing to know that the result of High Pass Filter is exactly equal to a Subtraction: between the original image and a Gaussian Blurred one (with the same radius).\n  \n\n  \n    \n  \n\n  \n    The only extra detail that you should keep in mind is that Offset is 128. The actual operation performed is then (speaking of values of each pixel, ranging from 0 to 255 alias black to white):\n  \n\n  \n    \n      High Pass = 128 + ( Original &#8211; Blurred )\n    \n  \n\n  \n    Incidentally, 128 is 50% gray, which is the neutral color of contrast enhancing blending modes (i.e. it does not affect the image below)\n  \n\n  \n    Be aware that if you like to test this yourself using RGB images, you must set the Photoshop Grayscale Default (Color Settings, Command + Shift + K for Mac or use Ctrl instead of Command for PC &#8211; I&#8217;ll be using Mac shortcuts throughout the post) to an ICC with a gamma that matches the one of the RGB file you&#8217;re working on. If you&#8217;re about to ask why, the reason is in the way Photoshop displays channels; as a rule of thumb if your file is ProPhoto RGB, set the Grayscale ICC to Gray Gamma 1.8, if you&#8217;re using Adobe RGB or sRGB set it to Gray Gamma 2.2 (more on colorspaces gamma in Bruce Lindbloom&#8217;s website)\n  \n\n  \n    Let&#8217;s move towards more interesting Halo Maps:\n  \n\n  \n    \n  \n\n  \n    The screenshot above shows the actual high frequency Dark Halos (left) and Light Halos (right) layers from Dan&#8217;s PPW sharpening action, without masks. The former is mostly white because its blending mode is set to Multiply &#8211; that is, any value is multiplied with the one from below, resulting in darker pixels. How come? Because for the operation the values used are defined in the range 0 to 1 and not 0 to 255 (either way, black to white): this way, if you multiply two 50% grays you end up with 0.5 * 0.5 = 0.25 which is 75% gray (definitely darker). White, which is what Dark Halos map layer is mainly made of, is the neutral color of Multiply.\n  \n\n  \n    Not surprisingly, the Light Halos layer looks mostly black, being in Screen blending mode (the inverse of Multiply) &#8211; where black is the neutral color.\n  \n\n  \n    To sum up, Multiply darkens (neutral: White), Screen lightens (neutral: Black), Overlay is a combination of Screen and Multiply (Gray 50% is the neutral color). It&#8217;s contrast enhancing just because it darkens and lightens at the same time, depending on the layer content: darker gray darkens, lighter gray lightens, pivoting on middle gray. Citing Paul Dunn, who has a very informative page about blending modes:\n  \n\n  \n    \n      [Multiply] = Base * Blend [Screen] = 1 &#8211; (1-Base) × (1-Blend) [Overlay, if (Base &gt; ½)] = 1 &#8211; (1-2×(Base-½)) × (1-Blend). [Overlay, If (Base &lt;= ½)] = (2×Base) × Blend\n    \n  \n\n  \n    This is what you need to know in order to replicate halo maps such as the ones found in the PPW action. Yet Dan applies them in a peculiar way (I&#8217;ll let you the pleasure to decipher the action in order to get the details) &#8211; I&#8217;ll be describing a more orthodox path, leading to halo maps that emulate the Unsharp Mask filter (USM)\n  \n\n  \n    \n      On a duplicate layer, apply USM, the settings you like the most\n    \n    \n      [for Light Halos] Fade to Lighten (Command + Shift + F)\n    \n    \n      Use Calculation and Subtract the original to the USM version\n    \n  \n\n  \n    \n  \n\n  \n    That&#8217;s it. To get the Dark Halos map instead, just Fade to Darken, subtract the USM faded Darken from the Original and invert the result of the Calculation. A useful shortcut is to use Subtraction blending mode on the USM layer to skip the Calculation step.\n  \n\n  \n    Now that you&#8217;ve built dark and light layers, set them to the appropriate blending mode. Which one? Dan&#8217;s been choosing Multiply and Screen because his workflow involves masks and a slightly different process, yet if you want to get the exact result of the USM filter with the layers we&#8217;ve built together you should use Linear Burn and Linear Dodge.\n  \n\n  \n    \n      [Linear Burn] = Base + Blend [Linear Dodge] = Base + Blend &#8211; 1\n    \n  \n\n  \n    If you look at the math, we&#8217;ve first performed a subtraction between Original and Sharpened, to extract the Difference (Halos); then we&#8217;re adding back that Difference (Halos) to the Original in order to get back the Sharpened. That is to say:\n  \n\n  \n    \n      Sharpened &#8211; Original = Halos Original + Halos = Sharpened\n    \n  \n\n  \n    Trivial. But… What for?! Many interesting little things, for instance driving separately the Amount and Radius of Dark and Light Halos in Sharpening &#8211; a common feature in Drum Scanners&#8217; softwares in the past, missing in Photoshop today &#8211; as you&#8217;ll see in the next post of the series.\n  \n\n  \n\n  \n    The Decomposing Sharpening series has been written as a research project for my script DoubleUSM: multi-radius sharpening. You might be interested also in Fixel Detailizer 2PS: multi-frequency contrast booster.\n  \n\n",
      tags: ["blending modes","Calculations","high pass","sharpening"],
      id: 101
    });
    

    index.add({
      title: "Adobe Exchange &#8211; featuring ALCE",
      category: ["Photoshop"],
      content: "\n  \n    Today Adobe has released in the Adobe Labs a preview of the new Adobe Exchange! Quoting the official website:\n  \n\n  \n    \n      The new Adobe® Exchange is a Creative Suite® extension marketplace. It is available as a panel within a variety of CS6 applications. The Adobe Exchange panel provides a new way to search, discover, and install plug-ins, extensions, and other content for Creative Suite products. With this preview of the Adobe Exchange panel, you can browse through items that are available for your CS6 applications; download and install those items; and update acquired items when new versions are available.\n    \n  \n\n  \n    Developers of Creative Suite extensions can now reach a great deal of new customers via the Exchange panel: that opens directly within any major CS application (including among others Photoshop, InDesign, Illustrator, Dreamweaver, Fireworks, Premiere, etc) and allows in-app buy. Users can search and navigate through extensions (either free and paid), read reviews, download trials if available. The buy is managed within Exchange &#8211; which relies on FastSpring servers for the e-commerce side &#8211; and the product is downloaded and installed via Adobe Extension Manager &#8211; and you&#8217;re ready to go! \n\n     An Adobe Exchange Producer Portal for developers lets them submit new products and control sells in quite a streamlined fashion &#8211; it looks like a modern, much more efficient evolution of the Adobe Marketplace.\n  \n\n  \n    That said, I&#8217;m proud and excited to inform you that RBG &#8211; the Roberto Bigano Group is there, with four extensions coded by the wonderful Giuly Abbiati and me!\n  \n\n  \n    \n      \n\n      \n        ALCE &#8211; Advanced Local Contrast Enhancer for Photoshop (featured product for the Adobe Exchange launch!!)&lt;/strong&gt; Read more about it in the ALCE website.\n      &lt;/li&gt;\n\n      \n        \n          Floating Adjustments &#8211; a free panel that let you use big floating windows for any Photoshop Adjustment Layer, see the dedicated blogpost.\n        \n      \n\n      \n        \n          CPT &#8211; Channel Power Tools, the amazing Swiss knife for channel operations in Photoshop with new features and CS6 support.\n        \n      \n\n      \n        \n          False Profiles &#8211; a free utility that installs and let you quickly choose among a full set of false ICC profile for color correction.\n        \n      &lt;/ol&gt;\n\n      \n        All these four extensions are available through the free panel Adobe Exchange, now live on the Adobe Labs! Go grab it, and if you happen to download and enjoy any of our panels, please write a review for the future generations 🙂\n      &lt;/div&gt;\n      \n\n",
      tags: ["Adobe Exchange","ALCE","Creative Suite"],
      id: 102
    });
    

    index.add({
      title: "Earthquake down time",
      category: ["Personal"],
      content: "\n  \n     Have you heard about Italy&#8217;s recent earthquakes (May 2012) in the Emilia-Romagna region?\n  \n\n  \n    I did, because I live(d) some 15 Km from the epicenter: and that&#8217;s the why you&#8217;ve not found any update to the blog for some time. But this one! Read along.\n  \n\n  \n    \n  \n\n  \n    \n\n    \n      Finale Emilia (MO) &#8211; Clock Tower, 1213 (yes, the year it&#8217;s been built and after the second earthquake nothing is left)\n    \n  \n\n  \n    I&#8217;m the luckiest guy: my wife Elena, my 6 yo daughter Anita and I, we&#8217;re ok (given the circumstances). Two earthquakes hit the region: the first Sunday, May 20 at 4:03 AM (magnitude 6.1 Richter) and a second one few days later on Tuesday, May 29 at 9:00 AM (magnitude 5.8 Richter), causing &#8220;only&#8221; 26 deaths but razing nearly to the ground many historical downtown&#8217;s buildings of San Felice sul Panaro, Mirandola and Cavezzo and several other small towns &#8211; houses, monuments, city halls and all the rest. Approximately +15.000 people live now in tents provided by the national Civil Protection; many other towns were severely damaged, for precaution the centers of Carpi and Cento (among others) are offlimits &#8211; inhabitants have been requested to leave their houses and barriers have been erected to prevent anyone&#8217;s access, but the police. The Army is on duty to protect the zone from looters.\n  \n\n  \n    \n\n    \n      Crevalcore, Porta Modena &#8211; Firemen start to gather near one of the two historical arches called &#8220;town&#8217;s door&#8221; to forbid the entrance in the downtown &#8220;red area&#8221;\n    \n  \n\n  \n    Virtually anyone who lived in these districts have been sleeping outside, either in cars or tents, for days on, since smaller quakes continue to hit (+30 a day, ranging from 2.5 to 3.5, is a fair estimate; with five extra peaks &gt;5.0 Richter). Not easy to keep calm there.\n  \n\n  \n    \n\n    \n      The epicenter, in Emilia Romagna (north of Italy)\n    \n  \n\n  \n    The area is called Pianura Padana, a large flatland 38.000 square Km wide, with big towns lined up across the ancient roman Æmilia way (after which the region is called). All the rest is a very productive countryside for both agriculture and zootechnics (Parmigiano Reggiano cheese comes from here: americans may know its faint clone called Parmesan), with a tradition of small to medium sized companies involved in the business of mechanical engineering, connected with major automotive/bikes internationally known firms: within a radius of 50km you can find Ferrari (Maranello), Lamborghini (S.Agata Bolognese, where my wife used to work as a programmer), Maserati (Modena), Ducati (Bologna). I&#8217;ve learned that many companies involved in biomedical tech settled here too. And they&#8217;re in serious troubles now. Quoting reliable, official sources, &#8220;the earthquake damage in Emilia for the economy as a whole, according to some estimates could exceed 4 billion&#8221;. The following picture shows a Parmigiano Reggiano seasoning warehouse: bear in mind that one seasoned item (about 30Kg, diameter 40cm) is sold for about 400 Euros (approx. 500 US Dollars), what you&#8217;re looking at is millions of Euros of damages. And it&#8217;s just one warehouse, there are many, many others.\n  \n\n  \n    \n\n    \n      Millions of Euros worth Parmigiano Reggiano original cheese in a seasoning warehouse\n    \n  \n\n  \n    The epicenters are (obviously) lined on one of the geological faults that cross here, due to the pressure caused by the Alps and the Appennins, the two mountains chains of Italy. Yet the landscape seems a very calm one, for the flatland is the result of thousand of years&#8217; sediments brought there from the longest river in the country, the Po, ending in the Adriatic sea.\n  \n\n  \n    \n\n    \n      Emilia Romagna faults\n    \n  \n\n  \n    According to some engineers, the earthquakes danger index of the area has been kept low since the last world wide war, to promote post-war reconstruction &#8211; as a result, an impressive amount of new buildings (have a look at this impressive picture from INGV) caved in as well as ancient structures, for they&#8217;ve been erected following loose anti-seismic building rules. Now it seems they will raise it again. A strange, unusual phenomenon happened here as well &#8211; the so called &#8220;ground melting&#8221;: basically, if a layer of fine sand lies on the top of a water bearing stratum, the earthquake would push the water through the sand layer, causing a mud flooding &#8211; right from the bowels of the earth. Many houses has been found filled with half of a meter mud layer &#8211; which is a bad sign: this means that the underground is now kind of empty and cannot support the weight of the building anymore &#8211; it will, slowly, subside.\n  \n\n  \n    \n\n    \n      List of Earthquakes (www.ingv.it) &#8211; Star is Magnitude &gt; 5 Richter; Square &gt; 4; Circle &gt; 3, Small Circle &lt; 3. Updated to June, 20th.\n    \n  \n\n  \n    The first quake came so early in the morning (4 AM) that almost nobody has been injured in churches and/or industrial structures &#8211; strange as it may seem, the darkness helped a lot: not to see your house vibrate on waves coming from the ground is a bliss, even if glasses, dishes, bottles and all the rest crashed on the floor. My dear hardware calibrating Quatographic IP213 monitor fell from the table to the ground too, but hey, it&#8217;s German made and worked flawlessly anyway.\n  \n\n  \n    \n\n    \n      Quatographic Monitor, twice earthquake survivor championship winner &#8211; pack your things and displace them in a safer place, hurry up!\n    \n  \n\n  \n    The second, stronger quake, on the contrary, came 9 days later, late in the morning and killed workers and people doing their daytime business: I was driving my car and felt clearly the vehicle swinging on its left-right axis and I almost lost the control. This time children were at school (like my daughter Anita &#8211; who, she said, has been caught by the earthquake while she was just sitting on the toilet) and my wife reported that she couldn&#8217;t exit the garden facing her cousin&#8217;s house because &#8220;the small gate was vibrating so badly that I couldn&#8217;t find a way to run through it&#8221;. Few hours later a couple of strong quakes hit again and I confirm that to see a big tree swinging that way is truly impressive. My dear Quatographic monitor fell on the ground one more time: hat off to Germans, it shows the very same small deltaE on colors that made it an Ugra/Fogra certified display for critical color work &#8211; I feel like it&#8217;s had its own fair share of crash tests so I finally moved it to a safer location. Not surprisingly, the Drobo (being so bulky) didn&#8217;t even shifted on the table.\n  \n\n  \n    To undergo a second earthquake just 9 days later, when people started to recover from the shock, has been a big trauma for everyone. Almost nobody is sleeping inside a house, but in tents (even right now, a full month later).\n  \n\n  \n    \n\n    \n      Caselle di Crevalcore &#8211; not uncommon to see tents in front of one&#8217;s house\n    \n  \n\n  \n    Buildings that did suffer the first quake collapsed, turning our towns into so-called, uninhabited, &#8220;red areas&#8221;. More than 160 schools are closed waiting for repairing or demolition. The latter case is the one of the primary grade Anita would have started in September: it seems that containers will be the solution for the next one/two years, then… we&#8217;ll see. And this is the fate shared by many other buildings, so damaged that repairing is not worthy at all: will they be reconstruct? The social structure of many small towns rely on this answer.\n  \n\n  \n    \n\n    \n      Cavezzo (Reuters/Benvenuti)\n    \n  \n\n  \n    We moved to Bologna, in my mother in law&#8217;s house. Since, as we say in Italy, tragedies never come alone, my wife&#8217;s mother died the very same night of the first earthquake, after months fighting a cancer: since January 2012, my wife lived with her, while my daughter and I we stayed in our house in the countryside, meeting just in the weekends &#8211; so each one of us has had his own share of work. Me, as a full time working dad (bring Anita to school, take Anita to the pool, feed Anita, clean the house, wash the dishes, fill the washing machine, do pre-press, find the cats, etc), her as a full time caregiver.\n  \n\n  \n    \n\n    \n      Elena and Anita enjoy the pleasures of the tent on the backyard\n    \n  \n\n  \n    Anyway, as I wrote: given the circumstances, we&#8217;re ok. We&#8217;ll have to face tough times but it could have been much worse. I&#8217;ve had a couple of big jobs to work on, a week of vacations on the Alps is already booked, we just don&#8217;t know what to do with our house (we were starting some big restoration &#8211; of course everything is on standby now, and we&#8217;re wondering whether it&#8217;s worth or not to start restoring at all; to sell the house is not an option, since the real estate market in the area is… well, you can imagine). I hope to update the blog with more happier posts in the future… but I had to write this one. Ciao!!\n  \n\n  \n    PS If you&#8217;re willing to contribute, this is the fundraising page for schools in Crevalcore. And here are some other pictures I took around in my village, Caselle di Crevalcore.\n  \n\n  \n    \n\n    \n      Collapsed building in Caselle &#8211; as seen from the Panaro river&#8217;s bank (click to enlarge).\n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    \n\n    \n      An empty, central square in the town of San Giovanni in Persiceto &#8211; the days after the first earthquake almost every store, bar, shop, office was closed. (Click to enlarge)\n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    \n\n    \n      Caselle&#8217;s church &#8211; after the second quake the belfry was irreparably damaged, forcing the people living in the building nearby to quit their house: it could come down right on it. Inside, one column fell on the ground ripping the roof open, and a long crack running from one side of the church, across the whole roof, to the other side, suggests that the facade is basically detached from the rest of the structure and it risks to bend forward and simply fall &#8211; as a result, the road has been closed to the traffic, splitting the village in two. (Click to enlarge)\n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    \n\n    \n      Caselle di Crevalcore &#8211; what the belfry looked like on May 20 (1st earthquake), May 29 (2nd earthquake) and the following days. The whole church will probably share the same fate &#8211; click to enlarge.\n    \n  \n\n  \n    More pictures in the Flickr account of the National Institute of Geophysics and Vulcanology.\n  \n\n",
      tags: ["Earthquake"],
      id: 103
    });
    

    index.add({
      title: "Dorsi digitali Phase One e Adobe Camera Raw &#8211; Paneling &#8220;bug&#8221;",
      category: ["Post-produzione"],
      content: "\n  \n    \n  \n\n  \n    Se usi un dorso digitale Phase One (serie IQ oppure P/P+) ti è stato detto di adoperare esclusivamente Capture One Pro per lo sviluppo dei raw. Qualità insuperabile, e dopo tutto se un&#8217;azienda produce sia hardware che software, sarà in grado di ottimizzare entrambi per lavorare in coppia come nessun altro.\n  \n\n  \n    Personalmente, temo di non condividere lo stesso entusiasmo dei sostenitori di Capture One: da quando Adobe ha distribuito il nuovo Camera Raw 7.0 (con Photoshop CS6, la cui tecnologia è incorporata anche in Lightroom 4) finalmente esiste un&#8217;eccellente alternativa &#8211; direi superiore sotto molti aspetti chiave. Purtroppo, devi tenere conto di un problema fastidioso e nascosto &#8211; amichevolmente chiamato il Paneling &#8220;bug&#8221;.\n  \n\n  \n    \n  \n\n  \n    Qual è la parte di &#8220;non ufficialmente supportato&#8221; che ti sfugge?\n  \n\n  \n    &#8230; Mi è stato detto una volta, ed avevano ragione in qualche modo. Aprire un file raw Phase One con estensione .IIQ in Adobe Camera Raw (ACR d&#8217;ora in poi) era teoricamente sconsigliato, perché mancava il &#8220;supporto ufficiale&#8221;. Cosa volesse dire era lecito domandarselo, perché ACR sembrava leggere ed aprire senza problemi gli IIQ proprio come qualsiasi altro file CR2, NEF, DNG.\n  \n\n  \n    Errore:  ogni file .IIQ contiene (meta)dati di calibrazione del sensore che ACR non è in grado di interpretare. Queste informazioni, da quel che vedo, sono usate per bilanciare la risposta del sensore, che è geometricamente diviso in otto sezioni (due righe per quattro colonne). Che siano otto elettroniche separate oppure otto sensori più piccoli messi assieme non ne ho idea, e obbiettivamente non mi interessa granché.\n  \n\n  \n    \n\n    \n      PhaseOne P65+ Sinistra: versione RGB. Destra: il canale \"a\" di Lab equalizzato, evidenzia Paneling (griglia).\n    \n  \n\n  \n    \n\n    \n      PhaseOne IQ180. Sopra: versione RGB, debole dominante incrociata verde/magenta sull'asfalto. Sotto: il canale \"a\" di Lab equalizzato evidenzia il problema più chiaramente.\n    \n  \n\n  \n    Il punto è che se questa calibrazione, diversa per ogni sensore, si perde (e ACR 7.0 e le versioni precedenti la perdono per strada di sicuro), la demosaicizzazione produce un&#8217;immagine non corretta. Le differenze tra ogni rettangolo della griglia possono venire accentuate piuttosto facilmente dalla color correction successiva: e credimi, tu non vuoi trovarti per le mani un file che ha dominanti verdi e magenta disposte a scacchiera nell&#8217;immagine.\n  \n\n  \n    \n      N.B. questo problema non si manifesta usando Capture One, che legge ed interpreta correttamente i tag proprietari dei file .IIQ.\n    \n  \n\n  \n    Ad oggi, sia Photoshop CS6 che Lightroom 4 supportano ufficialmente l&#8217;intera gamma di dorsi Phase One, eppure il problema persiste. Puoi facilmente controllare se i tuoi raw sono gestiti senza errori da ACR, senza essere affetti dal cosiddetto Paneling &#8220;bug&#8221;. Ti consiglio di provare almeno una volta su alcuni files (uno scatto test di un muro bianco ed un paio di immagini &#8220;vere&#8221;):\n  \n\n  \n    \n      \n        Apri il file .IIQ in Adobe Camera Raw, azzerando tutto eccetto il bilanciamento del bianco. Profondità di bit (8/16) e profilo RGB non contano, i tuoi valori di default andranno bene.\n      \n    \n\n    \n      \n        In Photoshop, converti il file in Lab (Immagine &#8211; Metodo &#8211; Colore Lab oppure Modifica &#8211; Converti in Profilo &#8211; Colore Lab).\n      \n    \n\n    \n      \n        Nella palette dei Canali rendi attivo il canale &#8220;a&#8221; (Command + 4 su Mac, CTRL + 4 su PC, Photoshop CS5 e CS6)\n      \n    \n\n    \n      \n        Immagine &#8211; Regolazioni &#8211; Equalizza.\n      \n    \n\n    \n      \n        Rendi attivo il canale &#8220;b&#8221; (Command + 5 su Mac, CTRL + 5 su PCs) e ripeti il passaggio #4.\n      \n    \n  \n\n  \n    Se nel canale &#8220;a&#8221; e/o &#8220;b&#8221; intravedi una griglia più o meno evidente, i tuoi files sono (non ufficialmente) non supportati in ACR! A volte la griglia non è così ovvia, e puoi trovare stranezze come quella nell&#8217;immagine qui sotto &#8211; brutta notizia comunque.\n  \n\n  \n    \n\n    \n      Phase One IQ180. Sopra: cielo con evidente Paneling (dominante magenta nella metà sinistra). Sotto: il canale \"a\" di Lab equalizzato evidenzia un pattern decisamente bizzarro.\n    \n  \n\n  \n    \n\n    \n      Clicca per ingrandire.\n    \n  \n\n  \n    Tra parentesi: so bene che i passaggi sopra descritti sono un modo estremo per brutalizzare un&#8217;immagine a fini di test &#8211; eppure l&#8217;aumento di contrasto nei canali &#8220;a&#8221; e &#8220;b&#8221; di Lab è piuttosto comune tra ritoccatori, ed il comando Equalizza è effettivamente usato nel cosiddetto Modern Man from Mars, una tecnica codificata da Dan Margulis, elemento fondamentale del suo Picture Postcard Workflow, aka PPW.\n  \n\n  \n    Mi è stato riferito da un utente Phase One che lo stesso tipo di Paneling &#8220;bug&#8221; potrebbe mostrarsi anche usando Capture One come raw processor. Nel qual caso la calibrazione interna del dorso digitale non corrisponde più alla vera risposta del sensore: che tradotto in termini pratici significa spedire tutto a revisionare in Danimarca (lui ha fatto così, ed ha risolto i suoi guai).\n  \n\n  \n    Usa Adobe Camera Raw lo stesso!\n  \n\n  \n    Personalmente, Capture One Pro (C1 Pro) non mi piace per nulla. Non è gentile scriverlo in certi forum (le persone sono piuttosto suscettibili quando si tratta di religione, cibo e raw processor), ma siccome ora non mi sente nessuno posso confessarlo: è un gran brutto software, macchinoso, lento, involuto, inutilmente complesso, la UI (interfaccia utente) e i controlli sono progettati male (prova a muovere un punto su una curva dei canali senza perdere la pazienza), anche dal mero punto di vista degli algoritmi ACR 7.0 è superiore. Questo è il mio personale, opinabile punto di vista &#8211; e non sono un fan di Adobe, di solito.\n  \n\n  \n    \n\n    \n      Capture One, pannello Curve.\n    \n  \n\n  \n    Nota bene: la qualità delle immagini di C1 è molto buona (per quanto più spesso io preferisca ACR nel confronto diretto) ma il prezzo in termini di usabilità è troppo alto. Se non avessi alternative, mi adatterei mestamente a C1. Ma ACR 7.0 restituisce una qualità eccezionalmente competitiva: la riduzione del rumore è senz&#8217;altro migliore, l&#8217;usabilità generale è di svariati ordini di grandezza superiore, il Local Laplacian filtering (l&#8217;algoritmo, rivisto, che sta dietro a Clarity) è davvero molto efficace, adesso che Adobe ha implementato le curve sui singoli canali R, G, B (era ora!) non ci sono ragioni per non usarla.\n  \n\n  \n    Quindi io apro i files .IIQ in ACR (perché sono un ritoccatore, i fotografi in genere adoperano Lightroom: che, tecnicamente parlando, è lo stesso motore in una macchina diversa). Come?\n  \n\n  \n    Evita la conversione in DNG\n  \n\n  \n    Non provare ad esportare gli .IIQ in DNG con Capture One (un processo stranamente veloce per gli standard di C1, peraltro). E&#8217; stato segnalato che precedenti versioni di C1 commettono piccoli errori sui metadati e pasticci sulla dimensione dei files (gonfiati nel DNG fino al doppio rispetto agli IIQ). Che questi problemi siano stati risolti o meno (non mi risulta, almeno per quel che riguarda il peso), anche usando una versione aggiornata di Capture One i DNG non sono calibrati, per cui quando vai ad aprirli in ACR si presenta lo stesso Paneling &#8220;bug&#8221;! Questo dal mio punto di vista è un problema non da poco, che dovrebbe essere preso in considerazione da Phase One: quando un utente converte in un formato standard come il DNG, si aspetta di ottenere dati corretti. Posso capire che alcune informazioni proprietarie di secondaria importanza, incorporate nei raw, possano non trovare spazio nelle specifiche DNG (penso ai punti usati dall&#8217;autofocus, che se la memoria non mi inganna sono recuperabili nei CR2 solo usando il Canon DPP). Qui però stiamo parlando dei canali dell&#8217;immagine, che sono intenzionalmente lasciati senza correzione!\n  \n\n  \n    Nel caso ti venisse l&#8217;idea: lascia pure stare l&#8217;Adobe DNG Converter &#8211; al quale dei dati di calibrazione del sensore non interessa un fico secco. L&#8217;unico stratagemma che ho trovato non è particolarmente elegante, ma funziona &#8211; se sei disposto a perdere qualcosa per strada.\n  \n\n  \n    Livelli e Modalità di Fusione\n  \n\n  \n    Negli ultimi tre anni ho avuto l&#8217;opportunità di lavorare con due dorsi Phase One P65+ ed un IQ180 (60MP ed 80MP rispettivamente), per cui sono questi i modelli dei quali ho esperienza diretta &#8211; lo stesso concetto si può applicare su ogni modello P1 che soffra del Paneling &#8220;bug&#8221; (ti faccio notare che scrivo: &#8220;bug&#8221;. Perché so che non è ufficialmente un bug &#8211; per quanto il fatto che C1 esporti un DNG non corretto a me pare un dettaglio non da poco; usare le virgolette mi sembra un equo compromesso).\n  \n\n  \n    Siccome il problema viene evidenziato nei canali &#8220;a&#8221; e &#8220;b&#8221; di Lab (più il primo, a cui corrisponde l&#8217;asse verde-magenta, che il secondo, blu-giallo: ovvero le componenti cromatiche dell&#8217;immagine) e non la &#8220;L&#8221;, è abbastanza facile trovare un escamotage processando il raw due volte e unendo i risultati come livelli in modalità di fusione Luminosità / Colore &#8211; nel modo seguente:\n  \n\n  \n    \n      \n        Apri il file .IIQ in ACR 7.0 (o qualunque altra versione di ACR che hai legalmente a disposizione).\n      \n    \n\n    \n      \n        Aggiusta tutti i parametri come credi &#8211; senza perdere tempo sul rumore cromatico (che andrà perso) &#8211; e aprilo in Photoshop come Oggetto Avanzato (shift-click sul pulsante Apri Immagine).\n      \n    \n  \n\n  \n    \n  \n\n  \n    \n      \n        Apri lo stesso .IIQ in Capture One.\n      \n    \n\n    \n      \n        In Capture One, cerca di ottenere un risultato cromaticamente simile (mimando le stesse curve, bilanciamento del bianco, ecc. impostate in ACR, fatta eccezione per la rimozione del rumore di luminosità e lo sharpening). Incrocia le dita e salva un TIFF con la stessa profondità di bit e profilo ICC di output usati in ACR.\n      \n    \n\n    \n      \n        Apri il TIFF in Photoshop, trascinagli sopra l&#8217;Oggetto Avanzato proveniente da ACR ed imposta la sua modalità di fusione in Luminosità. Se tieni premuto il tasto Shift durante il trascinamento, il livello si allineerà automaticamente al sottostante.\n      \n    \n\n    \n      \n        [Opzionale] Seleziona entrambi i livelli e convertili in un singolo Oggetto Avanzato (una buona idea se hai intenzione di correggere la prospettiva dello scatto).\n      \n    \n  \n\n  \n    In questo modo ti ritrovi con un documento a due livelli: il sottostante è l&#8217;interpretazione di Capture One, il superiore è la versione ACR in Luminosità. Ovvero: mantieni il colore di Capture One (profilo ICC di input, curve, correzione delle aberrazioni e riduzione del rumore cromatici, ecc.) e la luminosità di Adobe Camera Raw (con la sua riduzione del rumore, Clarity, recupero delle luci ed ombre, ecc.).\n  \n\n  \n    Puoi invertire l&#8217;ordine dei livelli (ACR sotto e C1 sopra, ma in modalità di fusione Colore), il risultato è il medesimo.\n  \n\n  \n    Se ti preme il color management, puoi essere molto soddisfatto di poter scegliere tra i profili ICC di input di Capture One; o molto insoddisfatto, perché è più complicato usare il ColorChecker Passport per profilare uno scatto in C1. In entrambi i casi, non preoccuparti troppo; non mi sono mai sembrate soluzioni particolarmente accurate. Del resto, come ha detto una volta Dan Margulis: &#8220;se un&#8217;immagine ha un problema, correggilo &#8211; è il tuo mestiere, che problema c&#8217;è&#8221;.\n  \n\n  \n    Conclusioni\n  \n\n  \n    Adobe Camera Raw 7.0 è un sostanziale passo avanti rispetto alle versioni 6.x. La sua usabilità è molto, molto superiore rispetto a Capture One, restituisce immagini di qualità eccezionale ed algoritmi decisamente più avanzati (sono in fremente attesa di vedere nella prossima release ufficiale l&#8217;applicazione della nuova correzione sulle aberrazioni cromatiche assiali). Eppure i raw provenienti dai dorsi digitali Phase One sono (non ufficialmente) non supportati a causa della mancata interpretazione dei dati di calibrazione del sensore incorporati negli .IIQ da parte di ACR (in qualsiasi sua versione fino alla corrente, la 7.0). Né i DNG esportati da Capture One vengono sottoposti ad alcuna normalizzazione (bug!). Quindi aprire un file raw Phase One in Adobe Camera Raw porta ad un&#8217;immagine che contiene una griglia, una scacchiera di dominanti incrociate &#8211; perlopiù sull&#8217;asse verde/magenta: in alcuni casi a prima vista invisibile (in altri molto ben presente da subito), che comunque è destinata a diventare molto evidente ed invasiva appena con la color correction vengono enfatizzati i colori.\n  \n\n  \n    Un espediente (in pia attesa che Capture One corregga l&#8217;esportazione dei DNG e/o Adobe supporti veramente i raw Phase One) che suggerisco è di processare il raw due volte (una con C1, una con ACR) unendo i due files in Photoshop come livelli in Colore e Luminosità. Non è una soluzione particolarmente elegante, ma non ho trovato alternative e funziona. Forse in questo modo si unisce il meglio dei due mondi. Forse! 😉\n  \n\n",
      tags: ["Adobe Camera Raw @it","Capture One @it","Dorso Digitale","IIQ","IQ180 @it","Oggetto Avanzato","P65+ @it","Phase One @it","raw"],
      id: 104
    });
    

    index.add({
      title: "Phase One digital backs and Adobe Camera Raw &#8211; Paneling &#8220;bug&#8221;",
      category: ["Photography Post-Production"],
      content: "\n  \n    \n\n    \n\n    \n    &nbsp;&lt;/p&gt;\n\n    \n      \n    \n\n    \n      If you&#8217;re a Phase One digital back owner (IQ and P+ series) you&#8217;ve been told that Phase One Capture One Pro is the main choice for raw processing. Finest quality, and above all they make the hardware, so they&#8217;re supposed to code the best software to drive it.\n    \n\n    \n      I&#8217;m afraid I don&#8217;t share the same confidence of Capture One passionate supporters: ever since Adobe distributed the new Adobe Camera Raw 7.0 (shipped with Photoshop CS6, which technology is embedded in Lightroom 4) we finally have an excellent alternative &#8211; I would say a far better choice under many key aspects. However, there&#8217;s a hidden, annoying pitfall that you must be aware of &#8211; friendly called the IIQ paneling &#8220;bug&#8221;.\n    \n\n    \n      \n    \n\n    \n      What is the part of &#8220;not officially supported&#8221; that you don&#8217;t understand?\n    \n\n    \n      That&#8217;s what I&#8217;ve been told once, and the man was right somehow. To open Phase One .IIQ raw files with Adobe Camera Raw (ACR from now on)is a bad thing to do, theoretically, because they were: &#8220;not officially supported&#8221;. What does this mean, it&#8217;s a fair question to ask, you may wonder, since ACR seems to read and open them flawlessly just like any other CR2, NEF, DNG file.\n    \n\n    \n      Wrong: it happens that .IIQ files contain sensor calibration data that is not correctly interpreted by ACR, or not interpreted at all. This data, as far as I can see, is used to balance the response of the sensor, which is geometrically divided into 8 sections (2 rows, 4 columns). Whether the sensor is controlled by different pieces of electronics, or it is made with smaller sensors glued together I don&#8217;t know and frankly I don&#8217;t care.\n    \n\n    \n      &nbsp;\n    \n\n    \n      \n\n      \n        PhaseOne P65+ test shot. Left: RGB version. Right: the &#8220;a&#8221; channel from Lab equalized shows paneling.\n      \n    \n\n    \n      &nbsp;\n    \n\n    \n      \n\n      \n        PhaseOne IQ180 test shot. Up: RGB version, weak green/magenta color casts in the asphalt. Bottom: the &#8220;a&#8221; channel from Lab equalized shows it more clearly\n      \n    \n\n    \n      The crucial fact is that if this calibration is lost (and ACR 7.0 and earlier versions lose it), the raw data will be translated into a faulty, demosaicized image. The differences between each zone can be easily boosted by the usual Photoshop color/contrast enhancing routines that are in your repertoire as a retoucher: and believe me, you won&#8217;t like to end up with an image which has both magenta and green casts displaced in a chessboard fashion: it&#8217;s bad times.\n    \n\n    \n      \n        Mind you: this problem doesn&#8217;t show up using Capture One, which reads and interprets correctly the proprietary tags embedded within the .IIQ files.\n      \n    \n\n    \n      Right now both Photoshop CS6 and Lightroom 4 officially support the whole range of Phase One backs yet the problem is there anyway. There&#8217;s an easy way to check whether your raw files are correctly managed by ACR, or they show a so-called paneling &#8220;bug&#8221;. It&#8217;s worth to try this one at least once on some of your files (a white wall test shot and a couple of real world pictures):\n    \n\n    \n      \n        Open the .IIQ file in Adobe Camera Raw, everything zeroed but the White Balance. Bit depth and RGB profile do not matter, your default values are fine.\n      \n      \n        In Photoshop, convert the file to the Lab colorspace (Image &#8211; Mode &#8211; Lab Color). You know Lab, don&#8217;t you?\n      \n      \n        In the Channels palette, make the &#8220;a&#8221; channel only active (Command + 4 on the Mac, CTRL + 4 for PCs, Photoshop CS5 to CS6)\n      \n      \n        Image &#8211; Adjustments &#8211; Equalize.\n      \n      \n        Make the &#8220;b&#8221; channel active (Command + 5 on the Mac, CTRL + 5 for PCs) and repeat the step #4.\n      \n    \n\n    \n      If either the &#8220;a&#8221; and/or the &#8220;b&#8221; channels show some evident grid, your files are unofficially unsupported in ACR! Sometimes the grid is not so obvious, yet you may find some oddities like the one below, which are bad times anyway.\n    \n\n    \n      &nbsp;\n    \n\n    \n      \n\n      \n        Phase One IQ180. Up: sky shot with noticeable color paneling (magenta cast in the left side). Bottom: the &#8220;a&#8221; channel from Lab equalized shows a weird paneling pattern\n      \n    \n\n    \n      &nbsp;\n    \n\n    \n      \n\n      \n        Unedited crop, click to enlarge\n      \n    \n\n    \n      As a side note, I know that the above mentioned moves are an extreme way to brutalize an image for testing purposes &#8211; yet Lab channel stretching is fairly used among retouchers, and the Equalize command is actually employed in the so-called Modern Man from Mars technique by Dan Margulis, a fundamental step in his Picture Postcard Workflow, aka PPW).\n    \n\n    \n      I&#8217;ve been personally reported by a Phase One user that the same paneling may appear even using Capture One as the one and only raw developer. That is, your own digital back&#8217;s internal calibration data doesn&#8217;t correspond to the actual sensor&#8217;s response no more, which sadly means: send it to Denmark for revision. (That is what he did and luckily the problem has been solved flawlessly)\n    \n\n    \n      Do open .IIQ files with Adobe Camera Raw anyway\n    \n\n    \n      I sincerely do not like Capture One Pro (also known as C1 Pro). It isn&#8217;t a polite thing to write in some forums (people seem to polarize when it comes to religion, food and raw processors), but nobody hears me now so I can tell you: it&#8217;s a bad piece of software, clumsy, so slow, involved, unnecessarily complex, UI and controls are badly designed (try to move a point on a channel&#8217;s curve while keeping self control), even from the sheer algorithms point of view ACR 7.0 surpasses it without any doubt. That&#8217;s my own, personal point of view of course &#8211; and I&#8217;m not a big Adobe&#8217;s fan, usually.\n    \n\n    \n      &nbsp;\n    \n\n    \n      \n\n      \n        Capture One curves dialog\n      \n    \n\n    \n      Mind you: C1 image quality is quite good (yet I still prefer ACR most of the times) but it comes with a price &#8211; way too high: in terms of usability, mainly. If I had no alternatives, I would patience and adapt to C1 cumbersome structure. But ACR 7.0 delivers a competitive, impressive image quality: noise reduction is superior, general usability is orders of magnitude superior, Local Laplacian filtering (the revised algorithm behind Clarity) is so nice, now that Adobe has implemented R, G, B single channel curves (it&#8217;s been a long wait: hurray!) there&#8217;s no reason not to use it.\n    \n\n    \n      So I do open .IIQ files with ACR (because I&#8217;m a retoucher, photographers are used to work with Lightroom: that, technically speaking, is the same engine within a different car). How?\n    \n\n    \n      Avoid DNG conversions\n    \n\n    \n      Do not try to export the .IIQ as DNG in CaptureOne (a task performed at unusual speed compared to C1 standards, I shall say). It&#8217;s been reported that earlier version of C1 messed with some metadata and filesize (inflated as much as the IIQ file&#8217;s double) &#8211; whether it&#8217;s been fixed or not (not so, at least for the filesize), even using an updated Capture One version those DNG are still not calibrated, so when you open them in ACR the very same paneling bug shows! This is a serious problem, that from my personal standpoint should be taken into account by Phase One: when users convert to a standard format like DNG, they should be given correct data. I can understand that some proprietary pieces of information may not fit into the DNG specs (the actual spots used by the camera to focus the image for instance, something you could get from Canon DPP as far as I remember), but here we talk about image channels that are intentionally left uncorrected!\n    \n\n    \n      Just in case you&#8217;re wondering, Adobe DNG converter couldn&#8217;t care less about Phase One sensor calibration data. The only workaround I came up with, well&#8230; it isn&#8217;t exciting, but it works, provided that you&#8217;re ready to lose a little something.\n    \n\n    \n      Using Layers and Blending Modes\n    \n\n    \n      I&#8217;ve had the opportunity to work extensively with two P65+ and one IQ180 Phase One digital back units (60 and 80 megapixels respectively) in the last three years, so these models are what I have a direct experience of; the following should apply to every P1 back that shows the paneling &#8220;bug&#8221; (please notice that I use quotes for I know it&#8217;s not officially a bug &#8211; yet the fact that a C1 exported DNG file shows paneling makes me call it bug anyway; surrounding the &#8220;bug&#8221; word with quotes sounds like a fair compromise to me).\n    \n\n    \n      Since the paneling affects &#8220;a&#8221; and &#8220;b&#8221; channels of Lab (more the former, green-magenta, and a bit less the latter, blue-yellow axis, i.e. the chromatic component of the image) and not the L, I came up with a decent workaround processing the raw twice and blend the results as a Luminosity and Color layers &#8211; as follows:\n    \n\n    \n      \n        Open the .IIQ file in ACR 7.0 (or any ACR version you legally own).\n      \n      \n        Tweak all the parameters to your taste &#8211; don&#8217;t spend too much time on chromatic noise reduction (it will be lost) &#8211; and open it in Photoshop as a Smart Object (shift-click on the Open Image button).\n      \n    \n\n    \n      \n    \n\n    \n      \n        Open the very same .IIQ file in Capture One.\n      \n      \n        In Capture One, mimic the curves, white balance and everything else you did in ACR except for luminosity noise reduction and sharpening, cross the fingers and save a TIFF (same bit depth and output ICC profile you used in ACR).\n      \n      \n        Open the TIFF in Photoshop, drag &amp; drop on it the Smart Object coming from ACR and set its blending mode as Luminosity. If you keep the Shift key pressed while doing the drag &amp; drop, the imported layer will automatically align.\n      \n      \n        [Optional] Select the two layers and convert them to a single Smart Object (good idea if you need to correct the perspective)\n      \n    \n\n    \n      So you end up with a two layers document: a background layer which is the Capture One interpretation of your file, and a upper layer which is the ACR version in Luminosity. That is: you use the color from Capture One (input ICC profile, curves, chromatic aberration correction and chromatic noise reduction, etc. etc) and the luminosity from Adobe Camera Raw (with its own noise reduction, Clarity, powerful shadows and highlights recovery, etc).\n    \n\n    \n      Please note that you can invert the layer&#8217;s order (the ACR below and the CaptureOne above, but in Color blending mode!), it gives the same net result.\n    \n\n    \n      If you are a color management geek, you may be either happy to choose from Capture One input ICC profiles; or unhappy because it&#8217;s harder to use your ColorChecker Passport as a profiling tool in C1. In both situations, don&#8217;t bother too much. I haven&#8217;t found either profiles as particularly accurate &#8211; and like Dan Margulis once said: if there&#8217;s a problem in a picture, just correct it and the problem disappears.\n    \n\n    \n      Conclusions\n    \n\n    \n      Adobe Camera Raw 7.0 is an impressive leap forward compared to 6.x version. Its usability is far, far higher compared to Capture One Pro, it delivers first class image quality and cutting edges algorithms. Yet many Phase One digital backs are unofficially unsupported because the sensor calibration data embedded in .IIQ files is not correctly interpreted by ACR; nor the DNGs exported from Capture One undergo any kind of normalization (bug!). As a result, to open a Phase One raw file in Adobe Camera Raw means ending up with an image that contains a geometric, chessboard pattern of opposite color casts, mainly in the green-magenta axis: an almost unnoticeable pattern (sometimes but not always), that may become more apparent easily if the retouch involves some color boosting steps.\n    \n\n    \n      As a workaround (waiting for Capture One to correct DNG exporting, and/or Adobe to fully support Phase One raw files) I suggest to process the raw twice (one with C1, one with ACR) and merge the two files in Photoshop as Color and Luminosity layers. It isn&#8217;t a particularly elegant solution, but I&#8217;ve found no alternatives and it works &#8211; possibly&#8230; it takes the best of two worlds. Possibly. 😉\n    \n  \n\n",
      tags: ["Adobe Camera Raw","bug","Capture One","IQ180","P65+","Phase One","Photoshop CS6"],
      id: 105
    });
    

    index.add({
      title: "iBooks Author &#8211; Compressione delle immagini",
      category: ["Digital Publishing @it","iBooksAuthor @it"],
      content: "\n  \n    \n  \n\n  \n    Se sei seriamente interessato alla qualità delle immagini per l&#8217;editoria elettronica, vuoi sapere quello che accade loro quando iBooks Author esporta un file .ibooks: non solo quale profilo ICC è meglio usare, che tipo di file (JPG e PNG), ma anche che genere di conversione, compressione JPG e ridimensionamento viene automaticamente applicato dal software alle immagini che finiscono su iPad. Ho condotto test specifici e alcuni risultati sono piuttosto sorprendenti. \n\n     Il formato degli iBook è proprietario, codificato da Apple. Non è uno standard (lamentano i sostenitori degli standard), ma è eccezionalmente versatile e semplice da realizzare, pur producendo risultati tutt&#8217;altro che amatoriali. Eppure iBooks Author, l&#8217;applicazione gratuita di Apple per comporre iBooks, non è priva di bug: alcuni effetti sono poco intuitivi (vedi il mio post precedente sulle immagini a pieno schermo e senza bordi), ma siamo solamente alla versione 1.1 e possiamo aspettarci numerosi ed interessanti aggiornamenti.\n  \n\n  \n    Formati di file, profili ICC e dimensioni\n  \n\n  \n    Per farla breve: usa JPG o PNG (che permette di usare la trasparenza), converti tutti in sRGB e tieni conto che praticamente tutte le immagini (salvo alcune eccezioni di cui ti dirò a breve) verranno ridimensionate a 2048 x 1536 pixel, la dimensione del nuovo iPad con retina display. Questo è quello che Apple raccomanda.\n  \n\n  \n    Come ispezionare file .iba e .ibook\n  \n\n  \n    iBooks Author salva i tuoi progetti come file .iba &#8211; mentre quando esporti per iPad viene prodotto un file .ibooks. Il pacchetto .iba contiene una copia di tutti gli elementi originali che hai importato, mentre nel pacchetto .ibooks le cose sono trasformate: ridimensionate, convertite, compresse. Entrambi possono essere ispezionati in questo modo:\n  \n\n  \n    \n      \n        Duplica il file .iba oppure .ibooks (giusto per sicurezza)\n      \n    \n\n    \n      \n        Rinominali in .zip\n      \n    \n\n    \n      \n        Scompattali in una cartella (attenzione: OSX non porta a termine questa operazione con un doppio click, io ho dovuto usare Stuffit Expander)\n      \n    \n  \n\n  \n    Ecco fatto. Come vedi, ottieni una struttura di cartelle che contengono file xhtml, css e tutti gli elementi necessari, comprese le immagini.\n  \n\n  \n    Comparazione: dimensione delle immagini\n  \n\n  \n    Per tutti i test che seguono, ho usato un&#8217;immagine di 2048 x 1536 pixel, salvati come JPG massima qualità, in sRGB. Quando importi un&#8217;immagine in iBooks Author puoi ridimensionarla e spostarla per costruire il design della pagina che hai in mente, e decidere se permetterne la visione a pieno schermo (per i dettagli vedi il mio post precedente).\n  \n\n  \n    Se l&#8217;immagine non è impostata per andare a pieno schermo, il pacchetto .ibooks conterrà una versione ridimensionata dell&#8217;immagine originale, corrispondente all&#8217;effettiva dimensione sulla pagina. Ovvero, se nel libro su iPad l&#8217;immagine (che originariamente è 2048 x 1536 px) occupa uno spazio di 300 x 400 px, verrà salvata nel file .ibooks come un JPG di 300 x 400 px.\n  \n\n  \n    Diversamente, se l&#8217;immagine è parte di un widget (Immagine oppure Galleria) le cose si complicano:\n  \n\n  \n    \n      Se hanno un contorno (il default è una linea continua di 1 pt) verranno ridimensionate a 2040 x 1530 px.\n    \n    \n      Se sono prive di contorno, la dimensione è di 2048 x 1536 px.\n    \n  \n\n  \n    Per cui applicare un contorno diminuisce l&#8217;effettiva dimensione delle immagini nel file .ibooks. Nota bene: l&#8217;opzione Full-screen only non ha nessun effetto, sull&#8217;iPad un widget immagine andrà comunque a pieno schermo quindi ti consiglio di tenerla disattivata. Inoltre un widget Immagine andrà al massimo ad una dimensione di 2008 x 1319 pixel (vedi qui i dettagli), mentre  per le Gallerie è consentita la visualizzazione a pieno schermo e senza bordi (2048 x 1536).\n  \n\n  \n    Come ultima opzione, se è parte di un widget Immagine Interattiva (ovvero non solo a pieno schermo, ma anche ingrandibile ed esplorabile), la dimensione finale dipenderà dalla percentuale di zoom &#8211; cioè puoi dare in pasto ad iBooks Author un file 5000 x 5000 px ed aspettarti di trovarlo negli assets del file .ibooks..\n  \n\n  \n    Comparazione: profili\n  \n\n  \n    \n\n    \n      Comparazione del gamut di iPad1/iPad2 ed il nuovo iPad © C. David Tobie\n    \n  \n\n  \n    Se ti interessi di Gestione Colore e non conosci ancora il blog di C. David Tobie (è Global Product Technology Manager &#8211; Imaging Color Solutions, Datacolor inc.) ti suggerisco di leggere almeno la sua serie di post sull&#8217;iPad (More answers about the new iPad and Color contiene i link a tutti gli articoli). E&#8217; molto competente, e puoi leggere contenuti originali che non troverai altrove.\n  \n\n  \n    I miei test hanno confermato che, sebbene entrambi i pacchetti .iba e .ibooks contengano immagini col medesimo tag sRGB (come l&#8217;originale), il file proveniente dall&#8217;esportazione in .ibooks appare visibilmente più chiaro. Segno che qualche tipo di conversione dietro le quinte viene effettuata dal software Apple. Ho misurato un ColorChecker sintetico, con questi risultati:\n  \n\n  \n    \n      circa 4 punti nella b di Lab nei blu e ciano (l&#8217;originale è più satura)\n    \n    \n      circa 1 punto nella a di Lab per gialli e arancioni\n    \n    \n      uno spostamento verso il verde nella base (per il JPG del file .ibooks)\n    \n  \n\n  \n    A sinistra l&#8217;originale, a destra il file processato nel .ibooks &#8211; la differenza è visivamente trascurabile. Niente da cui farsi ossessionare, anche se il ColorChecker di per sé non è molto significativo: con fotografie vere la versione .ibooks appare costantemente più chiara.\n  \n\n  \n    \n  \n\n  \n    Ancora: a sinistra l&#8217;originale, a destra il file proveniente dal pacchetto .ibooks &#8211; ora si vede la differenza! Per simulare l&#8217;effetto, aggiungi un livello di regolazione Curve all&#8217;originale ed alza il punto centrale della curva composita di circa +20 punti (input: 127, output: 147), questo è quello che vedresti più o meno nel file .ibooks.\n  \n\n  \n    Comparazione: compressione JPG\n  \n\n  \n    Ho importato in iBooks Author un JPG salvato per il wed in Photoshop CS6, massima qualità: quindi ho confrontato il JPG proveniente dagli assets del file .iba (identico all&#8217;originale) con quello proveniente dal file .ibooks. Come puoi vedere, gli artefatti di compressione sono molto evidenti nel file finale .ibooks: e non è strano, perché se il JPG del .iba pesa 1.2MB, quello del .ibooks è nemmeno la metà, 560KB.\n  \n\n  \n    Che tipo di compressione JPG è stata usata? Per scoprirlo, ho scritto un piccolo Javascript per Photoshop che salva per il web 101 JPG dallo stesso originale, con Qualità da 0 a 100. Credo di poter identificare la compressione .ibooks all&#8217;incirca poco sotto il livello 50: non è identica &#8211; ad esempio nella versione da .ibook c&#8217;è molto più dithering attorno ai bordi, il che mi fa pensare che sia stato usato qualche tipo di maschera per variare dinamicamente il livello di compressione &#8211; come si può anche fare in Photoshop con un canale alfa costruito appositamente, determinando il range massimo/minimo.\n  \n\n  \n    \n\n    \n      Sinistra, JPG importato originariamente (qualità 100); centro, JPG qualità 48; destra, il JPG contenuto nel file .ibooks (compressione JPG sconosciuta). Luminosità normalizzata. Clicca sull'immagine per ingrandire.\n    \n  \n\n  \n    Certo che, ingrandendo, il JPG del .ibooks sembra Frankenstein comparato all&#8217;immacolata bellezza dell&#8217;originale a massima qualità.\n  \n\n  \n    Conclusioni\n  \n\n  \n    Sii felice e produci i tuoi iBooks. Non usare niente di più di JPG a massima qualità, sRGB,  2048 x 1536 px (a meno che tu non preveda di usare il widget Immagine Interattiva, che può richiedere risoluzioni più alte). Non ti preoccupare delle conversioni occulte, non ci puoi fare niente &#8211; verifica le immagini su iPad ed eventualmente ricorreggile in Photoshop di conseguenza, se non ti piacciono. Si, certo: verranno ignobilmente compresse, e se le ispezioni al 400% vedrai un mare di orribili artefatti JPG, ma sai una cosa? Su un iPad con retina display non sono così evidenti (tantomeno su iPad di vecchie generazioni), possiamo vivere tranquilli.\n  \n\n  \n    E se per caso ti va di scompattare il file .ibooks, sostituire gli assets JPG con file di maggiore qualità, ricomprimere il pachetto e spedirlo ad Apple per l&#8217;approvazione sullo store online, be&#8217;, fammi sapere come va a finire! Non voglio essere il primo a fare un hack degli iBooks, rischiando di far arrabbiare la grande A 🙂\n  \n\n",
      tags: [],
      id: 106
    });
    

    index.add({
      title: "iBooks Author &#8211; Immagini a pieno schermo",
      category: ["Digital Publishing @it","iBooksAuthor @it"],
      content: "\n  \n    Una delle più interessanti (e ovvie) possibilità degli iBooks è l&#8217;ingrandimento delle immagini a pieno schermo, in modo che anche le più piccole illustrazioni a piè pagina possano essere apprezzate ad alta risoluzione. Peccato che esistano almeno tre modi diversi di farlo in iBooks Author, ognuno coi suoi pro e contro: perché pieno schermo e senza bordi non sono proprio sinonimi.\n  \n\n  \n    \n  \n\n  \n    \n  \n\n  \n    Immagini\n  \n\n  \n    Ti sarai accorto che non c&#8217;è una voce nei menu per le immagini. E&#8217; possibile trascinare direttamente nel documento un file JPG, o PNG se ti interessa la trasparenza, dal Finder o dal pannello Media, oppure Inserisci &#8211; Scegli&#8230; (Command Shift V). Come in Pages, puoi ridimensionare, spostare, ecc. In questo modo però un&#8217;immagine resta ancorata alla pagina e non potrà essere ingrandita su iPad, per quanto i tuoi utenti la sfreghino con le dita nel tentativo di azzeccare la gesture giusta. Devi trasformare l&#8217;immagine in un Widget.\n  \n\n  \n    Widget Immagine\n  \n\n  \n    In iBooks Author, seleziona l&#8217;immagine e apri il tab Widget del pannello Impostazioni (se non è visibile, clicca la &#8220;i&#8221; bianca su sfondo blu nella Toolbar).\n  \n\n  \n    Seleziona almeno il check del Titolo o della Caption (didascalia), oppure entrambi, in modo da trasformare l&#8217;immagine in un widget vero e proprio. Hai la possibilità di aggiungere un Label (etichetta) al titolo, scegliendo dal menu a discesa la definizione più corretta (Diagramma, Figura, Galleria, Illustrazione, Immagine, Interattiva, Video, Review), abilitando la numerazione automatica (ad esempio: Fig. 1.1).\n  \n\n  \n    Se non ti preme avere un titolo o una didascalia, cancella il testo presente nell&#8217;editor e lascia il campo vuoto. Nota bene: il Label non è vincolato al tipo di widget (è permesso, e non crea alcun problema, avere un&#8217;immagine che si chiama Video 1.0, ad esempio).\n  \n\n  \n    \n  \n\n  \n    Il tab Interazione (accanto a Layout) mostra un promettente checkbox Full-screen only. Ogni tutorial o articolo che ho trovato suggerisce di selezionarlo. Non fa nessuna differenza, anzi a dire il vero non ho capito a che serva, visto che le immagini sull&#8217;iPad vanno a pieno schermo con un doppio tap ugualmente:\n  \n\n  \n    \n  \n\n  \n    Eppure, sebbene l&#8217;immagine sia a pieno schermo, non è senza bordi, lasciando spazio ai testi. Se per qualche (buona) ragione vuoi che le tue immagini non siano mai ridimensionate in alcun modo, la dimensione massima ottenibile per questo tipo di visualizzazione è di 2008 x 1319 px, quindi in questo caso è inutile produrre immagini più grandi. Un bordo di almeno 20 px è sempre e comunque mantenuto, e l&#8217;immagine ha un passepartout bianco. Non ho trovato ancora il modo di cambiare il colore.\n  \n\n  \n    Widget Galleria\n  \n\n  \n    Una vera immagine a pieno schermo può essere ottenuta attraverso il widget Galleria. Clicca sul pulsante Widget nella toolbar e seleziona Galleria, oppure passa dal menu Inserisci &#8211; Widget &#8211; Galleria. Si tratta di uno slideshow, una proiezione di più immagini, rappresentata sulla pagina da un unico segnaposto (sia le icone che le didascalie personalizzate sono opzionali): con un doppio tocco si passa a pieno schermo (senza bordi), e scorrendo con un dito si naviga attraverso le immagini. È possibile realizzare una Galleria fatta di una sola ed unica immagine:\n  \n\n  \n    \n  \n\n  \n    Un tocco sull&#8217;immagine e sia titolo che didascalia scompaiono. Qual è il problema, dunque?\n  \n\n  \n    Non che il titolo sia Galleria e invece si tratti di un&#8217;immagine (puoi cambiarlo dal menu a discesa Label). Lo screenshot a sinistra viene da iBooks Author, e mostra due frecce grigie sotto l&#8217;immagine, per la navigazione. Non appaiono su iPad se la Galleria contiene una sola immagine, eppure tutto il widget mantiene quell&#8217;altezza, rovinando la composizione della pagina con un&#8217;incomprensibile spaziatura bianca in basso. A me infastidisce.\n  \n\n  \n    Se per te non è grave, hai trovato la soluzione migliore: un vero pieno schermo, senza bordi, con titolo e didascalia opzionale &#8211; e un piccolo difetto di design.\n  \n\n  \n    Widget Immagine Interattiva\n  \n\n  \n    Serve ad ingrandire l&#8217;immagine in punti predefiniti, che di solito contengono didascalie personalizzate: così come puoi aggiungere una Galleria di un&#8217;immagine sola, è possibile inserire una Immagine Interattiva senza alcuna vista extra né ingrandimenti. Aggiungi il widget e cancella le viste di default nel tab Widget del pannello Impostazioni. Questo è il risultato finale dell&#8217;ingrandimento su iPad:\n  \n\n  \n    \n  \n\n  \n    E&#8217; privo di titolo e didascalia &#8211; che potrebbero invece servirti.\n  \n\n  \n    \n  \n\n  \n    Eppure il problema più grave a mio parere è che, in iBooks Author, è praticamente impossibile ridimensionare l&#8217;immagine con lo slider per farla stare dentro al box del widget (o ridimensionare opportunamente pure quello) se non al quindicesimo tentativo e con molta fortuna, evitando che rimangano barre bianche ai bordi. Ti ricordo di cliccare sul pulsante blu (Set View nell&#8217;immagine) per confermare la scelta, altrimenti il widget tornerà al livello di zoom predefinito.\n  \n\n  \n    Ho lavorato con iBooks Author per alcuni giorni ormai, credo sia un ottimo software: per essere alla versione 1.1. Ha molti bug, il più noioso a mio parere è l&#8217;impossibilità di aggiungere immagini segnaposto nelle pagine mastro (AGGIORNAMENTO: per farlo, occorrono due passaggi, io saltavo il secondo. 1) Formato &#8211; Avanzato &#8211; Definisci come immagine segnaposto. 2) Nel pannello Impostazioni, scegli Impostazioni layout e sotto Oggetto layout seleziona il checkbox Modificabile su pagine che utilizzano questo layout).\n  \n\n  \n    Spero che le prossime versioni sistemino i vari problemi, ma soprattutto spero di vedere aggiornamenti frequenti da parte di Apple: iBooks Author può avere un brillante futuro!\n  \n\n  \n    &nbsp;\n  \n\n  \n    [Il copyright della fotografia è di Roberto Bigano]\n  \n\n",
      tags: [],
      id: 107
    });
    

    index.add({
      title: "iBooks Author &#8211; Image compression",
      category: ["Digital Publishing","iBooksAuthor"],
      content: "\n  \n    \n  \n\n  \n    If you&#8217;re serious about image quality, you want to know what happens to your pictures when iBooks Author exports a .iBook file: not only you&#8217;re interested in the ICC profile to use, the allowed filetypes (JPG and PNG), but also what kind of color conversion, JPG compression and resizing is applied to pictures on their way to the iPad. I&#8217;ve run some test and results are… somehow surprising.\n  \n\n  \n    \n  \n\n  \n    The iBook format is a proprietary flavor of e-books made by Apple. It&#8217;s not a standard (standards advocates argue), but it&#8217;s exceptionally versatile, easy to produce and fun. May I tell you? I love it. Yet iBooks Author, Apple&#8217;s free tool for authoring iBooks, is not bug-free; some actions aren&#8217;t straightforward (read my previous post about fullscreen, borderless images), but it&#8217;s only version 1.1 and we&#8217;re allowed to expect for it a bright future. Let&#8217;s start the analysis.\n  \n\n  \n    File Format, ICC profile and size\n  \n\n  \n    To make a long story short: use JPG or PNG (which support transparency), convert everything to sRGB, and be aware that all the images (with an exception, read along) will be resized to 2048 x 1536 pixels, the new retina display iPad size. This is what Apple recommends.\n  \n\n  \n    How to inspect .iba and .ibook files\n  \n\n  \n    iBooks Author saves projects as .iba files. When you export for the iPad, it outputs an .ibook file. The .iba package contains a copy of the original assets you&#8217;ve imported, while in the .ibook package things are transformed: resized, converted, compressed. Either ones can be inspected this way:\n  \n\n  \n    \n      Duplicate the .iba and/or the .ibook files (just to be safe…)\n    \n    \n      Rename them to .zip\n    \n    \n      Unzip them in a folder (be aware that OSX doesn&#8217;t expand them with a double click, I&#8217;ve got to use Stuffit Expander)\n    \n  \n\n  \n    That&#8217;s it. As you see, you end up with a folder structure with xhtml, css and all the needed assets.\n  \n\n  \n    Image dimension comparison\n  \n\n  \n    For all the following tests I&#8217;ve used a 2048 x 1536 pixel image, saved as JPG maximum quality, sRGB. When you import a picture in iBooks Author you resize and move it to fit the page&#8217;s design, deciding whether to allow the image to pop up fullscreen or not (more details about the three ways you can choose to do it in my previous post).\n  \n\n  \n    If the image is not allowed to go fullscreen, the .ibooks package will only contain an exact sized version of the original imported file: that is, if the page holds a 600 x 800 pixels JPG, the .ibook asset will be automatically rescaled as a 600 x 800 pixel JPG.\n  \n\n  \n    On the contrary, if the image is part of a widget (Image or Gallery) things get weird:\n  \n\n  \n    \n      Images with a stroke (default is a 1pt line) end up as a 2040 x 1530 px JPG.\n    \n    \n      Images without a stroke end up as a 2048 x 1536 px JPG.\n    \n  \n\n  \n    So applying a stroke actually shrink the .ibooks image dimension. Mind you, the Full-screen only option has no effect whatsoever on image dimension (on the iPad, a widget image will always go fullscreen), so I suggest you to keep it unchecked. Moreover, an Image widget will only expand fullscreen to maximum 2008 x 1319 pixels (see here for the details and examples), while Gallery images are allowed to go both fullscreen and borderless (2048 x 1536).\n  \n\n  \n    As a last option, if the image is part of an Interactive Image widget (i.e. not only it goes fullscreen, but you&#8217;re allowed to zoom in and explore preset details at higher magnification), the final size will depend on the zoom percentage &#8211; that is, you can feed iBooks Author with a 5000 x 5000 pixels image and expect to have it in the .ibooks assets.\n  \n\n  \n    Profile comparison\n  \n\n  \n    \n\n    \n      Gamut comparison of iPad1/iPad2 against the new iPad © C. David Tobie\n    \n  \n\n  \n    If you&#8217;re into color management and don&#8217;t know yet C. David Tobie&#8217;s blog (he&#8217;s Global Product Technology Manager &#8211; Imaging Color Solutions, Datacolor inc.) I strongly suggest you to read at least his post series about Color Management and the iPad (More answers about the new iPad and Color contains the links to them all). He&#8217;s very competent and you&#8217;ll read a lot of original content that you won&#8217;t find elsewhere.\n  \n\n  \n    To my tests I&#8217;ve confirmed that, even though both the .iba and .ibooks assets contain sRGB tagged images, the exported one appears lighter (hence some secret Apple processing has been going on). I&#8217;ve tested a synthetic ColorChecker image, measuring these differences:\n  \n\n  \n    \n      about 4 points in the b of Lab in blues and cyans (original has an higher saturation)\n    \n    \n      about 1 point in the a of Lab for yellow and orange\n    \n    \n      a shift towards green in the base color (.ibooks compressed JPG)\n    \n  \n\n  \n    Left is original, right is .ibooks asset &#8211; almost unnoticeable. Not something to be obsessed with &#8211; actually the ColorChecker comparison alone is not very much significative, if you test real world photographs the .ibooks JPG version appears constantly lighter.\n  \n\n  \n    \n  \n\n  \n    Again, left is original, right is .ibooks asset &#8211; now you see the difference! To simulate the effect, add a Curves adjustment layer to the original image and raise the middle point of the master curve about +20 points (input: 127, output: 147), this is more or less how the .ibooks file will look like.\n  \n\n  \n    Compression comparison\n  \n\n  \n    I&#8217;ve imported into iBooks Author a JPG saved for the web in Photoshop CS6, maximum quality: then I&#8217;ve checked the .ibooks JPG against the .iba (which is identical to the original). As you can see by yourself, there&#8217;s a lot of JPG compression and artifacts in the .ibooks file: and it isn&#8217;t shocking, because the .iba JPG is 1.2MB, while the .ibooks JPG is 560KB (less than half of the filesize!).\n  \n\n  \n    What kind of JPG compression has been used? To test it, I&#8217;ve put together a simple Photoshop script to save for the web 101 JPGs from the same original, with Quality ranging from 0 to 100. To the best of my eyesight, I&#8217;ve identified the compression just below 50: yet it isn&#8217;t identical &#8211; for instance, in the .ibook version there&#8217;s a lot more dithering around the edges, therefore that it may be that some quality filter based on image content has been used &#8211; just like when in Photoshop you set an alpha channel to determine the compression range.\n  \n\n  \n    \n\n    \n      Left: the original imported JPG (quality 100); middle, a JPG quality 48; right, the JPG from .ibooks assets (unknown JPG compression). Click the image to get a larger version.\n    \n  \n\n  \n    Yet, if you zoom in the .ibook image, it looks like Frankenstein compared to the immaculate beauty of the original, max quality JPG.\n  \n\n  \n    Conclusions\n  \n\n  \n    Be happy and build your iBooks. Stick to 2048 x 1536 pixels, sRGB, max quality JPGs (unless you plan to use Interactive Image widgets, that may require higher resolution images). Don&#8217;t worry about the conversion, there&#8217;s nothing you can do &#8211; proof your images on the iPad and tweak them in Photoshop accordingly, if they don&#8217;t look good to you. Yes, they&#8217;re horribly compressed and if you peep them 400% it&#8217;s like a sea of JPG artifacts, but you know what? On the iPad retina display they&#8217;re not that much obvious, you can live with them. And by the way you could always export the .ibooks, unzip it, substitute the JPGs with higher quality ones, zip it again, cross the fingers and submit the file for approval to Apple &#8211; if you succeed, please let me know! For sure I won&#8217;t be the first one to hack iBooks and risk to make the big A angry 😉\n  \n\n",
      tags: ["iBooks","image","iPad"],
      id: 108
    });
    

    index.add({
      title: "iBooks Author &#8211; Fullscreen images",
      category: ["Digital Publishing","iBooksAuthor"],
      content: "\n  \n     One of the most interesting yet obvious features of iBooks is the possibility to make an image to pop up fullscreen, so that any small illustration may be appreciated as big as the iPad display allows &#8211; and even more, zooming in. Alas, there are at least three ways to do this in iBooks Author, each one with its own drawbacks: because fullscreen and borderless aren&#8217;t synonyms.\n  \n\n  \n    \n  \n\n  \n    \n  \n\n  \n    Images\n  \n\n  \n    You&#8217;ve probably noticed that there&#8217;s no menu item for Insert &#8211; Image. You&#8217;re allowed to drag and drop a JPG or PNG (if the transparency matters to you) either from the Finder or the Media panel, or Choose… one (Command Shift V). Just like in Pages, you can resize it, move it, etc. Yet the picture won&#8217;t allow any kind of zoom/pinch: it will be a static image, merged within the background, when exported to the iPad. Users may tap or double tap on it without any effect. In order to add interactivity, the image has to be turned into a Widget.\n  \n\n  \n    Image Widget\n  \n\n  \n    Back in iBooks Author, select your image and open the Widget tab in the Inspector (if the Inspector is not visible as a floating palette, click on the white &#8220;i&#8221; on the blue circle icon, which is in the Toolbar).\n  \n\n  \n    Check either the Title or the Caption checkbox (or both) to transform the Image into an Image Widget. If you are willing to, you may Label the Title via its own drop-down menu and choose from a long list the right definition (Diagram, Figure, Gallery, Illustration, Image, Interactive, Movie, Review), enabling the automatic numbering (Fig 1.1 for instance).\n  \n\n  \n    If you&#8217;re not interesting in having a Title or a Caption, just delete the text in the editor and leave them blank. Mind you, the Label isn&#8217;t related to the kind of widget (you may have an Image Widget labeled Movie and everything will be ok anyway)\n  \n\n  \n    \n  \n\n  \n    The Interaction tab (just besides the Layout one) shows a promising Full-screen only checkbox. Each and every tutorial or post I&#8217;ve read suggest you to check it: do it, it makes no difference at all. Actually I&#8217;ve not understood what is it for, since it appears to have no effect in the exported iBook.\n  \n\n  \n    On the iPad the image will pop up fullscreen when double-tapped:\n  \n\n  \n    \n  \n\n  \n    Yet, you see that while the image is fullscreen, it&#8217;s not borderless. Actually, if for some reason you don&#8217;t want your image to be rescaled in any way, the maximum dimension it will have when displayed fullscreen are 2008 x 1319 px. A border of 20 px each side is always kept, and the image is put on a white background. I haven&#8217;t found a way to change its color yet.\n  \n\n  \n    Gallery widget\n  \n\n  \n    An actual fullscreen image can be obtained via the Gallery widget. Click on the Widget button on the toolbar and select Gallery, or from the menu Insert &#8211; Widget &#8211; Gallery. It&#8217;s a collection of pictures with one placeholder within the text (thumbnails for the other images are optional), that you can swipe with one finger either normal size or fullscreen. Images share the same title, while customized captions are optional. Sounds great right? You may build a Gallery with one image only, this is how it looks like (a true borderless fullscreen!)\n  \n\n  \n    \n  \n\n  \n    One tap on the image and both Title and Caption will disappear. So what&#8217;s wrong with the Gallery widget?\n  \n\n  \n    Not the fact that the title is Gallery even if it contains just one image (it can be changed via the Label drop-down menu). The screenshot at the left is from iBooks Author, and shows two grayed out arrows that are for navigating the pictures.\n  \n\n  \n    They don&#8217;t appear on the iPad if the Gallery contains a single image, yet the whole widget ends up having a wasted, empty, extra white space at the bottom that ruins the overall look of the page. At least to me, and I&#8217;ve got to live with me.\n  \n\n  \n    So far, this is the best solution to display a borderless fullscreen image with optional title and caption, but with one design drawback on the book page.\n  \n\n  \n    Interactive Image widget\n  \n\n  \n    This one is for zooming the image into predefined hot-spots, that usually contains textual captions: just like you can build an Gallery with a single image, you&#8217;re allowed to add an Interactive Image widget without any extra view or hot-spot. So add the widget and delete the views in the Widget tab of the Inspector panel. This is the fullscreen outcome:\n  \n\n  \n    \n  \n\n  \n    It shows no title and no caption &#8211; that may be useful to have.\n  \n\n  \n    \n  \n\n  \n    Yet the biggest issue to me is that, in iBooks Author, it&#8217;s almost impossible to make the small image to fit the widget&#8217;s box (tweaking the zoom), avoiding the ugly white bar to appear above and below the image. With the same picture I&#8217;ve been able to do it successfully once, then in order to grab a screenshot for this blogpost, there&#8217;s no way I could do it again. As a suggestion, remember to click the blue Set View button, otherwise the widget will zoom the image back to its default value.\n  \n\n  \n    I&#8217;ve been playing with iBooks Author for few days, and I admit it&#8217;s a beautiful piece of software, being at the version 1.1. It has a lot of bugs: the most annoying one, in my opinion, is the broken media placeholder (UPDATE: in order to successfully transform a picture/movie within a master page into a placeholder you must select it, then 1) Format &#8211; Advanced &#8211; Define as Media Placeholder. 2) From the Inspector panel, select the Layout inspector and below Layout Object select the Editable on pages using this layout checkbox. If you forget this checkbox, the placeholder won&#8217;t work).\n  \n\n  \n    I hope future releases will fix the issues, and above all, I hope we&#8217;ll see point updates to appear frequently &#8211; for iBooks Author is truly an amazing software!\n  \n\n  \n    &nbsp;\n  \n\n  \n    [The image is courtesy Roberto Bigano]\n  \n\n",
      tags: ["borderless","gallery widget","iBooks","image","image widget","interactive image widget"],
      id: 109
    });
    

    index.add({
      title: "Panoramiche col medio formato&#8230; lo stato dell&#8217;arte?",
      category: ["Post-produzione"],
      content: "\n  \n    Sono abbastanza fortunato da avere clienti che usano il Medio Formato (con dorsi digitali) per unire più scatti in panoramiche &#8211; e credimi, 20 o 40 files di un PhaseOne IQ180 (80MP) sono in grado di produrre risultati stupefacenti. Chiunque penserebbe che lo stato dell&#8217;arte della tecnologia produca immagini allo stato dell&#8217;arte: posso testimoniare che la realtà è alquanto diversa. Sembra strano che queste macchine, dorsi, teste panoramiche ed obbiettivi possano fallire così spesso, eppure nella fotografia panoramica è facilissimo sbagliare.\n  \n\n  \n    \n  \n\n  \n    \n  \n\n  \n    Dorsi Digitali / Camera\n  \n\n  \n    Il top di gamma per PhaseOne è la 645DF equipaggiata con il nuovo dorso IQ180 da 80MP. Ad ISO bassi (35/50) la gamma dinamica è stupefacente: al contrario già a 400 ISO è complicato distinguere il file del dorso dal JPG di una compatta fatta eccezione per le dimensioni. Vero, non usavi il negativo con la 20&#215;25 a mano libera, ma per 37K EUR (cioè trentasettemilaeuro iva esclusa) ti puoi sentire giustificato nell&#8217;aspettarti qualcosa in più. Comunque, tieni a mente che 35/50 ISO sono spettacolari, 100 ISO è assolutamente usabile.\n  \n\n  \n    \n  \n\n  \n    Obbiettivi a focale fissa\n  \n\n  \n    Per il sistema camera/dorso PhaseOne, gli obbiettivi allo stato dell&#8217;arte sono i leaf-shutter (ovvero ad otturatore centrale) della Schneider-Kreuznach (LS 55mm f/2.8, LS 80mm f/2.8, LS 110mm f/2.8 and LS 150mm f/3.5), dai 2K ai 3.5K EUR. Visto che ti interessa la fotografia panoramica, sappi che con la PhaseOne 645DF non è possibile alzare lo specchio ed aprire la tendina allo stesso tempo, in modo da usare solo l&#8217;otturatore centrale per scattare &#8211; minimizzando le vibrazioni. Invece al primo click si alza lo specchio, al secondo click (oppure usando il timer della macchina) si apre la tendina e scatta l&#8217;otturatore centrale. Secondo il supporto online di PhaseOne ciò è dovuto al modo in cui la macchina è stata progettata (ovvero: non c&#8217;è verso di fare altrimenti), e quindi occorre aspettarsi vibrazioni nei tempi tra 1/4 e 1/60 anche su cavalletto con una buona testa, a causa del vero e proprio colpo di frusta della tendina. Che, posso assicurare, c&#8217;è ed è piuttosto impressionante &#8211; la macchina vibra visibilmente! Ricorda che se vuoi usare l&#8217;IQ180 nel suo range migliore (35/50 o anche 100 ISO) a non sei sempre in Sicilia a mezzogiorno, è probabile che quelli siano proprio i tempi da usare con un&#8217;apertura decente: che, nella mia esperienza, su questo sistema non deve essere più chiusa di f/16, altrimenti entra in gioco la diffrazione. Vai a dire ad un fotografo di 50 anni che le fotografie ad 1/40 su cavalletto vengono mosse e ti prende per matto.\n  \n\n  \n    Cavalletto e Testa\n  \n\n  \n    Lo stesso customer support di PhaseOne, gentilmente e rapidamente (complimenti davvero!) suggerisce di usare l&#8217;Arca Swiss Cube come testa (circa 1K EUR) per minimizzare le vibrazioni. Un bell&#8217;oggetto, che viene venduto anche con una custodia in pelle inutilmente cara (+200 EUR) per i feticisti del cuoio. Fortunatamente, l&#8217;Arca Cube su un robusto Gitzo o Manfrotto (fino a 1K EUR) ma senza la colonna centrale (ovviamente, che domande), è in grado di restituire uno scatto senza vibrazioni anche ad 1/4 su cavalletto. Che per essere nel 2012 a fronte di più di quaranta mila Euro di spesa non è affatto male, vero?!\n  \n\n  \n    Testa Panoramica\n  \n\n  \n    E&#8217; ora di approcciare l&#8217;argomento. Una delle migliori scelte a mio parere (sia per la precisione, qualità di costruzione e robustezza) è la Really Right Stuff PG-02 Pro Omni Pivot Package (900 USD per un vero sistema 360°) che è compatibile con Arca Swiss. Il che significa che l&#8217;attacco rapido a L della camera entra tranquillamente nel clamp della testa. Potresti, tecnicamente parlando, attaccare la slitta orizzontale della Really Right Stuff all&#8217;attacco rapido dell&#8217;Arca Cube; ma questo ti porterebbe ad un sacco di guai: sai perchè?\n  \n\n  \n    Se ispezioni l&#8217;attacco rapido dell&#8217;Arca Cube, ti rendi conto che è fatto per alloggiare slitte di due dimensioni diverse (una più piccola, una più grande). Entrambe vengono fissate con successo, peccato però che con la misura più grande l&#8217;asse di rotazione sia spostato dal centro, perché l&#8217;attacco rapido non è simmetrico. Ovvero ciao-ciao punto nodale, e prova ad indovinare di che dimensione è la slitta orizzontale della Really Right Stuff? Esatto.\n  \n\n  \n    La soluzione? Smonta dal cavalletto senza colonna l&#8217;Arca Cube (1K EUR che tornano utile solo per gli scatti singoli) e monta il panning clamp della Really Right Stuff (la base circolare micrometrica) e vai avanti così. Attenzione, ti occorre la PG-02 Pro Omni Pivot Package e non la Ultimate Pro Omni Pivot Package (100 USD in meno) perché il braccio verticale della prima è più robusto: in questo modo sono stato in grado di evitare vibrazioni persino coi tempi incriminati e con un sistema corpo macchina / dorso / obbiettivo molto pesante. Il che almeno è una buona notizia.\n  \n\n  \n    Tilt / Shift\n  \n\n  \n    Se vieni dalla fotografia d&#8217;architettura, un obbiettivo molto attraente è lo Schneider-Kreuznach 120mm f/5.6 Tilt/Shift (3.5K EUR). E&#8217; davvero molto compatto, i movimenti vengono operati con le ghiere e senza manopole varie (come in altri obbiettivi T/S in commercio). Non è troppo luminoso, il decentramento è di soli 12 mm (non tanto) ma dai test che ho fatto è un buon obbiettivo, che restituisce un&#8217;ottima qualità di immagine.\n  \n\n  \n    Probabilmente sai già che per usare il decentramento a scopi di stitching di panoramiche è il dorso che deve muoversi, e non la lente &#8211; altrimenti si incorre in problemi di parallasse (vedi qui e un paio di soluzioni qui). Il che è decisamente infattibile, visto l&#8217;attacco sul 120 ed il peso di corpo macchina e dorso che graverebbero proprio sul movimento dell&#8217;obbiettivo.\n  \n\n  \n    Personalmente, ho pensato ad un setup diverso: ho montato un L-plate sulla PhaseOne, fissandola (non importa con quale orientamento) sulla slitta orizzontale della Really Right Stuff. Trovando il punto nodale (come al solito), avrei scattato un panorama orizzontale &#8211; una riga di immagini parzialmente sovrapposte. Poi avrei ripreso una seconda riga decentrando in alto, ed una terza riga decentrando in basso. Sarebbe forse più veloce che non usare il setup a 360° della Really Right Stuff, e sicuramente più stabile &#8211; visto il peso del 120 T/S e della macchina (la 645DF, IQ180 e lo Schneider-Kreuznach 80mm già arrivano al limite di capacità della testa).\n  \n\n  \n    \n  \n\n  \n    Peccato che non funzioni. Ho fatto due test (in esterno ed in interno, con la macchina in orizzontale ed in verticale): se per la riga centrale senza decentramento gli errori sono nella norma (1.6px con un 80MP sono niente), nelle righe superiori ed inferiori gli errori sono troppo alti a confronto, e causano problemi di stitching (ho usato PTGui Pro per il test). Dal che si evince che il decentramento sposta il punto nodale. Ah, e per la cronaca il decentramento verso il basso è di 10 mm soltanto, perché l&#8217;obbiettivo sbatte contro la slitta sulla quale è montata la macchina.\n  \n\n  \n    Detto ciò, se ti capita di scattare panorami ad alta risoluzione, con dorsi medio formato, a bassi ISO e senza vibrazioni, sono veramente fenomenali! Per la post-produzione, conta pure su di me. Per qualsiasi problema discusso in questo post, porta pazienza, è solo di denaro&#8230; 😉\n  \n\n",
      tags: [],
      id: 110
    });
    

    index.add({
      title: "Medium format Stitching, what a tough business!",
      category: ["Photography Post-Production"],
      content: "\n  \n    I&#8217;m lucky enough, as a post-producer and consultant, to be paid by photographers who want to use Medium Format (i.e. Digital Backs) to stitch panoramas. And believe me, 20 or 40 shots from a PhaseOne IQ180 (80MP) result in a gorgeous one. They think, as anyone would, that expensive state-of-the-art gear will lead to state-of-the-art photography: alas, I&#8217;ve personally witnessed that this is absolutely not the case. It&#8217;s rather unbelievable that such heads, backs and lenses fail epically when used improperly &#8211; which is fairly easy to do.\n  \n\n  \n    \n  \n\n  \n    \n  \n\n  \n    Digital Back / Camera system\n  \n\n  \n    PhaseOne bleeding edge is the 645DF camera equipped with an IQ180 digital back, 80MP. At low ISO (35 and 50) the dynamic range is astonishing, while if you go up to 400 ISO you can&#8217;t tell apart the PhaseOne file and a compact camera jpg but for the megapixel count. True, you didn&#8217;t shoot negatives hand held with a 8&#8217;x10&#8242; technical camera, but for 45K USD you&#8217;re allowed to expect a little bit more quality from the system. Anyway, keep in mind that 35/50 ISO are spectacular, and 100 ISO is fairly usable as well.\n  \n\n  \n    \n  \n\n  \n    Prime lenses\n  \n\n  \n    Now, it happens that PhaseOne bleeding edge lenses are the leaf shutter ones made by Schneider-Kreuznach (LS 55mm f/2.8, LS 80mm f/2.8, LS 110mm f/2.8 and LS 150mm f/3.5) priced in the 3K/5K USD range. If you&#8217;re into panorama stitching, please note that the PhaseOne 645DF camera doesn&#8217;t allow mirror lockup and open curtain at the same time, to let you use the lens&#8217; leaf shutter only: so that you click, and get no vibrations. Instead, the mirror can lock up as expected, but when you press the release button the second time (or use the camera&#8217;s timer) both the curtain and the leaf shutter go. According to PhaseOne customer support this is due to the way the camera has been engineered, and you have to expect vibrations in the 1/4 to 1/60 range even on a robust tripod with a solid head, due to the curtain&#8217;s whiplash. I can confirm: it&#8217;s impressive! Remember that if you wish to use the IQ180 bst ISO range (35/50) and you&#8217;re not always at noon near the equator, chances are that these are just the most common shutter speeds with a decent aperture (which, in my experience, should never be higher that f/16 otherwise diffraction starts to degrade image quality &#8211; with that lenses, on that digital back at least). Go tell a 50 years old photographer that 1/40 on a tripod gives motion blur in his pictures and he&#8217;d commit you to a mental asylum.\n  \n\n  \n    Tripod and head\n  \n\n  \n    The very same PhaseOne customer support, kindly and quickly (hat off to them, really!) suggested to use the Arca Swiss Cube head (about 1.7K USD) to minimize vibrations. A sturdy piece of a head, nice looking object that may be shipped with an overpriced case for those who are into leather fetish (+240 USD). Luckily, the Arca Cube on a robust Gitzo or Manfrotto tripod (about 1K USD) without the central column (of course&#8230; we&#8217;re serious shooters 🙂 ) gives you a single shot without any vibration even at 1/4. Big deal, being in 2012!\n  \n\n  \n    Pano head\n  \n\n  \n    Now the time has come for you to approach pano shooting. One of the better choices in my opinion (either for the precision, quality and sturdiness) is the Really Right Stuff PG-02 Pro Omni Pivot Package (900 USD for a true 360° system) which is Arca Swiss compatible &#8211; that is, the camera L-plate fits into the head&#8217;s clamp, the base rail too. You can, technically, fasten the Really Right Stuff horizontal rail to the Arca Cube quick release clamp: alas this will lead to unsuccessful stitching. Why?\n  \n\n  \n    If you inspect the Arca Cube clamp you&#8217;ll notice that it accommodates two different rail sizes. Either the small and the large will fit, alas with the larger one the rotation axis of the head is offset, because the clamp is not symmetric. Which means bye-bye nodal point, and guess what size the Really Right Stuff base rail is? Yep.\n  \n\n  \n    As a solution, forget and unscrew the 1.7K USD  Arca Cube (which is good for single shots only) from the column-less Gitzo tripod, mount the Really Right Stuff panning clamp and go ahead. Mind you, you need the  PG-02 Pro Omni Pivot Package and not the Ultimate Pro Omni Pivot Package (100 USD cheaper) because the vertical arm of the former is more robust: this way I&#8217;ve been able to test no vibration at all, even with the evil shutter speeds and such a heavy camera/back/lens system. That&#8217;s a good news.\n  \n\n  \n    Tilt / Shift pano\n  \n\n  \n    If you have an architecture photography background, a very good and appealing lens is the Schneider-Kreuznach 120mm f/5.6 Tilt/Shift lens (5K USD). I admit it is a very compact lens, with movements operated by rings and not knobs like other T/S lenses: it isn&#8217;t the fastest around, the shift is only 12mm (not that much) yet it seems to me a really good lens, delivering very good quality.\n  \n\n  \n    You may know that when using a T/S lens for panoramic purposes, it&#8217;s the back that should move and not the lens &#8211; otherwise you&#8217;ll bump into parallax issues (see here and a couple of workarounds here).\n  \n\n  \n    I&#8217;ve thought about a different setup: I would have used an L-plate for the PhaseOne camera, mounting it no matter whether portrait or landscape on the Really Right Stuff horizontal rail. Finding the nodal point, I would be shooting an horizontal panorama (one row of images), shifting the lens upwards and downwards to mimic a second upper row and a third lower row. Possibly quicker than using the full Really Right Stuff 360° setup (involving also the vertical arm and a second rail) and surely more stable: the Schneider-Kreuznach T/S is quite heavy and lens, camera and digital back would surely overtake the weight limit for the pano head.\n  \n\n  \n    \n  \n\n  \n    Alas, it does not work either (this post&#8217;s refrain). I&#8217;ve run two tests, indoor and outdoor: while mean error is quite low for the middle row, shifting the lens seems to cause stitching issues (I&#8217;ve used PTGui Pro for the test) so we must conclude the nodal point changes from row to row. Good idea, bad result. Oh, and by the way the downward shift is limited to 10mm and not 12mm, for the lens hits the Really Right Stuff rail.\n  \n\n  \n    That said, if you happen to shoot high resolution, medium format, low ISO rock steady panoramas, well, they&#8217;re really amazing, wonderful! As a post-producer, I&#8217;ll be happy to put my hands on them. For every issue covered in this post, keep calm and carry on, it&#8217;s just money 😉\n  \n\n",
      tags: ["Arca Swiss","Cube","Large Format","Panoramic head","Phase One","PTGui","Really Right Stuff","Stitching"],
      id: 111
    });
    

    index.add({
      title: "Media di più livelli in Photoshop",
      category: ["Photoshop @it"],
      content: "\n  \n    \n\n    \n      Tre versioni di una singola immagine, da sinistra: MO, DB, GA.\n    \n  \n\n  \n    In Correzione Colore si è recentemente scoperto che produrre più versioni di una singola immagine allo scopo di unirle, fonderle in modo opportuno, è una strategia eccezionalmente efficace. Il che non esclude che l&#8217;autore della correzione debba essere una persona sola, come il mio caro amico Marco Olivotto ha scritto in un bel manifesto sulla Correzione Colore collaborativa intitolato Tre teste sono meglio di una.\n  \n\n  \n    Sebbene maschere e modalità di fusione siano all&#8217;ordine del giorno, il blend più semplice (33-33-33 oppure 25-25-25-25) può non essere così intuitivo da ottenere, a meno che tu non sappia come fare!\n  \n\n  \n    \n  \n\n  \n    Il blend 50-50 è banale: il primo livello in basso va lasciato al 100% di opacità, quello superiore al 50%. Le cose sono diverse per fondere tre livelli, 33-33-33. Saresti forse tentato di impostare la base al 100% e i due livelli sopra al 33% (o forse entrambi al 50%) &#8211; ma non è la strada corretta.\n  \n\n  \n    Supponiamo di avere appunto tre livelli da fondere, e di voler dare ad ognuno di essi il medesimo peso.\n  \n\n  \n    Oggetti Avanzati e Stack\n  \n\n  \n    Un metodo è selezionare tutti i tre livelli, creare un Oggetto Avanzato che li contenga, e poi scegliere il menu Livello &gt; Oggetti avanzati &gt; Metodo serie immagini &gt; Media (in inglese: Layer &gt; Smart Objects &gt; Stack Mode &gt; Mean). Questa è senza dubbio la via più precisa, anche se gli Oggetti Avanzati possono essere mortalmente lenti/pesanti da gestire, e richiedono una versione Extended di Photoshop.\n  \n\n  \n    Sequenza di Opacità\n  \n\n  \n    La strada più veloce (perché siamo sempre di fretta, o magari vogliamo solo vedere se il blend funziona o no nell&#8217;economia della correzione) è impostare le varie opacità.\n  \n\n  \n    \n  \n\n  \n    Non è così ovvio come assegnarle però. Nel caso di tre livelli soltanto i numeri sono (dal basso verso l&#8217;alto) 100%, 50%, 33% come indicato nello screenshot a sinistra &#8211; e la formula molto semplice da usare è:\n  \n\n  \n    OPACITÀ = 100 / POSIZIONE\n  \n\n  \n    Ovvero, il livello di sfondo è in posizione uno, per cui la sua opacità è 100/1 = 100. Il secondo livello è in posizione due, per cui 100/2 = 50. E quindi il terzo livello, sopra gli altri, in posizione tre ha un&#8217;opacità di 100/3 = 33.\n  \n\n  \n    Per risparmiarti la fatica, la progressione delle opacità per 10 livelli è: 100, 50, 33, 25, 20, 17, 14, (12), 11, 10.\n  \n\n  \n    In verità il 12 dovrebbe essere 13 (perché 100/8 = 12,5), ma non fa una gran differenza. In ogni modo: se hai bisogno di precisione matematica e hai da fondere otto livelli e più, usa il metodo degli Oggetti Avanzati; altrimenti, la scala delle opacità è un ottimo compromesso!\n  \n\n  \n    Giusto un piccolo suggerimento che può tornare utile di tanto in tanto. E se vuoi sapere come Marco ha unito le tre versioni, leggi il post originale e scopri perché 1+1+1 &gt; 3.\n  \n\n  \n    \n  \n\n  \n    &nbsp;\n  \n\n",
      tags: [],
      id: 112
    });
    

    index.add({
      title: "How to stack layers in Photoshop as a Mean",
      category: ["Photoshop"],
      content: "\n  \n    \n\n    \n      Three versions of a single image stitched together. Left to right: MO, DB, GA.\n    \n  \n\n  \n    In Color Correction it is a wise strategy to produce multiple versions of a single image (that is: to correct the same picture few times collecting different variations), then blend them together in a number of ways &#8211; to get the better of each one. This doesn&#8217;t necessarily involve that it must be the same person the one in charge, as my dear friend Marco Olivotto stated in a beautiful manifesto on collaborative color correction called Three Heads are Better than One.\n  \n\n  \n    Although masks and blending modes may be required, the easiest blends of all (33-33-33 or maybe 25-25-25-25) can be difficult to obtain, unless you know the trick!\n  \n\n  \n    \n  \n\n  \n    The 50-50 blend of two layers is a piece of cake: you set the bottom to 100% opacity and the top to 50%. It&#8217;s a bit different if you&#8217;d like to blend three layers 33-33-33. You&#8217;d be tempted to set the background to 100% and the two above to 33% (or 50% perhaps?) &#8211; but you&#8217;d be wrong!\n  \n\n  \n    Let&#8217;s say you&#8217;ve three versions to blend (i.e. three layers) and you want to give the same weight to everyone.\n  \n\n  \n    Smart Objects and Stack Mode\n  \n\n  \n    One method is to select them all, create a Smart Object containing the three layers, then (if you have Photoshop Extended), select in the menu Layer &gt; Smart Objects &gt; Stack Mode &gt; Mean. This is doubtless a neat way, yet you may find this workflow a bit tedious (S.O. are a pain of slowness and add unnecessary complication for this task). And you need to have an Extended version of PS.\n  \n\n  \n    Opacity ramp\n  \n\n  \n    The fastest way (because we&#8217;re always in hurry and/or we may just want to give the blend a quick try) is to set each layer opacity.\n  \n\n  \n    \n  \n\n  \n    It&#8217;s not that obvious what percentage to assign to them. For three layers the numbers (bottom up) are 100%, 50%, 33% like in the screenshot at left &#8211; and the very easy general formula to use is\n  \n\n  \n    OPACITY = 100 / LAYER POSITION\n  \n\n  \n    That is, the background layer (at the very bottom) is in position one, so its opacity is 100/1 = 100. The second layer, above it, is in position two, so the opacity is 100/2 = 50. And the third, in position three as the topmost layer has an opacity of 100/3 = 33.\n  \n\n  \n    To save you some time, the ramp for 10 layers goes like that: 100, 50, 33, 25, 20, 17, 14, (12), 11, 10.\n  \n\n  \n    Actually 12 should be 13 (since 100/8 = 12.5), but I&#8217;ve chosen 12 to keep the slope smoother (up to you). Anyway: if you need mathematic precision and the layers&#8217; number you&#8217;ve got to deal with is about 8 or more, go with the Smart Object way; otherwise opacity it&#8217;s a fair compromise between speed and precision!\n  \n\n  \n    Just a tip that may come handy from time to time. And if you want to know how Marco blended the three versions to reach the final picture, go read the original post and find out why 1+1+1 &gt; 3.\n  \n\n  \n    \n  \n\n  \n    &nbsp;\n  \n\n",
      tags: ["layers","Mean","Photoshop @en","smart object","stack"],
      id: 113
    });
    

    index.add({
      title: "Photoshop Touch per iPad",
      category: ["Photoshop @it"],
      content: "\n  \n    \n  \n\n  \n    Photoshop Touch per iPad è finalmente uscito.\n  \n\n  \n    E ora Adobe, cortesemente, potresti far evolvere Photoshop per Mac/Win in un software leggermente più contemporaneo?\n  \n\n  \n    Grazie in anticipo. Distinti saluti,\n  \n\n  \n    Davide Barranca\n  \n\n",
      tags: ["Adobe","iPad","Photoshop"],
      id: 114
    });
    

    index.add({
      title: "Photoshop Touch for iPad",
      category: ["Photoshop"],
      content: "\n  \n    \n  \n\n  \n    Photoshop Touch for iPad is finally out.\n  \n\n  \n    And now please Adobe, could you gently make Photoshop for Mac/Win to evolve in a software just a little bit more contemporary?\n  \n\n  \n    Thanks in advance. Yours truly\n  \n\n  \n    Davide Barranca\n  \n\n",
      tags: ["Adobe","iPad"],
      id: 115
    });
    

    index.add({
      title: "Adobe Configurator 3, prerelease aperta",
      category: ["Photoshop @it"],
      content: "\n  \n    \n  \n\n  \n    In questo post, Jonathan Ferman (Creative Suite Solutions Product Manager di Adobe), annuncia la disponibilità per il download della prima versione beta di Adobe Configurator 3 in diversi programmi di prerelease (Photoshop, InDesign, Extension Builder). Se non appartieni a nessuno di questi ma ti interesserebbe provarlo, Jonathan stesso ti invita a metterti in contatto con lui.\n  \n\n",
      tags: ["Adobe Prerelease Programs","configurator","Creative Suite"],
      id: 116
    });
    

    index.add({
      title: "Adobe Configurator 3 prerelease open!",
      category: ["Photoshop"],
      content: "\n  \n    \n  \n\n  \n    According to this post by Jonathan Ferman (Creative Suite Solutions Product Manager at Adobe), engineers are at work on the next Adobe Configurator 3.0 version. First drop is available for download in several prerelease programs (Photoshop, InDesign, Extension Builder) &#8211; if you wish to try it, you should get in touch with Jonathan, he wrote.\n  \n\n",
      tags: ["Adobe Configurator","Adobe Prerelease Programs","Creative Suite"],
      id: 117
    });
    

    index.add({
      title: "PP nella Fotografia d&#8217;Arte Contemporanea (#1: Dettagli)",
      category: ["Post-produzione"],
      content: "\n  \n    Fotografia d&#8217;Arte Contemporanea significa, tra le altre cose, che il lavoro del tuo cliente l&#8217;Artista finirà stampato, montato e incorniciato in gallerie e musei, forse in tutto il mondo. Il trend corrente è la stampa GRANDE, come ad esempio quelle di Candida Höfer: non è insolito vedere opere impressionanti di 100 3/4 x 81 pollici (256 x 206 cm circa, ci dev&#8217;essere qualcosa di diabolico nei numeri pari: 100 x 80 pollici è sicuramente molto più volgare).\n  \n\n  \n    Quindi, come professionista in questo particolare business, il mio lavoro è sempre rivolto alla stampa GRANDE: cioè una spasmodica attenzione ai dettagli &#8211; perché il diavolo abita lì in mezzo, e siamo pieni di amici che gli fanno visita volentieri.\n  \n\n  \n    Negli ultimi anni ho visto il trend delle opere GRANDI stabilizzarsi &#8211; il che è una vera maledizione per chiunque sia coinvolto nella loro produzione: dall&#8217;Artista (costretto a trasportare tra un aeroporto e l&#8217;altro attrezzatura costosissima), al montatore (pannelli di alluminio, D-bond o foam-board più difficili da maneggiare), il corniciaio (plexiglass più spessi e pesanti), il corriere (grossi, grassi safe-box da spedire dall&#8217;altra parte dell&#8217;oceano).\n  \n\n  \n    Quando poi Giove va in trigono con Saturno e il lavoro di ognuno è fatto correttamente &#8211; ad esempio il montatore non distrugge una stampa (succede più spesso di quanto dovrebbe) &#8211; il nostro cliente l&#8217;Artista incontra amici e colleghi all&#8217;opening della mostra. Amici e colleghi che cominciano a perlustrare le opere ad una distanza (citando il compianto Bruce Fraser) largamente determinata dalla lunghezza del loro naso. Mi dicono che curatori, galleristi e critici di solito non prestano attenzione ad aspetti tecnici di nostra competenza, mentre lo sport preferito di amici e colleghi è la meticolosa ispezione dell&#8217;immagine, della stampa, della cornice, dell&#8217;illuminazione, (del buffet) per esprimere commenti con apparente cognizione di causa. Come:\n  \n\n  \n    \n      \n        &#8220;Troppo digitale.&#8221; (digitale cosa, non è dato sapere)\n      \n    \n\n    \n      \n        &#8220;Photoshop!&#8221; (il più grande oltraggio al pudore)\n      \n    \n\n    \n      \n        &#8220;Troppa maschera di contrasto&#8221;\n      \n    \n\n    \n      \n        &#8220;Troppa grana&#8221;\n      \n    \n\n    \n      \n        &#8220;Il bordo bianco non è abbastanza bianco/largo/stretto&#8221;\n      \n    \n\n    \n      \n        &#8220;La texture della carta è&#8230; troppa&#8221;\n      \n    \n\n    \n      \n        &#8220;Lo sponsor è troppo in evidenza&#8221;\n      \n    \n  \n\n  \n    Eccetera eccetera.\n  \n\n  \n    La mia impressione personale è che a volte si attacchino ai dettagli perché non colgono il quadro generale (ovvero il concetto dietro all&#8217;immagine), ma ho sentito così tanti commenti bizzarri che non presto loro più di tanta attenzione ormai. Eppure, in questo piccolo mondo, anche le critiche più balzane non possono essere completamente ignorate dall&#8217;Artista: che in (e di) quel piccolo mondo vive, sicché (per tornare in argomento) nel mio lavoro di Post-Produzione mi sono allenato a giocare in difesa.\n  \n\n  \n    Ammetto che per me non è una grande forzatura (non sono mai stato un giocatore d&#8217;attacco nemmeno negli scacchi): l&#8217;idea fondamentale è che nella Fotografia d&#8217;Arte Contemporanea ogni passaggio della post produzione deve essere inserito nella prospettiva di una stampa GRANDE, anche mentre prepari un file 20x25cm per un catalogo (se hai fortuna, prima o poi dovrai cavarci fuori una stampa GRANDE). Ogni correzione di contrasto porta, entro certi limiti, a problemi di rumore (che sia cromatico o grana), le correzioni di prospettiva possono esagerare la caduta di nitidezza degli obbiettivi, la griglia di Bayer e l&#8217;assenza di filtro anti-aliasing nei dorsi digitali ad alta risoluzione garantiscono arcobaleni di moirée, ecc. ecc. Che di per sé possono non essere grossi problemi se stampi in piccolo formato (lato lungo sotto al metro), o per un libro. Ma diventano difetti inaccettabili su una stampa inkjet 160 x 210 cm, che verrà esposta oltreoceano e ispezionata da branchi di amici.\n  \n\n  \n    Per necessità, sono stato forzato a sviluppare un&#8217;insana attenzione a certi dettagli, e purtroppo spesso non esistono soluzioni ai problemi di cui sopra che non contemplino una buona dose di lavoro manuale. Per sopravvivere, il grado di fanatismo del tuo cliente presto diventerà il tuo, e ti scoprirai a pennellare via su un livello in modalità Colore aloni verde e magenta sui riflessi (a seconda che siano davanti o dietro al punto di fuoco), o spuntinare penosamente polvere, graffi, depositi di calcare, macchie di sviluppo, impronte digitali (devo continuare?) che disturbano l&#8217;uniformità della grana in una scansione di originali di grande formato.\n  \n\n  \n    \n\n    \n      Nino Migliori - Il Tuffatore, 1951\n    \n  \n\n  \n    Bizzarra attività che, a seconda della tua attitudine, può essere accolta come una pratica zen, oppure una punizione selvaggia, una sorta di rito di passaggio: poco dopo aver fondato Fineartprint.it (assieme al mio collega e caro amico Roberto Legnani) avemmo modo di lavorare moltissimo in scansione e post-produzione sull&#8217;archivio di alcuni importanti autori degli anni &#8217;50 (come Nino Migliori e Piergiorgio Branzi) e ricordo ancora intere settimane spese a ritoccare vecchi negativi a pieno schermo &#8211; come visitare la superficie lunare; negativi passati da troppe incaute mani negli anni.\n  \n\n  \n    Tra le mie competenze c&#8217;è anche la stampa digitale fineart (ovvero inkjet di largo formato), così negli anni ho avuto il dubbio onore di confrontarmi col servizio tecnico Epson ed Hahnemuhle. Ho notato che in genere molte risorse sono spese nella cattura dell&#8217;immagine (cioè volare dall&#8217;altra parte del globo in remote locations con attrezzature costose) mentre spesso non viene prestata la medesima, equa, attenzione nel processo di produzione dell&#8217;opera (l&#8217;oggetto). Il che è ben strano. Ho visto coi miei occhi stampe GRANDI con banding scuro evidente (problemi di avanzamento carta), banding chiaro (ugelli otturati o allineamento delle testine non corretto), o montaggi di stampe con palesi problemi alla carta (che anche nelle più costose e teoricamente prestigiose carte a base cotone &#8211; come quelle Hahnemuhle &#8211; capitano con fetente regolarità).\n  \n\n  \n    Sapendo cosa costa il montaggio, cornice e trasporto, mi lascia perplesso vedere un approccio così rilassato alla produzione. Al meglio delle mie capacità (che in questo caso non si estendono granché) mi devo occupare anche di vero ritocco &#8211; con pennelli e acrilici intendo, per salvare una stampa dagli svariati problemi che la carta può presentare. A proposito dei quali ho raccolto negli anni una divertente casistica, che va dai punti di colla (puntini scuri di forma irregolare tra la carta base e lo strato di coating), alle impurità inglobate nella pasta della carta base (piccoli solidi che rigonfiano la superficie stampata), fino a peli rosso vivo &#8211; come diavolo possano finire intrappolati sotto l&#8217;ultimo strato di coating, non ho idea.\n  \n\n  \n    Per riassumere: quello che in alcuni lavori commerciali può essere visto come un&#8217;insana e maniacale attenzione al dettaglio, diventa una necessaria abitudine per chi si occupa di post produzione e stampa nella Fotografia d&#8217;Arte Contemporanea.\n  \n\n  \n    L&#8217;affascinante processo della produzione di un&#8217;opera d&#8217;arte (dal concetto all&#8217;oggetto) è la somma di molti, diversi passaggi che devono essere tenuti al più alto livello possibile; e in quanto responsabile di alcuni importanti aspetti tecnici del lavoro, devi tenerne conto.\n  \n\n  \n    Oppure, se preferisci vederla al rovescio: non facendo sempre del tuo meglio in post produzione, e lasciando correre qualche piccola imperfezione&#8230; alla fine queste si sommeranno alle altre: non l&#8217;idea più brillante, non la fotocamera giusta, non l&#8217;obbiettivo più adatto (ok, almeno per questi non è colpa tua), alcune aberrazioni cromatiche ai bordi, la prospettiva corretta così così, un ritocco tirato via, la stampa con micro-banding, la carta con qualche puntino ed una pieghetta, il montaggio con un rigonfiamento, il plexi sporco.\n  \n\n  \n    Il mondo contemporaneo ha proprio bisogno di opere così, non ne abbiamo già abbastanza?\n  \n\n  \n    [L&#8217;immagine di apertura è la mappa di ritocco (cioè la registrazione delle pennellate di ritocco) di un negativo 6&#215;12, acquisito con uno scanner Imacon-Hasselblad, risultante in un file 1&#215;2 metri @200 ppi, circa 16.000px lato lungo]\n  \n\n",
      tags: [],
      id: 118
    });
    

    index.add({
      title: "ALCE con panoramiche 360°, un tutorial",
      category: ["Photoshop @it","Post-produzione"],
      content: "\n  \n    \n\n    \n      Cappella di Somma, chiesa di San Giovanni a Carbonara (Napoli) © Alfonso Grotta - www.alfonsogrotta.com\n    \n  \n\n  \n    Ho appena pubblicato un tutorial sull&#8217;uso di ALCE (Advanced Local Contrast Enhancer for Photoshop) con immagini equirettangolari, quelle usate per realizzare panoramiche sferiche (tour virtuali a 360°). Alcuni suggerimenti per evitare problemi ai bordi, che devono potersi fondere senza disparità evidenti. Lo puoi leggere qui, nel blog RBG (Roberto Bigano Group), dove scrivo di tanto in tanto con Marco Olivotto e Giuliana Abbiati.\n  \n\n",
      tags: [],
      id: 119
    });
    

    index.add({
      title: "ALCE and 360° panoramic images, a tutorial",
      category: ["Photography Post-Production","Photoshop"],
      content: "\n  \n    \n\n    \n      Somma chapel, San Giovanni a Carbonara church (Naples, Italy) Photography © Alfonso Grotta &#8211; www.alfonsogrotta.com\n    \n  \n\n  \n    I&#8217;ve just posted a tutorial describing my suggested workflow for ALCE (Advanced Local Contrast Enhancer for Photoshop) on equirectangular images, the ones used to build those immersive, 360° virtual tours (a couple of useful recommendations to avoid issues in the edges, that must blend seamlessly). You can read it here, in the RBG blog (Roberto Bigano Group), where I use to post from time to time.\n  \n\n",
      tags: ["360","ALCE","equirectangular","panorama","tutorial","virtual tour"],
      id: 120
    });
    

    index.add({
      title: "PP in the Contemporary Art Photography business (#1: Details)",
      category: ["Photography Post-Production"],
      content: "\n  \n    Contemporary Art Photography means, among the rest, that the work of your client the Artist will end up printed, framed and exhibited in galleries and museums, luckily all over the world. Current trend is to print BIG, like Candida Höfer: not uncommon to see impressively sized 100 3/4 x 81 inch artworks (there must be something evil in even digits: 100 x 80 inch for sure sounds too cheap).\n  \n\n  \n    Anyway, as a Post-Producer in this particular business, my job has always been targeted to BIG prints: that is, an awful amount of attention to details &#8211; for the devil lives there, and we&#8217;re always full of friends that like to pay him a visit.\n  \n\n  \n    In the last few years I&#8217;ve seen the trend of BIG artworks stabilize &#8211; which is a true pain for everyone involved in the making of: starting from the Artist (who needs to carry around utterly expensive gear in airports), to the mounter (larger D-bond / aluminium / foam boards more difficult to manage), the framer (thicker, heavier plexiglass), the courier (bigger fatter safe-boxes to transport to the other side of the ocean).\n  \n\n  \n    When stars align properly and each one&#8217;s job is done right &#8211; i.e. the mounter doesn&#8217;t screw up the print while mounting it (which happens more often than it should) &#8211; our fellow artists and friends gather and start looking at the work of art at a distance (quoting the latest Bruce Fraser) largely determined by their nose&#8217;s length at the exhibition opening. I&#8217;m told that curators, gallerists and critics usually don&#8217;t pay attention to some aspect we&#8217;re highly concerned about, for their interest lies elsewhere; while colleagues&#8217; favorite sport is to meticulously inspect the image, then the print, the frame, the lights, (and the buffet) in order to express apparently well informed criticisms. Such as:\n  \n\n  \n    \n      \n        &#8220;Too much digital&#8221; (digital what, we&#8217;re not allowed to know)\n      \n    \n\n    \n      \n        &#8220;Photoshop!&#8221; (which of course is outrageous)\n      \n    \n\n    \n      \n        &#8220;Too much unsharp mask&#8221;\n      \n    \n\n    \n      \n        &#8220;Too much grain&#8221;\n      \n    \n\n    \n      \n        &#8220;The white border isn&#8217;t wide/white enough&#8221;\n      \n    \n\n    \n      \n        &#8220;The paper texture is too much textured&#8221;\n      \n    \n\n    \n      \n        &#8220;The sponsor is too much prominent&#8221;\n      \n    \n  \n\n  \n    Etc. etc.\n  \n\n  \n    My own impression is that sometimes they stick to details because they miss the BIG picture (that is: the concept behind the image), but I&#8217;ve heard so many funny commentaries that I tend not to pay attention to them anymore. Nonetheless, other&#8217;s opinions in this small world still matter to the Artist in the business who lives on it, so (back in topic) as a Post-Producer I trained myself to play in defence.\n  \n\n  \n    I admit this approach finds a good ground in my personal attitudes (I&#8217;ve never been an attack player even in chess) &#8211; but dealing with Contemporary Art Photography I&#8217;ve learned that you must always frame each post-production step in the perspective of a BIG print, even when preparing a file for a 8&#215;10 reproduction on a catalog (hopefully, sooner or later you&#8217;ll end up with a BIG print of it). Each contrast enhancement leads to some extent to noise problems (whether luminosity grain or chromatic noise), perspective distortion may exaggerate lens&#8217; corners focus fallout, sensor&#8217;s bayer grid in high resolution digital backs can paint weird moirée patterns, etc.; which aren&#8217;t a big deal by themselves if you print small (say, under 44 inches long side, 110 cm) or for a book. But become intolerable on a 64 x 82 inches (160 x 210 cm) inkjet print which will be exhibited abroad and inspected by a herd of friends.\n  \n\n  \n    As a necessity, I&#8217;ve been forced to develop an insane attention to small details and alas, there&#8217;s no ready made solution to the above mentioned problems except manual work. In order to survive your client&#8217;s degree of fanatism will soon become yours, and you&#8217;ll find yourself painting out on a Color layer green and magenta halos on specular highlights (depending on their position compared to the focus point, behind or ahead), or painfully remove dust, scratches (limestone deposits, development stains, fingerprints fat) that disturb the uniformity of the grain on a large format scan.\n  \n\n  \n    \n\n    \n      Nino Migliori - The Diver, 1951\n    \n  \n\n  \n    Weird activity that, depending on your attitude, may be embraced as a zen practice &#8211; or a wild punishment, kind of a rite of passage: when I started Fineartprint.it (my very first own company with the friend and photographer Roberto Legnani) we did a huge lot of scans and postproduction work for italian neorealism-years photographers (such as Nino Migliori and Piergiorgio Branzi) and I remember vividly entire weeks spent retouching vintage images fullscreen &#8211; I was like visiting the moon&#8217;s landscape; images whose B/W negatives did pass through too many hands over the years.\n  \n\n  \n    It happens that my job involves also digital fine art printing (aka large format inkjet), so I&#8217;ve had the dubious pleasure to deal with Epson and Hahnemuhle tech support. Strangely enough, it happens frequently that so much effort is spent on image capturing (i.e. flying to the other side of the world in remote locations with expensive gears), and frequently not a comparable, fair amount of attention is paid to the object production process &#8211; which is kind of bizarre. I&#8217;ve seen with my very own eyes big prints with evident dark banding (paper feed issue), light banding (clogged nozzles and/or head misalignment) or mounted images with evident paper flaws (that even in the most expensive rag based papers happens regularly).\n  \n\n  \n    Knowing how much they charge for mounting, framing and shipping, to see such a casual approach to printing still puzzles me. To the best of my skills (which in this case don&#8217;t go too far) I also do actual retouching &#8211; I mean, with brush and paint, to mitigate paper problems such as small rag paper base issues or impurity that may lay down between the paper base and the coating. Speaking of which, I keep a record of cases that includes, besides the usual black dots, even (bright) red curly hairs &#8211; how on earth do they happen to get stuck between the paper and the last coating layer I don&#8217;t know.\n  \n\n  \n    To summarize, what in some commercial work may be seen as an utterly insane maniacal attention to detail, it becomes for the Contemporary Art Photography Post-Producer (and printer) a good habit. The fascinating process of artwork production (from the concept to the framed print) is a sum of many, different steps that must be kept at their top level for the artwork to be called this way: and as the one in charge for such important technical aspects of the entire job you should be aware of this idea. Or, if you prefer to see it in reverse: if you don&#8217;t always do your best, agreeing to small imperfections, they will eventually sum up&#8230; not the brightest idea, not the right camera, not the fastest lens (ok, at least on these first three ones they can&#8217;t put the blame on you!), few chromatic aberrations left, the perspective not exactly corrected, a so-and-so retouching, the print with some micro-banding, the paper with a couple of bumps, the mounting with a bulge, the plexi with some dust. Does the contemporary world really needs more of that, aren&#8217;t we filled up enough?\n  \n\n  \n    [The opening picture is the retouch map (i.e. a description of retouching strokes) of a 6&#215;12 negative, acquired with an Imacon-Hasselblad scanner, resulting in a 1&#215;2 meters @200 ppi file, about 16.000px long side]\n  \n\n",
      tags: ["Contemporary Art","Photography Post-Production","Post Production","print"],
      id: 121
    });
    

    index.add({
      title: "Post-Produzione nel business della Fotografia d&#8217;Arte Contemporanea (Intro)",
      category: ["Post-produzione"],
      content: "\n  \n    Il mio lavoro principale è la Post-Produzione, in gran parte per un artista nel business della Fotografia d&#8217;Arte Contemporanea. Bizzarra nicchia, sotto molti aspetti &#8211; posso permettermi di dire dopo aver frequentato l&#8217;ambiente per diversi anni. E che richiede capacità particolari sia sotto l&#8217;aspetto comunicativo che tecnico, ho appurato passando attraverso altri progetti &#8220;normali&#8221; (perlopiù di prestampa per case editrici specializzate in pubblicazioni d&#8217;arte di lusso).\n  \n\n  \n    Siccome in questi mesi ho deciso di lavorare meno e stare più vicino alla mia famiglia (che sta attraversando un momento un po&#8217; complicato per ragioni di salute), ho dovuto mettere in stand-by gran parte dei progetti non assolutamente necessari. Ne approfitto però per raccogliere in una serie di post il mio punto di vista sul lavoro di Post-Produzione in questa curiosa, affascinante e folle nicchia. Stay tuned!\n  \n\n",
      tags: [],
      id: 122
    });
    

    index.add({
      title: "Post-Production in the Contemporary Art Photography business (Intro)",
      category: ["Photography Post-Production"],
      content: "\n  \n    My fulltime job is Post-Production, most of the time for an artist in the Contemporary Art Photography business. Weird niche in many ways, must I say after several years spent working in the field. I&#8217;ve noticed, switching in and out different, collateral projects (mainly pre-press consultancy for art publishing companies) that it requires really peculiar skills, both in the communication and technical sides.\n  \n\n  \n    In the last few weeks I came up with the decision to work less and stay closer to my family (facing a tough period due to some health issues), so I&#8217;ve had to switch in stand-by many secondary projects: it&#8217;s probably a good time to start a post series on my thoughts about the Post Producer perspective in the curious, fascinating and definitely foolish business of Contemporary Art Photography. Stay tuned!\n  \n\n",
      tags: ["Contemporary Art","Photography Post-Production","Post Production"],
      id: 123
    });
    

    index.add({
      title: "Subclipse Version Control in Flash Builder: installazione e setup",
      category: ["Coding @it"],
      content: "\n  \n    &#8220;Salva con Nome&#8221; è la più semplice ed usata forma di Version Control &#8211; ovvero la gestione delle revisioni di un documento. Tipo:\n  \n\n  \n    \n      BlogPost.txt\n    \n    \n      BlogPost_V1.txt\n    \n    \n      BlogPost_V2_Links.txt\n    \n    \n      BlogPost_Final.txt\n    \n    \n      BlogPost_Final_OK.txt\n    \n  \n\n  \n    Eccetera. Siccome lo screenshot a destra è una veritiera rappresentazione della mia cartella di sviluppo per ALCE, ho deciso che è giunta l&#8217;ora di adottare una qualche forma di controllo versione, ovvero: Subclipse (un plugin per Flash Builder 4.6 che gestisce il supporto a Subversion).\n  \n\n  \n    Sto ancora imparando: in questo post ho raccolto una serie di informazioni che spero possano servire da riferimento per l&#8217;installazione ed il setup di un repository locale (cioè non su server) a chi, come me, sta giusto cominciando a mettere un po&#8217; di ordine nel caos.\n  \n\n  \n    \n  \n\n  \n    Subversion e Subclipse\n  \n\n  \n    Se l&#8217;argomento ti è nuovo, ti consiglio di leggere questa chiarissima Visual Guide to Version Control System (VCS). Spiega tutti i concetti del version control e la sua terminologia.\n  \n\n  \n    \n      In pratica, nel paradigma centralizzato (opposto a quello distribuito), definisci un Repository: una cartella (locale o su un server remoto) che contiene una copia di riferimento del tuo codice. Puoi fare Check Out (scaricarlo) sulla tua macchina per sviluppare; poi Sincronizzi col Repository e infine fai Check In (anche detto Commit), ovvero carichi nel repository la tua versione. Il sistema traccia i cambiamenti (come somma di differenze), definisce le revisioni, funge da backup; in più, puoi ramificare lo sviluppo (Branch) ovvero duplicare la versione principale del codice (Trunk) per sperimentare nuove implementazioni &#8211; per poi eventualmente riunirla (Merge). Più una lunga serie di altre funzioni il cui senso ancora mi sfugge.\n    \n  \n\n  \n    Subversion è un comune sistema di Version Control, conosciuto anche come svn (il tool a riga di comando). Se vuoi sentire parlar male di Subversion, Linus Torvalds è un&#8217;ottima fonte. Se ti stai chiedendo perché scegliere Subclipse e non Subversive come plugin per Eclipse (e quindi Flash Builder), vale la pena leggere questa Q&amp;A.\n  \n\n  \n    Installazione\n  \n\n  \n    In Flash Builder (io uso la versione 4.6 ma sono propenso a credere che valga anche per la 4.5 e 4.0), vai al menu:\n  \n\n  \n    Help &#8211; Eclipse Marketplace\n  \n\n  \n    E nella finestra seguente cerca &#8220;Subclipse&#8221;:\n  \n\n  \n    \n  \n\n  \n    Clicca il pulsante Install e riavvia Flash Builder. Il che pare sufficiente per considerare Subclipse installato.\n  \n\n  \n    Setup di un Repository locale\n  \n\n  \n    Molti dei tutorial che ho letto a questo punto ti mostrano come creare un repo (conosciamo il gergo, eh? ;-)) su un server remoto. Peccato che a me non serva, né ho la più pallida idea di come impostare un server remoto. Tutto quello che mi occorre è una cartella sul mio disco. Dopo un paio di tentativi finiti male (uno dei quali basato su MAMP), ho capito che il processo è:\n  \n\n  \n    \n      \n        Identificare una cartella e dire a svn (non Subclipse: svn, il tool a riga di comando) di creare un repository.\n      \n    \n\n    \n      \n        Puntare Subclipse a quello.\n      \n    \n  \n\n  \n    Il che ha senso &#8211; ma senza queste informazioni ti trovi a fissare Flash Builder (come ho fatto io) pensando: e ora che faccio?\n  \n\n  \n    Creare un repository\n  \n\n  \n    Subclipse non può creare un repository. Ma non è un grosso problema.\n  \n\n  \n    Poniamo di volere la cartella &#8220;repo&#8221; (che conterrà i repository) nella Home &#8211; userò prima Finder per creare &#8220;repo&#8221; come una cartella qualsiasi. Poi, nel Terminale:\n  \n\n  \n    \n  \n\n  \n    Occorrono un paio di comandi:\n  \n\n  cd repo\n\n  \n    Il primo entra nella cartella &#8220;repo&#8221; appena creata\n  \n\n  svnadmin create ALCE_repo\n\n  \n    Il secondo dice a svn di creare, nella cartella &#8220;repo&#8221;, un vero repository chiamato &#8220;ALCE_repo&#8221; (che conterrà il mio progetto ALCE).\n  \n\n  \n    \n\n    \n      Questo è un repository\n    \n  \n\n  \n    Infatti &#8220;repo&#8221; conterrà i vari repository, uno per ogni progetto che voglio monitorare. Nello screenshot puoi vedere che un repository è ben più di una semplice cartella.\n  \n\n  \n    Riempire un repository\n  \n\n  \n    Abbiamo un repo nuovo di zecca: il secondo passo è riempirlo con un progetto di Flash Builder (una CS extension, o qualsiasi altro codice Flash/AIR).\n  \n\n  \n    \n\n    \n      SVN Perspective\n    \n  \n\n  \n    In Flash Builder, dal menu Window &#8211; Open Perspective &#8211; Other&#8230; scegli SVN Repository Exploring.\n  \n\n  \n    Questa vista ti da accesso al pannello SVN Repositories, che dovrebbe essere vuoto. Per comunicare a Flash Builder l&#8217;esistenza del repository, fai click destro dentro al pannello (nello spazio bianco) e seleziona New &#8211; Repository Location&#8230; e quindi nella finestra seguente digita:\n  \n\n  file:///Users/[your user]/repo/ALCE_repo\n\n  \n    \n  \n\n  \n    Nota bene che &#8220;file:///&#8221; ha tre slash.\n  \n\n  \n    \n  \n\n  \n    Una nuova location dovrebbe essere creata, come vedi nello screenshot a destra &#8211; ovvero Flash Builder ora sa che esiste un repository.\n  \n\n  \n    Il prossimo ed ultimo passo è di collegare un progetto a questo repo. I comandi Subclipse nel Flash Builder IDE stanno nel menu Team. Per accedervi, torna alla vista standard (Flex o Extension Builder), e nel pannello Package Explorer fai click destro sul progetto che ti interessa aggiungere al repository, quindi seleziona:\n  \n\n  \n    Team &#8211; Share\n  \n\n  \n    Nella finestra seguente scegli SVN come tipo di repository:\n  \n\n  \n    \n  \n\n  \n    quindi Use existing repository location:\n  \n\n  \n    \n  \n\n  \n    ed infine Use project name as folder name:\n  \n\n  \n    \n  \n\n  \n    Non ti preoccupare se la Console riporterà un messaggio di errore:\n  \n\n  Filesystem has no item svn: URL 'file:///Users/Davide/repo/ALCE_repo/myTest' non-existent in that revision\n\n  \n    Puoi tranquillamente ignorarlo (il comando successivo, mkdir, creerà la cartella myTest mancante). Secondo la documentazione, a questo punto il progetto sarà committed &#8211; ovvero: i files copiati nel repository.\n  \n\n  \n    Per ragioni che mi sfuggono, a me questo non succede, devo farlo manualmente: click destro sul progetto (myTest in questo caso, vedi lo screenshot sulla destra) nel pannello Package Explorer, scegliendo:\n  \n\n  \n    Team &#8211; Commit\n  \n\n  \n    Nella finestra che segue, puoi selezionare quali risorse copiare nel repository &#8211; aggiungendo anche un commento:\n  \n\n  \n    \n  \n\n  \n    E&#8217; tutto! Se il processo è andato a buon fine, la Console dovrebbe contenere alcuni messaggi di log.\n  \n\n  \n    Inoltre, vai nel pannello SVN repositories e clicca sul pulsante di refresh (le due frecce gialle) per aggiornare la vista: il repository ora contiene la cartella myTest e tutti i file del progetto.\n  \n\n  \n    Adesso puoi cominciare ad usare il Version Control System &#8211; scrivi il tuo codice e fai Check in (Commit) ogni volta che desideri definire una nuova revisione &#8211; che poi troverai nel pannello History.\n  \n\n  \n    Spero che queste informazioni ti possano far risparmiare un po&#8217; di tempo! Nel prossimo post della serie, mi soffermerò più approfonditamente (ehi, sto ancora imparando!) su Tag, Branches, revisioni, ecc. Quindi&#8230; a presto!\n  \n\n  \n    Links\n  \n\n  \n    Cercando di capire come funziona Subclipse e un VCS, ho letto parecchio materiale &#8211; parte del quale può tornare utile anche a te:\n  \n\n  \n    \n      Version Control with Subversion, pdf del manuale di Subversion (pubblicato anche da O&#8217;Reilly); esiste anche la versione in italiano, ma apparentemente meno aggiornata.\n    \n    \n      Guy Rutemberg blog, Creating Local SVN Repository (con il tool a riga di comando svn).\n    \n    \n      Documentazione IBM: Subversion and Eclipse.\n    \n    \n      North Carolina State University, seminario Subclipse for Configuration Management.\n    \n  \n\n",
      tags: [],
      id: 124
    });
    

    index.add({
      title: "Version Control in Flash Builder: installation and setup",
      category: ["Coding"],
      content: "\n  \n    &#8220;Save As&#8221; is the very first, basic, form of Version Control &#8211; the business of tracking and/or reverting the changes you made to (code, images, files) over time. You know:\n  \n\n  \n    \n      BlogPost.txt\n    \n    \n      BlogPost_V1.txt\n    \n    \n      BlogPost_V2_Links.txt\n    \n    \n      BlogPost_Final.txt\n    \n    \n      BlogPost_Final_OK.txt\n    \n  \n\n  \n    Etc. The image at the right is a screenshot of my actual ALCE development main folder, so I&#8217;ve decided to adopt some kind of Version Control, namely: Subclipse (a Subversion support provider plugin) for Flash Builder 4.6.\n  \n\n  \n    Since I&#8217;m still learning, I will show here my findings from the point of view of a single developer, hoping that this first post (about Installation and setup of a local repository) will save some time to those who, like me, are beginning to put some order to the chaos.\n  \n\n  \n    \n  \n\n  \n    Subversion and Subclipse\n  \n\n  \n    First if you&#8217;re new to the topic (as I am), please have a look to this well done and super clear Visual Guide to Version Control System (VCS). It explains the concept of revision control and its terminology.\n  \n\n  \n    \n      Basically, in the centralized (and not distributed) paradigm you set a Repository: i.e. a folder (either local or on a remote server) that contains a reference copy of your code. You Check Out (download it) on your machine in order to be able to edit/change it; you Synchronize with the Repository and then Commit (or Check In, i.e. upload) the changes. The system keeps track of the changes (as a sum of differences), sets revision numbers, can backup and restore; moreover, you may Branch the code &#8211; that is: duplicate the Trunk to experiment new implementations &#8211; and eventually Merge them back on the main line. Plus a whole bunch of advanced features that I don&#8217;t understand yet.\n    \n  \n\n  \n    A very common Version Control System is Subversion, also known as svn (the command line tool). If you wanna hear bad bad things about Subversion, Linus Torvalds is a great source. And if you wonder why choose Subclipse and not Subversive as an Eclipse (and Flash Builder) plugin, there&#8217;s a Q&amp;A worth reading here.\n  \n\n  \n    Installation\n  \n\n  \n    In Flash Builder (I use 4.6 but afaik the same applies for 4.5 and 4.0), go to the menu:\n  \n\n  \n    Help &#8211; Eclipse Marketplace\n  \n\n  \n    and in the following window search for &#8220;Subclipse&#8221;:\n  \n\n  \n    \n  \n\n  \n    Click the Install button and restart Flash Builder. It seems enough to consider Subclipse installed.\n  \n\n  \n    Setup of a Local Repository\n  \n\n  \n    Many of the tutorials I&#8217;ve read at this point show you how to setup a repo (we know the lingo now ;-)) on a remote server. Thanks, but I don&#8217;t collaborate with people who are in need to sync, and I don&#8217;t even know how to setup a remote server to put my small repo into. I just need a folder on my local disk. After a couple of unsuccessful tries (one of which involved MAMP), I&#8217;ve understood that the process is:\n  \n\n  \n    \n      Locate a folder and tell svn (not Subclipse: svn, the command line tool) to create a new repository.\n    \n    \n      Point Subclipse to it.\n    \n  \n\n  \n    It makes sense &#8211; but if you&#8217;re new to Version Control Systems you&#8217;d stare at Flash Builder (as I did) thinking: how on earth I&#8217;m supposed to do it?\n  \n\n  \n    Create a repository\n  \n\n  \n    First, Subclipse cannot create a repository.\n  \n\n  \n    Let&#8217;s say that I want my &#8220;repo&#8221; folder (that will contain repositories) to be in my Home &#8211; I&#8217;ll use Finder to create that &#8220;repo&#8221; new folder as I would do for any other folder I need. Then, I must open the Terminal:\n  \n\n  \n    \n  \n\n  \n    And type a couple of commands:\n  \n\n  cd repo\n\n  \n    This first line is to enter into the &#8220;repo&#8221; folder I&#8217;ve just created in Finder.\n  \n\n  svnadmin create ALCE_repo\n\n  \n    This second line tells svn (the command line tool) to create, within the &#8220;repo&#8221; folder, an actual repository called &#8220;ALCE_repo&#8221; (that will contain my ALCE project).\n  \n\n  \n    \n\n    \n      This is what a repository looks like\n    \n  \n\n  \n    That is: &#8220;repo&#8221; will contain as many different repositories as the projects I need to take control of. If you look at the command&#8217;s output, you&#8217;ll see several new files.\n  \n\n  \n    Fill a repository with things\n  \n\n  \n    We now have a brand new repo: the second step is to fill it with a Flash Builder project (either a CS extension, or any other Flash/AIR thing you&#8217;re coding).\n  \n\n  \n    \n\n    \n      SVN Perspective\n    \n  \n\n  \n    In Flash Builder, from the menu Window &#8211; Open Perspective &#8211; Other&#8230; browse to SVN Repository Exploring.\n  \n\n  \n    This perspective gives you access to the SVN Repositories panel, which should be empty. To make Flash Builder aware that a repository exists, right click inside the panel and select New &#8211; Repository Location&#8230; then in the following window, please enter:\n  \n\n  file:///Users/[your user]/repo/ALCE_repo\n\n  \n    \n  \n\n  \n    Please notice that &#8220;file:///&#8221; has a triple forward slash.\n  \n\n  \n    \n  \n\n  \n    A new location should be now set, as you see in the screenshot on the right &#8211; that is: Flash Builder knows about a repository location.\n  \n\n  \n    The next and final step is to link a project to this repo.\n  \n\n  \n    The Subclipse commands in the Flash Builder IDE belongs to a menu item called Team. To access it, switch back to your usual Flex or Extension Builder perspective, in the Package Explorer panel right click on the project you want to add to the repository, then select:\n  \n\n  \n    Team &#8211; Share\n  \n\n  \n    In the following window select SVN as the repository type:\n  \n\n  \n    \n  \n\n  \n    then Use existing repository location:\n  \n\n  \n    \n  \n\n  \n    and finally Use project name as folder name:\n  \n\n  \n    \n  \n\n  \n    Mind you, the Console will report a red warning saying that:\n  \n\n  Filesystem has no item svn: URL 'file:///Users/Davide/repo/ALCE_repo/myTest' non-existent in that revision\n\n  \n    Which you can safely ignore (the following command, mkdir, creates the missing myTest folder). According to the documentation, now the whole project should be committed &#8211; that is: all the files copied to the repository.\n  \n\n  \n    To me, this doesn&#8217;t happen, so I&#8217;ve to do it manually; right click on the project (myTest in this case, see the screenshot on the right) in the Package Explorer panel and choose:\n  \n\n  \n    Team &#8211; Commit\n  \n\n  \n    In the window that will pop up, you&#8217;re able to select which resource send to the repository &#8211; setting a custom comment too:\n  \n\n  \n    \n  \n\n  \n    That&#8217;s all! To check that everything went fine, you should notice several log messages in the console.\n  \n\n  \n    Also, go to the SVN repositories panel and click the refresh button (the two yellow arrows) in order to display the changes. The repository now contains the myTest folder and all the project files.\n  \n\n  \n    Now that the binding is done, you can start using the Version Control System &#8211; that is, code freely and Commit each time you want to set a new revision; revisions can be found later in the History panel.\n  \n\n  \n    I hope this summary has been of some help for you. In the next post of this series, I&#8217;ll try to dig a little bit deeper (I&#8217;m still learning!) around Tags, Branches, revisions, etc. so stay tuned!\n  \n\n  \n    Useful links\n  \n\n  \n    I&#8217;ve collected a good amount of links while writing this post. Some of them may help you too:\n  \n\n  \n    \n      Version Control with Subversion, pdf version of the Subversion manual (also published by O&#8217;Reilly).\n    \n    \n      Guy Rutemberg on Creating Local SVN Repository (with svn command line tools).\n    \n    \n      IBM howto about Subversion and Eclipse.\n    \n    \n      North Carolina State University on Subclipse for Configuration Management.\n    \n  \n\n",
      tags: ["Flash Builder","Subclipse","Subversion","svn","Version Control"],
      id: 125
    });
    

    index.add({
      title: "Sviluppare per Creative Suite e Photoshop",
      category: ["Coding @it","Photoshop @it"],
      content: "\n  \n    Dopo aver letto e condiviso le riflessioni di Gabe Harbs (noto sviluppatore InDesign) sull&#8217;argomento, mi piacerebbe aggiungere qui un paio di considerazioni personali, incentrate su Photoshop.\n  \n\n  \n    Estensioni Creative Suite (CS)\n  \n\n  \n    Qualche informazione per inquadrare l&#8217;argomento: sviluppare per CS oggi è incredibilmente più agevole di quanto non potessimo immaginare solo qualche anno fa. La tecnologia di Macromedia (acquisita da Adobe) è stata inserita nella Creative Suite, si è accoppiata con le Flex SDK, per maturare finalmente in un prodotto che si chiama Extension Builder (un plugin di Flash Builder / Eclipse), basato sulle nuove CS SDK.\n  \n\n  \n    Siamo ora in grado di scrivere pannelli che:\n  \n\n  \n    \n      \n        si fondono quasi perfettamente con la UI (interfaccia utente) delle applicazioni;\n      \n    \n\n    \n      \n        guidano Photoshop, InDesign ecc. esattamente come lo scripting;\n      \n    \n\n    \n      \n        sono basati su un linguaggio OOP come ActionScript 3, e su servizi Flex per il data management.\n      \n    \n\n    \n      \n        usano componenti Flash per il design della UI.\n      \n    \n  \n\n  \n    Strumenti di incredibile potenza.\n  \n\n  \n    \n  \n\n  \n    In più, il team CS SDK guidato dal PM Gabriel Tavridis sembra essere veramente motivato nel far crescere a ritmo sostenuto la piattaforma; ascoltano attentamente gli sviluppatori, sia per quel che riguarda l&#8217;aspetto tecnico che strategico e commerciale; sebbene io abbia già menzionato altrove i miei dubbi su Adobe come azienda, questo sembra un caso davvero speciale.\n  \n\n  \n    Ovviamente non tutto è rose e fiori: da quando (con la CS4) è stato introdotto, il supporto ai pannelli Flash è andato migliorando (forse perché non poteva peggiorare 🙂 ), però il debug a distanza sui clienti è sempre un lavoraccio.\n  \n\n  \n    \n      \n        Su Mac, problemi apparentemente irrilevanti (Font di Sistema, preferenze corrotte) portano a pannelli senza contenuto.\n      \n    \n\n    \n      \n        Su PC (da qui in avanti mi riferisco a Photoshop come l&#8217;applicazione host per le estensioni CS), lo stesso script lanciato tout-court o incorporato in un pannello, talvolta si comporta in modo diverso.\n      \n    \n\n    \n      \n        Adobe Extension Manager sembra essere particolarmente ostile agli utenti meno esperti &#8211; la mia richiesta di supporto #1 riguarda l&#8217;installazione via AEM. I recenti problemi con OSX Lion, fortunatamente risolti con un upgrade ufficiale a Dicembre 2011, confermano che di spazio per migliorare ce n&#8217;è in abbondanza.\n      \n    \n  \n\n  \n    Detto ciò, so per certo che il team sta attualmente lavorando per risolvere questi problemi, per far restare le CS SDK una piattaforma nella quale sia vantaggioso e piacevole lavorare.\n  \n\n  \n    Tre strade\n  \n\n  \n    Uno sviluppatore che voglia scrivere estensioni per Creative Suite può seguire tre vie (che possono anche incrociarsi):\n  \n\n  \n    \n      \n        C++\n      \n    \n\n    \n      \n        Scripting (multi-piattaforma: JavaScript)\n      \n    \n\n    \n      \n        CS SDK\n      \n    \n  \n\n  \n    La prima è per programmatori hardcore (cioè non il sottoscritto). Ma dato che il mio caro amico Marco Olivotto ha in cantiere un plugin per Photoshop, ho sentito da lui una delle più ampie raccolte di maledizioni rimostranze della storia, perlopiù rivolte alla documentazione: oscura, mancante, fuorviante, obsoleta, ecc. ecc.\n  \n\n  \n    Per quanto JS possa essere usato sia come linguaggio procedurale (e veloce), oppure come uno strumento per costruire sistemi più complessi, personalmente trovo che le CS SDK ed un linguaggio più fortemente Object Oriented come Actionscript si adattino meglio alle esigenze di sviluppatori (come il sottoscritto) mediamente abili sia nella programmazione che nella gestione del Photoshop DOM (Document Object Model).\n  \n\n  \n    Segui un corso su Java come il meraviglioso, gratuito, introduttivo &#8220;Programming Methodology&#8221; della Stanford University (tenuto da Mehram Sahami: vulcanico!) e sei pronto per cominciare a sviluppare estensioni con AS, mxml e CS SDK.\n  \n\n  \n    Estendere Photoshop\n  \n\n  \n    Personalmente, vedo almeno un paio di aree nelle quali potrebbe evolvere in modo interessante l&#8217;estensibilità delle applicazioni CS, ed in particolare Photoshop.\n  \n\n  \n    Integrazione\n  \n\n  \n    Semplificando, il CS SDK mette a disposizione (tra l&#8217;altro) gli strumenti per accedere al DOM di Photoshop da ActionScript. Ci sono però un paio di problemi:\n  \n\n  \n    \n      \n        Alcune funzioni non sono tradotte in AS. Ad esempio, il metodo .suspendHistory() di Photoshop esiste in AS (mutuato da JS) ma non funziona. Almeno: non funziona ancora.\n      \n    \n\n    \n      \n        Questione più cruciale: lo sviluppo delle CS SDK è slegato dall&#8217;evoluzione dello scripting delle varie applicazioni CS.\n      \n    \n  \n\n  \n    La comunità PS-Scripts raccoglie continuamente feature requests per Photoshop, anche se il supporto allo scripting non pare essere molto in alto nella lista delle priorità per il team di sviluppo: l&#8217;accesso ad una buona parte di strumenti è al di fuori del DOM e richiede codice Action Manager (non proprio user-friendly). Altre opzioni come l&#8217;area media di campionatura del contagocce o la posizione del cursore tanto per citarne un paio (vedi il mio progetto Power Info Palette), sono totalmente al di fuori della nostra portata: informazioni che non possono essere raggiunte, punto.\n  \n\n  \n    \n      Lo so che non è semplice, ma a mio modo di vedere sarebbe molto più produttivo se il team CS SDK potesse collaborare con i team delle varie applicazioni (PS, ID, ecc) per sviluppare e integrare più strettamente a livello di ActionScript le nuove (e vecchie) features che gli utenti richiedono. Per dirla in altri termini, ancora più chiaramente: se Adobe ha intenzione di promuovere il CS SDK come la via preferenziale per estendere la Creative Suite, dovrebbe dare al team la possibilità di guidare l&#8217;evoluzione del supporto allo scripting per ogni applicazione &#8211; non solo consentire di incorporare (e tradurre in AS) quel che già c&#8217;è, cosa che potremmo dare per scontata.\n    \n  \n\n  \n    So bene che le estensioni sono un grande passo in avanti di per sé &#8211; ciononostante credo che rendere il team di Tavridis il punto di riferimento per gli sviluppatori, anche per quel che riguarda l&#8217;integrazione con le applicazioni, sia un modo certamente migliore di sfruttarne le potenzialità sul lungo periodo.\n  \n\n  \n    Accesso a basso livello\n  \n\n  \n    Questo è uno dei miei chiodi fissi. Sviluppando per InDesign (del quale ho scarsissima esperienza) è possibile manipolare un documento e ogni tipo di oggetto che contiene &#8211; anche, da quel che so, il testo contenuto in un frame. Essendo InDesign un software che (semplificando molto) è fatto per manipolare testo, uno sviluppatore ha quindi accesso agli elementi di base: i caratteri.\n  \n\n  \n    Traducendo per Photoshop, è come se potessimo avere accesso agli elementi costitutivi di un&#8217;immagine, i suoi atomi, cioè: i pixel. Purtroppo no. Possiamo duplicare un documento, un livello, applicare filtri ad una selezione o un canale &#8211; ma un livello bitmap in PS non può essere direttamente accessibile alle estensioni come una BitmapImage ActionScript. L&#8217;unica alternativa è:\n  \n\n  \n    \n      \n        salvare una copia temporanea dell&#8217;immagine su disco;\n      \n    \n\n    \n      \n        farla caricare dal pannello (ed elaborarla);\n      \n    \n\n    \n      \n        salvarla di nuovo su disco;\n      \n    \n\n    \n      \n        caricarla in PS e incollarla nel documento originale come un nuovo livello.\n      \n    \n  \n\n  \n    Un disastro. Sapevo che un passaggio diretto tra Photoshop ed una &#8220;estensione&#8221; (nelle due direzioni, come un JPG a livelli uniti) è permesso &#8211; dalle Photoshop Touch SDK (stranamente scomparse dall&#8217;Adobe Devnet?!). Ovvero, le applicazioni Touch che tanto Adobe sembra spingere ora. Sapevo anche che Daniel Koestler e Renaun Erickson hanno scritto le API in Actionscript (nelle Touch SDK, che ho scaricato in Maggio 2011) &#8211; Mi chiedo, sperandolo disperatamente, se questo piccolo passagio sarà permesso prima o poi anche dalle CS SDK.\n  \n\n  \n    \n      Lasciare che un&#8217;estensione abbia accesso diretto ad un livello in Photoshop come BitmapImage sarebbe eccezionale: implicherebbe la possibilità di scrivere veri plugin di Digital Image Processing, con UI in Flash, nessun problema di memory management, usando kernel binari di PixelBender (il cui codice è quindi protetto), e tutti gli strumenti e shader già implementati in Actionscript. Certo, le performance rispetto ad un plugin C++ sarebbero inferiori ma&#8230; che importa.\n    \n  \n\n  \n    Pensavo di essere l&#8217;unico a richiedere questa feature, ma dopo aver ricevuto un paio di email da altri sviluppatori, mi sono deciso: Adobe, per piacere&#8230; 😉\n  \n\n  \n    Il futuro dell&#8217;estensibilità per Creative Suite\n  \n\n  \n    Mentre ragionavo su questo post, ho scritto a Gabriel Tavridis di alcuni dubbi che mi sono sorti dopo aver letto della faccenda di Flash e Flex. E&#8217; perfettamente chiaro che, sebbene a breve Flex non possa essere rimpiazzato da HTML5, l&#8217;interesse sul lungo periodo di Adobe per questa tecnologia è forte. Dunque, qual è la roadmap per le estensioni CS? Domandavo. Gabriel mi ha risposto molto gentilmente che avrebbe scritto in merito un post nel blog ufficiale &#8211; ora online e&#8230; ti suggerisco di leggerlo!\n  \n\n  \n    Concludendo, non importa quale tecnologia useremo negli anni a venire, è importante però che Adobe sia trasparente nei confronti dei suoi utenti: come sviluppatore, noto con piacere che da questo punto di vista il team CS SDK è all&#8217;avanguardia.\n  \n\n",
      tags: [],
      id: 126
    });
    

    index.add({
      title: "Creative Suite extensibility and Photoshop",
      category: ["Coding","Photoshop"],
      content: "\n  \n    Having read the (InDesign developer) Gabe Harbs&#8217; post on the subject, I&#8217;d like to add here a couple of personal points focusing on Photoshop.\n  \n\n  \n    CS extensions basics\n  \n\n  \n    Few lines to frame the topic: Creative Suite extensibility as we know it today is far better than we could even dream few years ago. This happens because Macromedia technology has plugged into Creative Suite and mixed with Flex SDK, to finally mature and end up in a product called Extension Builder (a Flash Builder / Eclipse plugin) based on the new CS SDK.\n  \n\n  \n    We&#8217;re able to build panels that:\n  \n\n  \n    \n      \n        (almost seamlessy) integrate with the host application UI;\n      \n    \n\n    \n      \n        can drive Photoshop &amp; C. just like scripting did;\n      \n    \n\n    \n      \n        are based upon an OOP language such as ActionScript 3, and Flex services, to manage data;\n      \n    \n\n    \n      \n        use Flash components for UI design;\n      \n    \n  \n\n  \n    Incredibly powerful tools.\n  \n\n  \n     Moreover, the\n\n    CS SDK team led by PM Gabriel Tavridis seems to be really motivated making the extensibility platform to evolve at a high rate, and they&#8217;ve a strong attitude to listen to developers suggestions: not only in terms of technical matters, but strategic ones too. Even though I&#8217;ve already mentioned all my doubts about Adobe as a company elsewhere, I&#8217;d say: what not to like.\n  \n\n  \n    Of course the path it&#8217;s not entirely lined with roses &#8211; ever since CS4 the flash support for panels got better (possibly because it couldn&#8217;t be worse 😉 ), but debugging customer&#8217;s troubles on a remote machine is still a ruthless job.\n  \n\n  \n    \n      \n        On a Mac, apparently unrelated issues like system Fonts may lead to a blank panel, as corrupted preferences frequently do.\n      \n    \n\n    \n      \n        On a PC (from now on I&#8217;ll be referring to Photoshop as the host application for CS extensions), depending whether a script is embedded within a panel or called as a script tout-court, the behavior may be different.\n      \n    \n\n    \n      \n        Adobe Extension Manager seems to be particularly hostile to unexperienced users &#8211; my topmost support request subject is related to installation via AEM. Recent troubles with OSX Lion, finally solved with an official update, may confirm that there&#8217;s plenty of room for improvement.\n      \n    \n  \n\n  \n    That said, I know that the CS SDK team is working to address these technical issues and keep the CS extensibility a pleasing platform to work in.\n  \n\n  \n    Three ways\n  \n\n  \n    A developer willing to code for Creative Suite applications can follow three paths (that may also cross):\n  \n\n  \n    \n      \n        C++ (the old, steep way)\n      \n    \n\n    \n      \n        Scripting (cross-platform JavaScript, mainly)\n      \n    \n\n    \n      \n        CS SDK\n      \n    \n  \n\n  \n    The first one is for hardcore programmers (which I&#8217;m not). Since my friend Marco Olivotto is on his way to coding a Photoshop plugin, I&#8217;ve heard from him one of the largest collection of cursing complaints ever, mostly focused on the documentation: obscure, missing, misleading, out of date, etc. While JS scripting may be used either as a quick, procedural way to drive the application or as a tool to build more complex systems, I&#8217;ve found that CS SDK and a true OOP language such as ActionScript better fit the needs of a coder (like me) just averagely skilled in both the programming and the DOM side of the job.\n  \n\n  \n    Take a course on Java, like the great introductory Stanford&#8217;s &#8220;Programming Methodology&#8221; (lectures by the volcanic professor Mehram Sahami) and you&#8217;re ready to start with AS and extension development.\n  \n\n  \n    Extending Photoshop\n  \n\n  \n    Personally, I see at least a couple of areas where CS (and especially Photoshop) extensibility may evolve in an interesting way.\n  \n\n  \n    Integration\n  \n\n  \n    Generally speaking the CS SDK provides, amongst other things, the access to Photoshop DOM from ActionScript (plus a way to embed Javascript code too); there are a couple of problems IMHO:\n  \n\n  \n    \n      \n        A minor one: Some functions simply don&#8217;t traslate in AS successfully. For instance, PS .suspendHistory() exists as an AS method (like in JS) but doesn&#8217;t work at all. Yet\n      \n    \n\n    \n      \n        A crucial one: The CD SDK development is unlinked to the actual application&#8217;s scripting evolution.\n      \n    \n  \n\n  \n    The PS scripting community is constantly gathering feature requests for Photoshop-next, even though scripting support doesn&#8217;t look like one of the highest priority for the PS development team: a good amount of tools are not within the reach of the DOM, and can be accessed only via unfriendly Action Manager code. Whereas for instance cursor position or eyedropper sample size (see my open project Power Info Palette), just to mention a couple of items, are out of the reach of any code at all, i.e. you can&#8217;t get that piece of information, period.\n  \n\n  \n    \n      I know it&#8217;s not easy, but in my opinion it would be far more productive if the CS SDK team could somehow collaborate with the application teams (PS, ID, etc) to develop and tightly integrate within the Actionscript layer new (and old) features that users are requesting. A bolder statement: if Adobe&#8217;s promoting the CS SDK as the preferred way to extend Creative Suite applications, it should give to the team the possibility to drive the evolution of each application&#8217;s scripting development, not only the license to embed current features into the CS SDK, which we could take for granted.\n    \n  \n\n  \n    I know that CS extensions are a huge leap forward themselves &#8211; however I believe that a mixed approach (with a personal bias to make the Tavridis&#8217; team the reference point for developers) would be the best way to exploit their power on the long run. \n  \n\n  \n    Low level access\n  \n\n  \n    This is one of my ancient hobby horses. With InDesign scripting, which BTW I&#8217;m totally unexperienced of, a scripter can manipulate documents and all kind of objects (like frames) within the documents. And, as far as I know, any text that belongs to those frames. So, being InDesign (roughly speaking) a software about text management, you have a direct access to the smallest building block &#8211; i.e. characters.\n  \n\n  \n    Translating to Photoshop, it&#8217;s like if we could access the image&#8217;s smallest building block, i.e. pixels. Which, sadly, we can&#8217;t. We&#8217;re allowed to duplicate documents, layers, apply PS filters to selections and channels &#8211; it&#8217;s fine &#8211; but the active bitmap layer in PS can&#8217;t (as far as I know) be directly grabbed and used by ActionScript as a BitmapImage. The only workaround, to the best of my knowledge is:\n  \n\n  \n    \n      \n        save a temporary copy on the disk as an image;\n      \n    \n\n    \n      \n        make the panel load and elaborate it;\n      \n    \n\n    \n      \n        save a copy back on the disk;\n      \n    \n\n    \n      \n        tell PS to load and paste it in the current document.\n      \n    \n  \n\n  \n    A pain. I knew that a direct sharing between Photoshop and an &#8220;extension&#8221; (back and forth, as a JPG flattened image) is allowed &#8211; within the Photoshop Touch SDK, suddenly disappeared from the Adobe Devnet?! That is: mobile applications, those Touch apps Adobe is proud of. I also knew that Daniel Koestler and Renaun Erickson wrote the Actionscript API (which were in the Touch SDK that I downloaded in May 2011) &#8211; I&#8217;m just wondering and desperately hoping if we&#8217;ll eventually see this little extra step in CS SDK as well.\n  \n\n  \n    \n      To let an extension have direct access to a Photoshop layer as a BitmapImage would be terrific: it would mean the possibility to write actual PS plugins with Flash UI without the headache of dealing with C code (and memory management). It would imply the use of (binary, that is: protected) Pixel Bender kernels, plus a whole bunch of ready made ActionScript shaders. It would possibly result in lower performances compared to C++ plugins but frankly&#8230; who cares.\n    \n  \n\n  \n    I thought to be the only one requesting this feature around, but a couple of emails from other developers pushed me to write this plea: Adobe, please, pretty please&#8230; 🙂\n  \n\n  \n    The future of CS Extensibility\n  \n\n  \n    While I was in the middle of this post&#8217;s writing, I mailed to Gabriel Tavridis some doubts that came to my mind &#8211; expecially after reading about the Flash/Flex affaire. It&#8217;s clear that, even if Flex cannot be replaced by HTML5 in the short run, Adobe&#8217;s commitment to this newer technology is quite strongly advertised. So what&#8217;s the company&#8217;s roadmap for CS extensions, I was asking. Gabriel kindly wrote back that he was going to post detailed answers to this (and similar) questions in the CS SDK official blog &#8211; which is now online and&#8230; I suggest you to read it!\n  \n\n  \n    As a conclusion, no matter what technology we&#8217;ll be using in the years to come, it&#8217;s important that Adobe embraces a transparent attitude toward its users &#8211; as a developer, I&#8217;m happy to notice that the CS SDK group in this respect seems to be avant-garde.\n  \n\n",
      tags: ["Creative Suite","CS Extension Builder","CS SDK","scripting"],
      id: 127
    });
    

    index.add({
      title: "Is Adobe at a crossroads?",
      category: ["Photoshop"],
      content: "\n  \n    Adobe Systems is undergoing a crucial, fast (and some would add: questionable) shift that will reverberate on us all &#8211; did you notice it? You will.\n  \n\n  \n     DISCLAIMER  This post contains publicly available data and a fair amount of opinions I&#8217;ve gathered from colleagues, clients, professional and amateur users &#8211; I&#8217;ve met them personally or in usergroups and forums. To some extent we all are speculating and conjecturing, so be warned and read along!\n  \n\n  \n    \n  \n\n  \n    I work with Photoshop since version 5.0, my last buy is the Master Collection CS5. Over the years, my attitude toward Adobe (and Photoshop in particular) took a slow change to the worse: I used to be a blind enthusiast, I&#8217;ve turned more pessimistic and critical. This long post would like to prove that it&#8217;s not (only&#8230;) senile dementia &#8211; If you whish to follow me, let&#8217;s start from the latest, surprising news.\n  \n\n  \n    Official News\n  \n\n  \n    Surprising for you too&#8230; or not? Since April 2011, and particularly about the November Financial Analyst Meeting, Adobe announced amongst other things:\n  \n\n  \n    \n      Creative Suite 5.5 (focused on Digital Publishing).\n    \n    \n      A (monthly or annual) subscription program, in addition to the usual Creative Suite licensing system.\n    \n    \n      750 full time employee layoffs (10% of the total).\n    \n    \n      Flex SDK donation to the Apache Software Foundation.\n    \n    \n      The intention to drop Flash Player developing for mobile devices.\n    \n    \n      Creative Cloud, a subscription service that provides licensing for all Adobe CS applications, amongst other things.\n    \n    \n      A drastic cut to the CS upgrade policy.\n    \n  \n\n  \n    If you&#8217;re not a Flex developer (or one of the 750), you&#8217;re about to leave this page. Wait a moment, please. Add a financial catalyst and the picture will look more clear and dynamic.\n  \n\n  \n    The Chief\n  \n\n  \n    \n\n    \n      Adobe CEO Adobe Shantanu Narayen, interviewed by Walt Mossberg (© Asa Mathat, All Things Digital &#8211; D9) Click on the picture to launch the video\n    \n  \n\n  \n    Shantanu Narayen is the Adobe CEO (Chief Executive Officer) since late 2007, COO (Chief Operative Officer) and President since 2005. He&#8217;s 48, grew up in India, got a Bachelor in Electronic Engineering, Masters in Computer Science and Business Administration. He joined Adobe in 1998 at the age of 35, as a VP (Vice President) of the worlwide product research. He worked his way up quickly, so that Barack Obama has appointed him as a member of his Management Advisory Board. Sure he&#8217;s talented and has a firm hand.\n  \n\n  \n    According to some analysts&#8217; opinion, Narayen finds himself in the unhappy position of solving a trivial problem: Adobe&#8217;s stocks stagnation in a worldwide economy crisis. Unless he finds a way to revitalize them, the company may risk to become a takeover target &#8211; bad end from an executive point of view, since they got part of their incomes as a stock options.\n  \n\n  \n    Strong paces\n  \n\n  \n    The problem has roots in the past. As long ago as 2000, reliable sources tell, Photoshop 6 release was delayed due to a longer than expected debug phase (which of course delayed revenues as well); ever since, as a result, upgrade cycles have been fixed to 18 months &#8211; there&#8217;s no getting away/around.\n  \n\n  \n    Things got even worse when first Creative Suite shipped in late 2003, forcing programs&#8217; developement cycles to be coordinated too. Photoshop, Illustrator, InDesign, Acrobat &amp; C. must be ready and appealing for users to upgrade at the very same date. It&#8217;s a simultaneous orgasm in a 18 (applications) orgy &#8211; Master Collection CS5, for instance. They will unlikely climax together &#8211; from a software developement point of view, don&#8217;t you think?\n  \n\n  \n    \n\n    \n      ADBE stocks chart (source: Yahoo Finance &#8211; http://yhoo.it/rqU0Kv)\n    \n  \n\n  \n    In fact, about 4 months after Creative Suite 4 release Adobe&#8217;s stocks slumped badly, from $43 to $17 (-53%), as a direct result of a fall in sales.\n  \n\n  \n    That being so, Adobe started what has been rather cynically named the “Layoffs Pre-Holiday season”.\n  \n\n  \n    \n      \n        December 2008, first round: 600 employees (8% of the workforce).\n      \n      \n        A year on, November 2009: some more 680 layoffs (9%), when shares are worth $35, looking forward to seeing whether CS5 (April 2010) will be welcomed by customers or not.\n      \n      \n        Early this month, with perfect pre-holyday timing: November 2011, a last 750 strong round of fulltime employee layoffs (10% of the total).\n      \n    \n  \n\n  \n    In just three years, more than 2000 people fired, one quarter of the entire workforce. I&#8217;ve been told that few companies are actually able to sustain such a high rate, and the atmosphere changes too.\n  \n\n  \n    A key event\n  \n\n  \n    A brief summary: Adobe software developement cycle is stabilised and closed for all the Creative Suite programs, and the company has bet a large part of its incomes on customers&#8217; loyalty to upgrades. Winning some important victories, in the meantime: InDesign bumped Quark XPress (being cheaper and a better product).\n  \n\n  \n    Nonetheless, at least one upgrade failed dramatically; some commenters said that Adobe has been unable to build products the quality of which could support sufficient sales to prevent a series of shake-outs; and it may now appear that the company urges to find a better financial position in the stock market, in order not to become a takeover target.\n  \n\n  \n    In this frame, cunning old foxes suggest that some speeches &#8211; even the technical ones, apparently addressed to users &#8211; actually talk to analysts, trying to reassure them about the company&#8217;s future. In that respect, the communication failure on the Flex SDK transfer (I&#8217;ll write about it later on) has been a perfect example: for 3 days, the Official Flex Team Blog sent into a frenzy a worlwide developers community.\n  \n\n  \n    \n\n    \n      Kevin Lynch (CTO) with moiré\n    \n  \n\n  \n    November 2011, Financial Analyst Meeting (FAM), NYC: where Shantanu Narayen and colleagues in impeccable business suits faced analysts, revealing details about Adobe&#8217;s roadmaps &#8211; which are, we know, to read with a critical eye.\n  \n\n  \n    What&#8217;s cooking\n  \n\n  \n    If you&#8217;ve jumped to this paragraph without reading anything before it (&#8230;) just be aware that Adobe&#8217;s ecosystem seems to be ruled by finance, and the company has seen better days. It must come up with something new to cope with the stagnation, and its plans are going to influence us all.\n  \n\n  \n    \n\n    \n      Adobe&#8217;s 2011 acquisitions\n    \n  \n\n  \n    At FAM, Adobe presents its core business as equally splitted: Digital Media and Digital Marketing. In addition to the Creative business we all know, the other side comes from the acquisition of Omniture (2009 &#8211; online marketing and web analytics), followed this year by Demdex (audience management/optimization), EchoSign (online digital signatures), Auditude (video advertising), and it&#8217;s all about content monetization. Again: half of Adobe&#8217;s core business is not even distantly related to creative content creation &#8211; did you know?\n  \n\n  \n    Narayen announces for 2012 the Creative Cloud, a subscription based service which includes licenses for all CS programs, plus software as Muse (kind of a code-free, visual only Dreamweaver) and Edge (HTML5 animation) still in beta, plus: the Digital Publishing Suite, and TypeKit (font on the web), a suit of applications for touch devices &#8211; for Shantanu wants to port a version of classic softwares (PS &amp; C.) to mobile. There&#8217;s a portal for social activities (portfolio, etc) and file sharing. The price is aggressive, a monthly fee of $49.99, but on annual basis only, as far as I understand.\n  \n\n  \n    To support the launch, heavy artillery is on the field:\n  \n\n  \n    \n      An annual developement cycle is announced, following the scheme: one year major release (CS5), one year minor (point) release (CS5.5).\n    \n    \n      Hands tied on upgrades: that are allowed only to previous major release owners &#8211; that is: if you want CS6, you must have CS5 otherwise you&#8217;re going to pay full price like a new user.\n    \n    \n      To finish the job, and with some lack of scruples, Adobe announced a CS5.5 discounted upgrade price for a limited time.\n    \n  \n\n  \n    So to speak: CS6 will be released next year (you just need algebra to know it), if you wish to buy it as an upgrade, you must buy now the CS5.5, but hurry up! With the net result to blow up upgrade sales in late 2011 and keep the stock market quiet for a little bit..\n  \n\n  \n    \n\n    \n      Adobe seems to fully embrace HTML5\n    \n  \n\n  \n    At FAM, Narayen and colleagues try to persuade analysts (some say) with few, constantly repeated buzzwords: Digital Publishing, HTML5 and Cloud. HTML5 is the new, ascending technology, a multi-year innovation vector (same words used by the late Steve Jobs) and talking about Edge, the new timeline based animation software with HTML5 export, they say that Adobe moved its best developers from the Dreamweaver, Flash e Flex team to work on this new project.\n  \n\n  \n    Clearly, this move has had repercussions: MAX 2011 (developers conference, where Flash/Flex technology is one of the core topics) is just ended, and Adobe suddenly announces that it will drop Flash Player support for mobile devicesi (as Jobs foresaw) and will donate to Apache Software Foundation the Flex SDK. Throwing enterprise application developers community into a panic, with buzzword #1 filled official communication: “In the long-term, we believe HTML5 will be the best technology for enterprise application development”.\n  \n\n  \n    On a war footing\n  \n\n  \n    These news, on different fronts, have triggered worldwide scale comments.\n  \n\n  \n    Scott Kelby, whose fate is interwined with Adobe for better of for worse, wrote an open letter, wich, considering his position, contains the most criticizing words that he could express. One of the keypoint is the suddenness of the change: it&#8217;s not fair to revolutionize upgrades without any warning, and at the same time forcing them to plan quickly an upgrade schedule (CS5.5, then CS6).\n  \n\n  \n    Customers who put their trust in the company now seem to have lost it. For instance, on mobile devices Flash is still supported via AIR, but developers can reasonably suspect that Adobe may cut it in the future, facing a new crisis. Many companies focused on enterprise applications are dealing with clients who heard that Adobe dropped Flex, and some of their projects (just about to start) have been suspended. HTML5 will be the future (PhoneGap by Nitobi it&#8217;s been another Adobe&#8217;s 2011 acquisition), but, according to many experienced developers, it&#8217;s not (yet) ready to replace Flex/AS3 &#8211; i.e. JS it&#8217;s not really an OOP language.\n  \n\n  \n    Moreover: this new SaaS (Software as a Service) model, does it implies that subscribing to Creative Cloud we&#8217;re blidly putting ourselves in Adobe&#8217;s hands &#8211; having this company proved to be able of sudden and unfair changes from the customers point of view?\n  \n\n  \n    Cashing up\n  \n\n  \n    A CS Design Premium CS 5.5 today is worth:\n  \n\n  \n    \n      $1.899 full license.\n    \n    \n      $649 as an upgrade from CS4, $399 from CS5.\n    \n    \n      $95 a month ($1.140 annui).\n    \n  \n\n  \n    After 6 years, a user who constantly upgrades (point versions included) spends about $2.400, while if he upgrades on major releases only, he spends about $1.950. On FAM&#8217;s slides, a Creative Cloud subscriber is charged the very same $2.400 (but with more services). Adobe plans 800.000 new users and, on the long run, a 100% shift to subscriptions.\n  \n\n  \n    People who&#8217;s been in the business for a long time, they say that today these figures are crazy, both because of the industry and the economy as a whole. Especially for Creative Cloud, which can&#8217;t be successful on a large scale priced as it is. It&#8217;s true that a two years developement cycle (for major releases) is a good thing from the product&#8217;s quality point of view &#8211; on the other side, while upgrades seem to be more frequent, chances are that a big part of the user base will not upgrade to point releases at all, the net result being that Adobe may end up with 3/4 of the total upgrades revenue. Someone is predicting that there will be less users, but totally up-to-date (resulting in the same income), which, alas, is not sufficient: Adobe on the stock market has to aim for a new peak. The solution it&#8217;s suggesting (do you want to upgrade? Upgrade always) it&#8217;s not a viable one, as it is.\n  \n\n  \n    Conclusion&#8230;\n  \n\n  \n    Adobe is facing a crossroads: to avoid the risk of becoming a takeover target, its stocks have to raise quickly. It presents itself as a deeply changed company (after three big layoffs rounds), with two sides &#8211; Digital Media e Digital Marketing: creative content production, and content monetization; and with a very aggressive battle plan for the future.\n  \n\n  \n    \n\n    \n      Digital Marketing and Digital Media&#8230; in the Clouds\n    \n  \n\n  \n    To its clients&#8217; eyes, it&#8217;s becoming a company apt to unexpected changes like the one on upgrade policies. It wants us to keep always up-to-date, but it has an history of some bad upgrades; it has new products and SaaS services that may unfit a large part of current users. It&#8217;s focusing on new, emerging technologies (HTML) which are maybe not yet ready for critical, large enterprise applications, and communicating badly its intentions about a robust technology (such as Flex) which future seems to be, if not uncertain, more uncertain than before. How to reconcile these two points of view, the finance and the users one?\n  \n\n  \n    &nbsp;\n  \n\n  \n    &#8230; and very personal comments\n  \n\n  \n    The perception I&#8217;ve had about Adobe, until not long ago, has been based exclusively on CS softwares &#8211; mainly Photoshop. No any substantial change in the application paradigm, no algorithm improvements on many filters, bad UI redesigns (like the Adjustments panel &#8211; it&#8217;s not just chance that I wrote a custom extension called Floating Adjustments), lot of focus on features (like 3D &#8211; which to me workflow are totally useless) and a huge carelessness to small but annoying bugs. Setting aside the Flash technology on CS applications (thanks to John Nack, thanks!) which allows me and other developers to write extension without any C++, there are few (if none) killer features that would have made essential to upgrade.\n  \n\n  \n    Nonetheless, I always did, either because the economy was generally better, and because as a small developer I&#8217;ve to keep myself updated. And yes, for I&#8217;m an impudent modernist. I did upgrade but with a latent fatalism, having verified that features I would find utterly interesting, will never be implemented &#8211; but I&#8217;ve learned to live with that. But, if when I entered this business 10 years ago we all assumed we will have upgraded to PS-next, over the years the situation has almost entirely reversed &#8211; people assume they won&#8217;t, unless something really useful to their workflow appears.\n  \n\n  \n    Adobe markets services like the Creative Cloud that do not particularly intrigue me, and policy changes on upgrades with a malicious timing, that in my opinion fit better their needs than mine. I don&#8217;t like subscriptions, rents: I don&#8217;t usually buy anything if I don&#8217;t have the money for it, and I like to use indefinitely what I&#8217;ve bought and what&#8217;s mine. Although as a professional user, I&#8217;m considering the option to surrender, somewhere in the future. I find hard to believe that 800K new users will carefree adhere to such a service &#8211; especially, as it happens by an large, they&#8217;re not (exclusively) professional users. I like even less the behavior on the Flex/Flash business, and I frankly start to have doubts even on the hold of beloved programs such the CS SDK (maybe it will be abandoned, or fell into next layoffs pre-holiday season).\n  \n\n  \n    Some say the company is the new Microsoft (too big, too full of itself), some sign a petition to make the Narayen to step down as CEO. In an optimism&#8217;s outburst I may agree with K.S. who wrote:\n  \n\n  \n    \n      “I am not overly concerned with Adobe&#8217;s actions. Ultimately, it can&#8217;t afford to charge more than its customers can afford to pay. And in our present economic climate, customers can&#8217;t afford much higher prices than they are presently being charged. Thus, Adobe will be forced to be rational.\n    \n  \n\n  \n    But it&#8217;s not, only, a matter of prices: it&#8217;s about technologies, offers, long run strategy &#8211; and how they would fit or not the needs of each one of us. Good luck. What do you think about?\n  \n\n  \n    I wish to thank very much all the experienced ones whose comments and opinion I&#8217;ve stolen collected in this post, and helped making up my mind. Thank you!\n  \n\n",
      tags: ["Adobe","Creative Cloud","Creative Suite"],
      id: 128
    });
    

    index.add({
      title: "Adobe al bivio",
      category: ["Photoshop @it"],
      content: "\n  \n    Adobe sta cambiando rotta in maniera risoluta, veloce e, nell’opinione di molti, discutibile. Te ne sei accorto? Te ne accorgerai molto presto.\n  \n\n  \n     AVVERTENZA!  Questo post contiene dati pubblicamente disponibili e molte considerazioni che ho raccolto da colleghi, clienti, utenti professionali e non, direttamente o partecipando a forum e usergroup internazionali. E’ territorio di critiche, speculazioni e informazioni poco note &#8211; leggi a tuo rischio e pericolo 🙂\n  \n\n  \n    \n  \n\n  \n    Lavoro con Photoshop dalla versione 5.0, l’ultima licenza che ho acquistato è della Master Collection CS5. Negli anni la mia attitudine verso Adobe &#8211; Photoshop in particolare &#8211; è cambiata: da entusiasta coi paraocchi sono diventato più critico e pessimista. Questo lungo post vuole dimostrare che non è (solo) senilità incipiente &#8211; se mi vuoi seguire, partiamo dalle ultime, sorprendenti, novità e vediamo dove portano.\n  \n\n  \n    Notizie ufficiali\n  \n\n  \n    Sorprendenti anche per te&#8230; oppure no? Da Aprile scorso, ed in particolare a cavallo del Financial Analyst Meeting di Novembre, Adobe ha annunciato tra l’altro\n  \n\n  \n    \n      \n        La Creative Suite 5.5 (focalizzata al Digital Publishing).\n      \n    \n\n    \n      \n        Un programma di sottoscrizione (mensile o annuale) che si affianca al tradizionale acquisto di licenze per la Creative Suite.\n      \n    \n\n    \n      \n        750 licenziamenti di dipendenti fulltime (10% ca. della forza lavoro).\n      \n    \n\n    \n      \n        La donazione delle Flex SDK alla Apache Software Foundation.\n      \n    \n\n    \n      \n        L’abbandono dello sviluppo di Flash Player su dispositivi mobili.\n      \n    \n\n    \n      \n        La Creative Cloud, un servizio ad abbonamento che prevede tra l’altro la licenza per tutti i software Adobe CS.\n      \n    \n\n    \n      \n        Un taglio molto drastico alle politiche di aggiornamento della CS.\n      \n    \n  \n\n  \n    Se non sei uno sviluppatore Flex (o uno dei 750 ex-dipendenti), stai già per cambiare canale. Fermo lì. Aggiungi un paio di catalizzatori che vengono da dove proprio non ti aspetti e il quadro si trasforma in qualcosa&#8230; di più esplosivo.\n  \n\n  \n    Il capo indiano\n  \n\n  \n    \n\n    \n      Adobe CEO Adobe Shantanu Narayen, intervistato da Walt Mossberg (© Asa Mathat, All Things Digital - D9) Clicca sull'immagine per vedere il video\n    \n  \n\n  \n    Shantanu Narayen è il CEO (Chief Executive Officer) di Adobe da fine 2007, COO (Chief Operative Officer) e Presidente dal 2005. E’ un indiano di 48 anni con una laurea in Ingegneria Elettronica, master in Computer Science e Business Administration. Entra in Adobe nel ’98 ed alla mia età è già VP (Vice Presidente) della worldwide product research da un anno. Fa carriera molto in fretta dentro e fuori Adobe, tanto che Barack Obama lo ha inserito nel suo Management Advisory Board. Nel 2010, secondo Forbes, ha guadagnato la qualcosa come 12 milioni di dollari. Di sicuro non gli mancano: qualità, polso e abbondante pelo sullo stomaco.\n  \n\n  \n    Narayen, secondo molti analisti, è nella scomodissima posizione di dover risolvere un problema da niente: la stagnazione delle azioni Adobe nella crisi finanziaria globale &#8211; azioni che, non dovessero rimbalzare in fretta, rischiano di trasformare l’azienda in bersaglio di acquisizione. Lo sapevi?\n  \n\n  \n    Ritmi forzati\n  \n\n  \n    I problemi hanno radici antiche. Stando a fonti attendibili, nel lontano 2000 Photoshop 6 uscì in ritardo rispetto alle previsioni, a causa di una prolungata fase di debug del codice; ciò venne malvisto dal management, che fissò tassativamente il ciclo delle future release a 18 mesi &#8211; in fondo, i bug possono essere corretti anche dopo&#8230; Le cose, da questo punto di vista, peggiorarono con l’introduzione delle Creative Suite (CS1 a fine 2003) che impose cicli chiusi e coordinati tra le applicazioni. Photoshop, Illustrator, InDesign, Acrobat &amp; C., dovevano essere pronti, e appetibili per l’upgrade, in contemporanea. E’ orgasmo simultaneo in un’orgia a 18 applicazioni, ad esempio per la Master Collection CS5. Un po’ improbabile &#8211; che tutti siano ugualmente soddisfatti, intendo. O no?\n  \n\n  \n    \n\n    \n      Andamento delle azioni Adobe (Fonte: Yahoo Finance - http://yhoo.it/rqU0Kv)\n    \n  \n\n  \n    E Infatti: la Creative Suite 4 esce in Ottobre 2008, e nel giro di nemmeno 4 mesi le azioni Adobe crollano verticalmente, da $43 a $17 (-53%), a causa delle rovinose vendite: è un fallimento.\n  \n\n  \n    Con questi presupposti, Adobe comincia quella che qualcuno ha cinicamente chiamato la “Layoffs Pre-Holiday season” (la stagione dei licenziamenti prima delle feste).\n  \n\n  \n    \n      \n        \n          E’ del Dicembre 2008 la prima tornata di tagli che Shantanu avalla: 600 dipendenti (8% della forza lavoro).\n        \n      \n\n      \n        \n          Poco meno di un anno dopo, Novembre 2009: altri 680 licenziamenti (9%), quando le azioni sono tornate a $35 l’una, in attesa di vedere come andrà la CS5 (Aprile 2010).\n        \n      \n\n      \n        \n          E pochi giorni fa il nuovo annuncio, con perfetto timing pre-natalizio: Novembre 2011, sono 750 i dipendenti fulltime licenziati (10% della forza lavoro).\n        \n      \n    \n  \n\n  \n    In tre anni più di 2000 persone fatte fuori, un quarto del personale; chi ha più esperienza di me sostiene che sono poche le aziende in grado di ristrutturare così profondamente senza perdere pezzi per strada, o compromettere in modo grave il clima lavorativo.\n  \n\n  \n    Appuntamenti importanti\n  \n\n  \n    Riassumo: Adobe, stabilizzatasi su un ciclo di release “chiuso” per i programmi della Suite, ha scommesso gran parte del suo guadagno sulla fedeltà degli utenti agli upgrade. Nel processo, ha conquistato eccezionali traguardi: ad esempio InDesign ha soppiantato Quark XPress, sia perché il prodotto è migliore, sia perché costa meno.\n  \n\n  \n    Ciò nonostante, Adobe non è (evidentemente) stata in grado di realizzare prodotti la cui qualità sostenesse le vendite necessarie ad evitare pesantissimi ridimensionamenti &#8211; vedi il buco della CS4. Ed oggi si trova in borsa a dovere risalire in fretta la china, pena il rischio di essere acquisita.\n  \n\n  \n    In quest’ottica, alcune vecchie volpi sostengono che certe comunicazioni &#8211; anche quelle più apparentemente tecniche &#8211; siano in realtà rivolte a rassicurare gli analisti finanziari sul buon futuro dell’azienda, più che informare gli utenti. E da questo punto di vista in effetti la défaillance comunicativa sulla cessione di Flex (ne parlerò a breve) è stata esemplare: per 3 giorni l’Official Flex Team Blog è stato un delirio di commenti di sviluppatori di tutto il mondo in preda al panico.\n  \n\n  \n    \n\n    \n      Kevin Lynch (CTO) con moiré\n    \n  \n\n  \n    Arriva un appuntamento molto importante questo Novembre, il Financial Analyst Meeting (FAM), nel quale Shantanu Narayen e colleghi (in perfette business suits e con la voce che tradisce, a tratti, una certa tensione) sono a NYC di fronte agli analisti finanziari per raccontare in dettaglio le roadmap di Adobe. Con dettagli tecnici e strategici insospettati &#8211; i quali, abbiamo imparato, sono da leggere con occhio smaliziato.\n  \n\n  \n    Cosa bolle in pentola\n  \n\n  \n    Se sei saltato a questo paragrafo senza leggere i precedenti (&#8230;) sappi almeno che l’ecosistema nel quale Adobe vive è governato dalla finanza, ed Adobe ha visto giorni migliori. Deve escogitare qualcosa per sopravvivere, e le sue pensate influenzeranno molto pesantemente tutti.\n  \n\n  \n    \n\n    \n      Acquisizioni Adobe del 2011\n    \n  \n\n  \n    Al FAM, Adobe si presenta con un core business sostanzialmente bipartito: Digital Media e Digital Marketing. L’altra metà della torta rispetto al mondo Creative che conosciamo viene dall’acquisizione di Omniture (2009 &#8211; online marketing e web analytics), seguita più recentemente da Demdex (audience management/optimization), EchoSign (online digital signatures), Auditude (video advertising), e si occupa di monetizzare contenuti. Ripeto: metà del core business Adobe non è nemmeno lontano parente della creazione di contenuti creativi. Lo sapevi?\n  \n\n  \n    Narayen annuncia per il 2012 la Creative Cloud, un servizio a sottoscrizione (mensile o annuale) che prevede l’accesso a tutti (tutti!) i programmi della CS, più software come Muse (una specie di Dreamweaver visuale) ed Edge (animazioni HTML5) ancora in beta, e ancora: la Digital Publishing Suite, poi TypeKit (font su web), le applicazioni touch per tablet già prodotte ed in cantiere &#8211; perché Shantanu vuole portare i software classici (PS &amp; C.) su mobile. C’è una parte importante di servizi social &#8211; un portale di condivisione del portfolio ad esempio &#8211; e file sharing. Il prezzo è estremamente aggressivo, $49.99 al mese ma solo, apparentemente, per abbonamenti annuali.\n  \n\n  \n    A supporto del lancio, si muove l’artiglieria pesante e non fa prigionieri.\n  \n\n  \n    \n      \n        Viene annunciato un ciclo di sviluppo annuale, che seguirà lo schema: un anno major release (CS5), un anno point release (CS5.5).\n      \n    \n\n    \n      \n        Si stringe il cappio al collo degli utenti sugli upgrade: sarà possibile aggiornare solo se si possiede la major release precedente &#8211; ovvero se vuoi passare alla CS6 devi avere la CS5, altrimenti ti tocca pagare il prezzo completo.\n      \n    \n\n    \n      \n        Per completare l’opera e con una certa spregiudicatezza finanziaria, Adobe a questo annuncio ha fatto seguire quello di sconti sull’upgrade alla CS5.5 fino al termine del 2011.\n      \n    \n  \n\n  \n    Come dire: l’anno prossimo uscirà la CS6 (bastano nozioni di algebra per prevederlo), se vuoi essere in grado di acquistarla come upgrade, devi per forza comprare, adesso, il passaggio alla CS5.5, ma fai in fretta! perché è a prezzo calmierato sono entro Dicembre. Col risultato di far gonfiare i guadagni da upgrade entro l’anno e tenere quieta la borsa per un po’.\n  \n\n  \n    \n\n    \n      Adobe sembra abbracciare totalmente HTML5\n    \n  \n\n  \n    Al FAM, Narayen e colleghi tentano di convincere gli analisti (secondo alcuni commentatori) con largo uso di buzzwords &#8211; paroloni ad effetto &#8211; ripetuti a mitragliatrice: ovvero Digital Publishing, HTML5 e Cloud. HTML5 è presentato come una tecnologia in fase ascendente, un vettore di innovazione destinato a durare nel tempo (stesse, identiche parole di Steve Jobs) e parlando di Adobe Edge, un nuovo software per le animazioni con gestione di una timeline ed export in HTML5, viene raccontato come Adobe abbia spostato i migliori developers dai team di Dreamweaver, Flash e Flex per farli lavorare sul nuovo progetto.\n  \n\n  \n    Ma evidentemente lo spostamento di risorse ha avuto ripercussioni: è appena terminato il MAX 2011 (appuntamento dedicato agli sviluppatori, dove si è parlato molto anche di Flash) che Adobe annuncia a freddo l’abbandono del Flash Player su dispositivi mobili (come pronosticato da Jobs) e rivela l’intenzione di donare alla Apache Software Foundation le Flex SDK. Gettando nel panico la comunità internazionale di sviluppatori di applicazioni enterprise con una comunicazione ufficiale nella quale veniva riproposta la buzzword #1: “In the long-term, we believe HTML5 will be the best technology for enterprise application development” (Sul lungo periodo, crediamo che HTML5 sarà la migliore tecnologia per sviluppare applicazione enterprise).\n  \n\n  \n    Sul piede di guerra\n  \n\n  \n    Queste mosse, su diversi fronti, hanno scatenato commenti a livello mondiale.\n  \n\n  \n    Scott Kelby, che come mi è stato fatto notare è legato a doppio filo ad Adobe nel bene e nel male, ha scritto una lettera aperta i cui toni, tenuto conto della sua posizione, sono il massimo di criticismo che può esprimere. Uno dei punti chiave dei malumori è l’estemporaneità del cambio di politica: non è leale nei confronti degli utenti rivoluzionare dal giorno alla notte gli upgrade, senza nessun avvertimento e (all’atto pratico) costringerli a decidere in pochissimi mesi una catena di aggiornamenti (CS5.5, in funzione di CS6). Non pensare solo alla tua copia di Photoshop&#8230; una Master Collection &#8211; magari a licenze multiple &#8211; ha tutt’altro prezzo. La finanza non ha molti scrupoli.\n  \n\n  \n    E’ venuta meno, secondo alcuni, la fiducia nei confronti dell’azienda: ad esempio, su dispositivi mobili Flash resta supportato via AIR, cosa garantisce gli sviluppatori che Adobe non lo tagli improvvisamente in futuro a fronte di nuove crisi? (non sarebbe la prima volta). Moltissime aziende il cui business è incentrato sulle applicazione enterprise si ritrovano a dover fronteggiare clienti a cui è giunta voce che Adobe ha abbandonato Flex, col risultato netto di lasciare nel limbo progetti in fase di partenza. HTML5 sarà il futuro (PhoneGap di Nitobi è stata acquisita quest’anno da Adobe), ma al momento, ascoltando la voce di sviluppatori di mezzo mondo, non è tecnicamente in grado di rimpiazzare Flex/AS3, non fosse altro perché JS non è un linguaggio veramente OOP (orientato agli oggetti) ed in grado di restituire le stesse performance.\n  \n\n  \n    E ancora: dato il nuovo modello SaaS (Software as a Service), affidandoci ad un programma di sottoscrizione come la Creative Cloud non ci mettiamo completamente in mano ad un’azienda che ha dimostrato di essere capace di cambi di rotta repentini, economicamente parlando non favorevoli ai clienti di vecchia data? (vedi la nuova upgrade policy).\n  \n\n  \n    I conti della serva\n  \n\n  \n    Per quello che riguarda il mondo CS, facciamoci due conti in tasca. Una CS Design Premium CS 5.5 costa oggi:\n  \n\n  \n    \n      $1.899 in licenza.\n    \n    \n      $649 come upgrade dalla CS4, $399 dalla CS5.\n    \n    \n      $95 al mese ($1.140 annui).\n    \n  \n\n  \n    In 6 anni, un utente che aggiorna ad ogni versione (comprese le .5) per via tradizionale arriva a spendere $2.400 circa, mentre se aggiorna ogni major release soltanto (ovvero le .0) spende $1.950. Nei conti che provengono dal FAM, un utente che sottoscrive la Creative Cloud spende gli stessi $2.400 (con un ritorno in servizi decisamente maggiore. Adobe prevede 800 mila nuovi utenti e a lungo termine il 100% di passaggio al nuovo sistema a sottoscrizione.\n  \n\n  \n    Commentatori indipendenti e di lungo corso sostengono che queste cifre siano folli, insensate, per lo stato attuale dell’industria e dell’economia. Soprattutto per quel che riguarda la Creative Cloud, che non può funzionare sui grandi numeri a quel prezzo. Da un lato l’allungamento a 2 anni per ciclo di major release è rassicurante (i software saranno migliori, forse). Ma d’altro canto, mentre in apparenza gli upgrade saranno più frequenti, è molto probabile che la maggioranza degli utenti scelga di non acquistare le point release, col risultato che gli aggiornamenti principali scendono ad una frequenza 3/4 inferiore &#8211; con conseguente leggero calo delle entrate. L’equazione sostenuta da alcuni (meno utenti, più aggiornati = pareggio di bilancio) è insufficiente: Adobe è in una delicata situazione in borsa e non deve, cambiando strategia, sperare in un pareggio, ma puntare ad una netto rimbalzo. E la soluzione che propone a noi (vuoi aggiornare? Aggiorna sempre) non è, così come è ora strutturata, accettabile.\n  \n\n  \n    Conclusioni&#8230;\n  \n\n  \n    Adobe è ad un bivio: per non rischiare di essere acquisita, le sue azioni in borsa devono risalire in fretta. Si presenta agli analisti come un’azienda pesantemente ristrutturata, a due facce &#8211; Digital Media e Digital Marketing, cioè produzione di contenuti e monetizzazione di contenuti &#8211; e con un piano molto aggressivo di crescita per il futuro.\n\n    \n      \n\n      \n        Digital Marketing e Digital Media... in the Clouds\n      \n    \n\n     Si presenta ai suoi clienti, invece, con un repentino cambiamento di politica sugli upgrade a nostro sfavore, volontà di aggiornamenti costanti &#8211; ma con una storia passata di aggiornamenti spesso insufficienti &#8211; nuovi prodotti con prezzi troppo alti per fare presa sulla maggioranza degli utenti e soluzioni SaaS inadatte a larga parte di loro. Concentrandosi su tecnologie in ascesa (HTML5) ma ancora immature per determinate applicazioni e mal comunicando la propria intenzione a riguardo di tecnologie consolidate (Flex) il cui futuro appare, se non incerto, molto meno certo di prima. Come si concilieranno questi due punti di vista, finanziario e degli utenti?\n  \n\n  \n    &#8230; e commenti molto personali\n  \n\n  \n    La mia percezione di Adobe, fino a poco tempo fa, si è esclusivamente basata sulle release dei software della CS &#8211; in particolare Photoshop. Nessun cambio sostanziale del paradigma del programma, nessun miglioramento sugli algoritmi di certi filtri, redesign dell’interfaccia a volte funesti (vedi pannello Adjustments &#8211; non per niente ho scritto un’estensione che si chiama Floating Adjustments), pesante accanimento su features (come il 3D &#8211; per i miei scopi del tutto inutile) e totale disattenzione ad una costellazione di piccoli ma fastidiosi bug che affliggono il programma. A parte l’introduzione di Flash nelle applicazioni della CS (spinta a suo tempo da John Nack, grazie al quale oggi posso scrivere estensioni in Flex senza scomodare il mio rudimentale C++), non ci sono molte altre killer feature che avessero reso imprescindibile l’upgrade.\n  \n\n  \n    Eppure ho sempre aggiornato, un po’ perché l’economia lo permetteva, un po’ perché la componente di affezione al prodotto ha un suo peso &#8211; e come piccolo sviluppatore devo essere up-to-date. Con un certo fatalismo però, avendo appurato che le features di mio interesse non sarebbero mai state implementate. Ma se, quando ho cominciato dieci anni fa, era naturale presupporre l’aggiornamento alle nuove versioni salvo casi eccezionali, con gli anni la tendenza s’è invertita: ed ora per molti è diventato naturale il contrario &#8211; cioè presupporre di non acquistare l’upgrade &#8211; finché si può.\n  \n\n  \n    Adobe propone ora soluzioni come la Creative Cloud che non mi intrigano in modo particolare, e cambiamenti di policy sugli aggiornamenti che hanno un timing maliziosamente spiacevole; che trovo modellati più sulle esigenze finanziarie loro, che non su quelle pratiche mie. Non amo le sottoscrizioni, gli affitti, ma un utente professionale ad un certo punto può anche cedere, in cambio di un servizio opportuno. Faccio molta fatica a credere che gli 800 mila nuovi utenti previsti aderiscano così incondizionatamente alla Creative Cloud &#8211; in specie se, come spesso capita &#8211; sono utenti non (esclusivamente) professionali. Mi piace ancora meno il comportamento tenuto sulla vicenda Flex/Flash, e comincio ad avere dubbi anche sulla tenuta futura di programmi come quello delle CS SDK (magari potrebbero essere abbandonate, o prese di mira dalla prossima layoffs pre-holiday season).\n  \n\n  \n    C’è chi sostiene che l’azienda sia ostaggio della borsa, chi dice che Adobe sia diventata la nuova Microsoft (troppo grande e piena di sé), chi firma petizioni per far dimettere il CEO. In un accesso di ottimismo, potrei condividere la posizione di chi ha detto:\n  \n\n  \n    \n      “Non sono particolarmente preoccupato dal comportamento di Adobe. In definitiva, non può permettersi di far costare i suoi prodotti e servizi più di quello che i suoi clienti sono in grado di pagare. E dato il clima economico di questi anni, i clienti non possono permettersi di pagare molto più di quando fanno ora. Quindi, Adobe sarà costretta a comportarsi razionalmente”.\n    \n  \n\n  \n    Ma non è sempre, solo, una questione di prezzi: anche di tecnologie, offerte, cambiamento delle strategie &#8211; e di quanto queste si adattino alle personali esigenze di ognuno di noi. Buona fortuna. Tu cosa ne pensi?\n  \n\n",
      tags: [],
      id: 129
    });
    

    index.add({
      title: "Sharpening &#8211; uno studio",
      category: ["Photoshop @it"],
      content: "\n  \n    L&#8217;ultimo esempio che ho presentato nella mia lezione sullo Sharpening al CCC in Ottobre è stata la correzione di un&#8217;immagine subacquea. Mi è stato domandato, giorni dopo, di mostrarla nuovamente in tutti i suoi passaggi &#8211; così ho chiesto al mio caro amico e fotografo Davide D&#8217;Angelo: molto gentilmente mi ha messo in contatto col suo collega Paolo Fossati, fotografo subacqueo e autore di questa immagine. Grazie mille ad entrambi! Eccola qui in forma di piccolo studio sullo sharpening.\n  \n\n  \n    \n\n    \n      Versione finale (clicca sull'immagine per ingrandire) - © Paolo Fossati\n    \n  \n\n  \n    \n  \n\n  \n    L’originale è un file RAW di una Nikon D80 (3872 x 2592 px, tanto per contestualizzare i raggi di maschera di contrasto che userò), e, per quel che ho potuto capire da Google, si tratta di una Actiniaria (o Attinia, l&#8217;animale rosso coi tentacoli) in compagnia di un pesce pagliaccio (il Nemo giallo). L&#8217;immagine proviene dal viaggio di Paolo Fossati in Arabia Saudita ai Seven Reefs (galleria completa). Rispetto al CCC, ho un più tempo per soffermarmi sui vari passaggi, ma per brevità do’ per scontata la conoscenza di alcuni strumenti – se qualcosa non ti fosse chiaro, scrivimi nei commenti a fondo pagina!\n  \n\n  \n    \n\n    \n      Versione di partenza, azzerando tutto in Adobe Camera Raw - clicca per ingrandire\n    \n  \n\n  \n    Parte 1 &#8211; Colore e contrasto\n  \n\n  \n    Ammetto di non avere particolare esperienza con questo tipo di immagini naturalistiche, zeppe di trabocchetti che Davide D&#8217;Angelo conosce certamente meglio di me: infatti oltre a fotografare in scena ed essere specialista di infrarosso, è un esperto subacqueo. Quindi ha una dimestichezza particolare nella correzione di queste immagini (che ha elaborato a migliaia) &#8211; meno ovvie di quel che sembrano a prima vista.\n  \n\n  \n    \n\n    \n      Adobe Camera Raw default - clicca per ingrandire\n    \n  \n\n  \n    Al CCC di Bologna ho lavorato su un soggetto più semplice, così ho potuto descrivere anche tutta la parte di correzione colore. Cosa che purtroppo non posso fare qui &#8211; e non per nasconderti chissà quali segreti; a dire la verità non sapevo che strada prendere, e ho sperimentato liberamente. Comunque la mia versione corretta (che è un intermedio di lavorazione, e sarà la base di tutto lo sharpening che faremo) è decisamente lontana sia dal default che propone Camera Raw, sia dall&#8217;originale molto piatto dal quale ho deciso di partire. Comunque, grosso modo, la correzione prevedeva:\n  \n\n  \n    \n      \n        Esportazione del RAW in ProPhoto RGB (spazio colore che in genere evito come la peste, ma che qui è opportuno per avere un canale del Rosso con più dettaglio, che mi servirà nei passaggi successivi).\n      \n    \n\n    \n      \n        Una prematura conversione in Lab per lavorare sul contrasto nel canale L (anche con channel blending).\n      \n    \n\n    \n      \n        Curve di dubbia morale sui canali a e b, ed azioni MMM ed Helmholtz-Kohlrausch dal pannello PPW di Dan Margulis.\n      \n    \n\n    \n      \n        Nel caso servisse RGB, useremo sRGB (che, per fortuna, sembra obliterare il terribile banding nel cielo nell&#8217;acqua, che è stato una penitenza dall&#8217;inizio).\n      \n    \n  \n\n  \n    Possiamo sgarrare, forse, su una delle regole base della correzione colore: cioè il punto di ombra neutro. Siamo sott&#8217;acqua, annegati di Blu ovunque eccetto per l&#8217;area di primo piano colpita dalla luce del flash; peraltro io non sono mai stato in immersione (e dubito ci andrò mai), quindi non ho la più pallida idea di quale sia il tipo di visione là sotto, né come appaiano queste creature dal vero ad un subacqueo a che esplori il fondale.\n  \n\n  \n    \n\n    \n      Versione intermedia - clicca per ingrandire\n    \n  \n\n  \n    \n\n    \n      Dettagli - clicca per aprire l'alta risoluzione in una nuova finestra\n    \n  \n\n  \n    Detto questo, la mia versione intermedia è forse troppo ricca di colore &#8211; che di per sé non è un difetto, credo. Se ti interessa comparare gli stili di correzione di diversi ritoccatori, dai un&#8217;occhiata alla versione di Paolo Fossati / Davide D&#8217;Angelo &#8211; più drammatica e tridimensionale. Io sono stato meno fotografico (del resto non sono un fotografo), e più descrittivo; per certi versi la mia versione richiama l&#8217;estetica di certi acquerelli del 18 secolo sugli uccelli del paradiso della Nuova Guinea, se la cosa non suonasse del tutto folle&#8230; E&#8217; la visione del fondale (immaginario) di chi non c&#8217;è mai stato &#8211; un po&#8217; come i libri di viaggio scritti senza viaggiare, un&#8217;interpretazione fantastica e non sempre corrispondente al vero &#8211; che di per sé, dicevo, può anche non essere un difetto.\n  \n\n  \n    Parte 2 &#8211; HiRaLoAm\n  \n\n  \n    E’ giunto il momento di pianificare la strategia di sharpening. In generale, non vorrei accentuare troppo lo sfondo, e mantenere invece l’attenzione sui soggetti in primo piano. Siccome siamo in Lab, il canale della b (opponenti blu/giallo) ci tornerà utile per una maschera. Una passata di HiRaLoAm (High Radius Low Amount – ovvero il filtro Maschera di Contrasto con Raggio Alto e Fattore Basso) male non fa, per cui:\n  \n\n  \n    \n      \n        Duplica il livello di sfondo e chiamalo HiRaLoAm.\n      \n    \n\n    \n      \n        Attiva il canale L, ed applica la Maschera di Contrasto con Fattore: 500 &#8211; Raggio: 12px &#8211; Soglia: 0px.\n      \n    \n  \n\n  \n    \n\n    \n      HiRaLoAm spinto - clicca per ingrandire\n    \n  \n\n  \n    Fa schifo, eh? Molto bene, perché:\n  \n\n  \n    \n      \n        Crea una maschera di livello per HiRaLoAm, e applicaci il canale L dallo Sfondo. Ci siamo quasi.\n      \n    \n\n    \n      \n        Abbassa l’opacità di HiRaLoAm al 40% circa..\n      \n    \n  \n\n  \n    Sapendo che avrei usato una maschera di livello per attenuare l’effetto (dopo), ho preferito avere la mano pesante col filtro (prima). Volendo, poi, c’è anche l’opacità che fornisce un ulteriore controllo – il flusso di lavoro così è più flessibile, a mio parere. Lo sharpening è andato solo sul canale L per evitare guai sotto forma di aloni colorati. C’è ancora un problema – l’acqua è troppo dettagliata, in particolare i pescetti sullo sfondo sembrano radioattivi. Così, come dicevamo prima:\n  \n\n  \n    \n      \n        Fai doppio click sull’icona del livello HiRaLoAm nella palette dei Livelli, per aprire la finestra degli Stili di Livello – Opzioni di Fusione (Layer Style – Blending Options), e sposta come in figura gli slider Fondi Se (Blend If), per restringere l’effetto alle parti “calde” del canale b, tagliando quelle “fredde” (cioè l’acqua blu).\n      \n    \n  \n\n  \n    \n  \n\n  \n    Le Opzioni di Fusione sono un modo per limitare la visibilità di un livello (di qualsiasi tipo) sulla base del suo contenuto o del contenuto del livello sottostante. Fondi Se (BlendIf) funziona come una soglia: prima selezioni quale/i canale/i usare come base, poi trascini gli slider (eventualmente, ALT+click separa in due uno slider e sfuma la transizione) per definire se e quando verrà limitata la visibilità del livello superiore: a seconda di quanto range del canale scelto gli slider non racchiudono.\n  \n\n  \n    \n      \n        Premi contemporaneamente (Command + Option + Shift + E) se sei su un Mac, (CTRL + ALT + Shift + E) se sei su un PC. Un nuovo livello verrà creato e riempito col contenuto visibile. Butta pure il livello HiRaLoAm sotto assieme alla sua maschera, e rinomina il livello fresco di creazione come “HiRaLoAm”.\n      \n    \n  \n\n  \n    \n\n    \n      Sharpening a Raggio Alto, Fattore Basso, con maschera L e Opzioni di Fusione - clicca per ingrandire\n    \n  \n\n  \n    \n\n    \n      Dettagli - clicca per aprire l'alta risoluzione in una nuova finestra\n    \n  \n\n  \n    Parte 3 &#8211; Ehi, dove siamo?\n  \n\n  \n    Ok, pausa. Fin qui, ho corretto il colore e contrasto in modi che non so dire più: il risultato sta felicemente come base (ed àncora di salvataggio) nel livello di Sfondo. Poi abbiamo un secondo livello chiamato “HiRaLoAm”, che è il risultato di un livello con sharpening HiRaLoAm con maschera e opzioni Fondi Se, trasformato per semplicità con un’operazione esoterica di “Unisci Visibili su Nuovo Livello” in un livello… normale. HiRaLoAm è usato per aggiungere una specie di effetto di contrasto locale (anche se non è un vero contrasto locale come ALCE (conosci ALCE, vero?). Manca una cosa e abbiamo finito.\n  \n\n  \n    Parte 4 &#8211; Sharpening tradizionale\n  \n\n  \n    Senza esitazione:\n  \n\n  \n    \n      \n        Duplica il livello HiRaLoAm e chiamalo “USM” (che sta per Unsharp Mask, ovvero Maschera di Contrasto).\n      \n    \n\n    \n      \n        Attiva il canale L e applica, indovina cosa, il filtro Maschera di Contrasto con Fattore: 500 – Raggio: 1.4px – Soglia: 0px.\n      \n    \n  \n\n  \n    Un po’ tanto, eh? Nessun problema, visto che il Maestro (Dan Margulis) insegna che gli aloni più fastidiosi sono quelli chiari &#8211; rispetto a quelli scuri. Ci occorre solo un modo per dimezzarli:\n  \n\n  \n    \n      \n        Sempre tenendo il canale L di USM come l’unico attivo, Applica il canale L dal livello HiRaLoAm, modalità di fusione Scurisci (Darken), 50% di Opacità.\n      \n    \n  \n\n  \n    In questo modo, gli aloni scuri di USM sono sempre lì, mentre quelli chiari sono stati ridotti del 50% grazie all’intervento del livello HiRaLoAm (che ne era privo). Ci sei fin qui? Ricorda che se hai domande, puoi scrivere nei commenti! L’ultimissimo passaggio è l’applicazione di una maschera che tagli l’effetto nell’acqua sullo sfondo. Potremmo usare il canale b come abbiamo fatto prima, ma c’è un’alternativa che è più efficace &#8211; o ugualmente efficace, ma ha il pregio di mostrare come esista sempre più di una soluzione ad ogni problema. Riesci ad indovinare quale?\n  \n\n  \n    \n      \n        Duplica il documento senza unire i livelli, chiama il nuovo documento “RGB”. Tieni solo il livello di sfondo, butta tutto il resto. Converti al buon vecchio sRGB.\n      \n    \n\n    \n      \n        Torna al documento originale in Lab, aggiungi una maschera di livello ad USM, e applicaci il canale R dal documento RGB.\n      \n    \n\n    \n      \n        Curva la maschera di livello per schiarirla un po’ (quindi facendo passare di più l’effetto dello sharpening &#8211; le maschere funzionano così).\n      \n    \n  \n\n  \n     In questa particolare situazione, vogliamo una maschera che sia nera come la pece nell’acqua, e quindi il canale R è a priori una buona scelta. Ma se prendi il canale R di uno spazio RGB piccolo, uscirà già completamente nero e senza dettaglio nelle sue ombre (l’acqua) per cui qui ed ora: più piccolo è, meglio è.\n  \n\n  \n    \n\n    \n      La maschera del livello USM: il canale R di un duplicato del documento, convertito in RGB\n    \n  \n\n  \n    \n\n    \n      La palette dei Livelli al termine di questa correzione\n    \n  \n\n  \n    Ce l’abbiamo fatta! Terminiamo con un documento a tre livelli, con uno Sfondo che rappresenta la versione coi soli colori corretti, un livello di mezzo con un HiRaLoAm che agisce principalmente sul primo piano, e un livello in cima con la Maschera di Contrasto tradizionale mascherata opportunamente, e i cui aloni chiari sono stati dimezzati. E’ più lunga da scrivere che non da fare, credimi.\n  \n\n  \n    Pensi che il risultato sia un po&#8217; troppo spinto? Sei probabilmente in ottima compagnia, ma la soluzione è a portata di mano: abbassa l&#8217;opacità del livello HiRaLoAm e/o USM a tuo piacere &#8211; è sempre meglio strafare, potendo aggiustare il tiro con precisione in seguito.\n  \n\n  \n    Comunque, se per qualsiasi ragione ti fossi perso per strada, usa i commenti a fondo pagina per fare domande e chiedere spiegazioni: farò del mio meglio per farti nuotare in acque basse.\n  \n\n  \n    \n\n    \n      Versione finale (clicca sull'immagine per ingrandire) - © Paolo Fossati\n    \n  \n\n  \n    \n\n    \n      Dettagli - clicca per aprire l'alta risoluzione in una nuova finestra\n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    Credits Ringrazio infinitamente Davide D&#8217;Angelo per il suo aiuto, e Paolo Fossati che mi ha gentilmente concesso di scegliere ed usare una sua fotografia. Se posso permettermi un consiglio: visita i loro siti, contengono immagini che valgono la pena di essere viste.\n  \n\n",
      tags: [],
      id: 130
    });
    

    index.add({
      title: "Sharpening case study",
      category: ["Photoshop"],
      content: "\n  \n    As a last example of my CCC Sharpening lecture in October, I did a correction of an underwater image. Later, I&#8217;ve been solicited with a very gentle email to show once again all the steps, and I asked my dear friend the photographer Davide D&#8217;Angelo: he kindly got me in touch with his colleague the underwater photographer Paolo Fossati, author of the picture I&#8217;ll show throughout this post. Thanks so much to both of you! So here it is, a little sharpening case study.\n  \n\n  \n    \n\n    \n      Final, processed version (click for a larger image) &#8211; © Paolo Fossati\n    \n  \n\n  \n    \n  \n\n  \n    The original file comes as a RAW file from Nikon D80 (3872 x 2592 px just to keep in context the sharpening radii that I&#8217;ll be using), and as far as I&#8217;ve been able to understand from my Google quest, it depicts an actinia (the red thing with tentacles) in good company of an anemonefish (the yellow Nemo). The picture comes from Paolo Fossati&#8217;s trip in the Arabian Seven Reef (full gallery page). Even though, compared to the CCC, I&#8217;ve some more room here to explain what I&#8217;m doing, I will take for granted some basic color correction terms and tools for the brevity&#8217;s sake.\n  \n\n  \n    \n\n    \n      Start version, zeroed in Adobe Camera Raw &#8211; click for a larger image\n    \n  \n\n  \n    Part 1 &#8211; Color and tone\n  \n\n  \n    I&#8217;ve to admit that I&#8217;ve zero experience with this kind of naturalistic images, while Davide D&#8217;Angelo knows certainly better than me their (many) pitfalls: besides his works on stage in theaters and as an IR landscape photographer, he&#8217;s used to scuba diving as well, so he&#8217;s acquired peculiar skills for underwater photography retouching.\n  \n\n  \n    \n\n    \n      Adobe Camera Raw default &#8211; click for a larger image&#8221; margin-bottom=&#8221;5\n    \n  \n\n  \n    On my CCC lecture I worked on a different (and a lot easier) image, so I could show the steps for the color correction part too. Here the original is trickier and, believe me I don&#8217;t tell this to hide any secret sauce at all, I can&#8217;t exactly replicate (nor I exactly remember) how on earth I got to the intermediate version &#8211; the one that will be my base for the sharpening moves. Although, you may agree that the default ACR conversion is utterly different both from the zeroed version above (which was my actual starting point) and my intermediate result. Basically, the correction steps involved:\n  \n\n  \n    \n      Exporting the raw to ProPhoto RGB (a workspace that I usually keep away from me, but is needed to avoid Red channel&#8217;s clipping, critical here in order to have more room for further processing).\n    \n    \n      An early conversion to Lab working with, and blending to the L channel for contrast.\n    \n    \n      Lot of a and b curves tweaking and masking, MMM and Helmholtz-Kohlrausch actions from Dan Margulis&#8217; PPW panel.\n    \n    \n      If RGB is needed again, I&#8217;ll use sRGB (which, by the way, obliterated almost entirely the ugly banding in the sky waters up above, that has been a pain in the neck ever since the Camera Raw export).\n    \n  \n\n  \n    One of the color correction basic rules (neutral shadows) is less mandatory here perhaps &#8211; everything is drowned in deep blue, except for the area hit by the flash light in the foreground. Moreover, I&#8217;ve never been undersea (nor I will) so I haven&#8217;t the faintest idea of what these creatures would look like to a scuba diver approaching them.\n  \n\n  \n    \n\n    \n      Intermediate version &#8211; click for a larger image\n    \n  \n\n  \n    \n\n    \n      Details &#8211; click for a high-resolution preview on a new window\n    \n  \n\n  \n    That said, my intermediate version is maybe a little bit&#8230; too colorful, but it&#8217;s not that bad anyway, at least in my opinion. If you&#8217;re interested in comparing retoucher&#8217;s styles, have a look to the Paolo Fossati / Davide D&#8217;Angelo version &#8211; more &#8220;dramatic&#8221; and 3D, if you will. I&#8217;ve been less &#8220;photographic&#8221; and more &#8220;descriptive&#8221; perhaps &#8211; it may remind you the esthetic of some New Guinea paradise bird watercolors from the 18th century, if that wouldn&#8217;t sound totally crazy.\n  \n\n  \n    Part 2 &#8211; HiRaLoAm\n  \n\n  \n    Now the time to plan our sharpening strategy has come. Generally speaking I&#8217;d like to affect less the blue water than the animals to keep the focus on them, so while we&#8217;re in Lab the b channel (blue/yellow opposites) may work as a good mask. An HiRaLoAm (High Radius Low Amount) pass can&#8217;t hurt, so you may:\n  \n\n  \n    \n      Duplicate the background layer, call it HiRaLoAm.\n    \n    \n      Activate the L channel only, and apply the filter Unsharp Mask with Amount: 500 &#8211; Radius: 12px &#8211; Threshold: 0px.\n    \n  \n\n  \n    \n\n    \n      HiRaLoAm full blast &#8211; click for a larger image\n    \n  \n\n  \n    Yes! It looks like crap! Just don&#8217;t worry, because:\n  \n\n  \n    \n      Create a blank layer mask to the HiRaLoAm, and apply the L channel from the background layer to it. Almost back on track.\n    \n    \n      Lower the HiRaLoAm layer opacity to something around 40%.\n    \n  \n\n  \n    Since a layer mask is involved, I prefer to beat hard with the filter &#8211; then, afterwards, lower the opacity to taste. You&#8217;ve more flexibility this way, in my opinion. The sharpening went to the L channel alone, to avoid any bad, bad color shift. There&#8217;s still a problem &#8211; the water has gained too contrast and it&#8217;s distracting, not to mention the lighter halos around small fishes. So as we said before:\n  \n\n  \n    \n      Double click to the HiRaLoAm icon in the Layer palette to open the Layer Style &#8211; Blending Options window, and move this way the Blend If sliders in order to restrict the effect to the &#8220;hot&#8221; parts of the b channel, cutting the &#8220;cold&#8221; ones &#8211; i.e. the blue water.\n    \n  \n\n  \n    The Blending Options are a way to restrict the effect of a layer (no matter which kind of layer), depending on its content or the content of the layer beneath it. The Blend If acts as a threshold: first you select which channel to use as a base, then click and drag the slider (or ALT+click to separate the slider creating a smooth transition) to define if and where the upper layer visibility will be cut, depending on the channels range the sliders don&#8217;t encompass.\n  \n\n  \n    \n      Type at the same time (Command + Option + Shift + E) if you&#8217;re on a Mac, (CTRL + ALT + Shift + E) if you&#8217;re on a PC. A new top layer is created and filled with the &#8220;merged visible&#8221; content. Trash the HiRaLoAm layer below it, and rename as HiRaLoAm the newly created one.\n    \n  \n\n  \n    \n\n    \n      After High Radius Low Amount sharpening application &#8211; click for a larger image\n    \n  \n\n  \n    \n\n    \n      Details &#8211; click for a high-resolution preview on a new window\n    \n  \n\n  \n    Part 3 &#8211; Where are we at?\n  \n\n  \n    Ok, pause. So far I&#8217;ve corrected the color with some unrepeatable moves, and the result of this is our friend the background layer, a safety net if any of the subsequent sharpening move will mess everything. Then I&#8217;ve got a second layer on top called HiRaLoAm, which used to be a masked layer with BlendIf options, that has been merged so you can now deal with it as a normal, standard and boring layer. HiRaLoAm is used to add kind of local contrast (while it&#8217;s not a true local contrast enhancer like my beloved ALCE (don&#8217;t you know ALCE?). I need just one more thing and I&#8217;ll be done.\n  \n\n  \n    Part 4 &#8211; Traditional sharpening\n  \n\n  \n    Without any further ado, please:\n  \n\n  \n    \n      Duplicate the HiRaLoAm layer and call it &#8220;USM&#8221; (which stands for Unsharp Mask).\n    \n    \n      Activate the L channel only of the USM layer and apply, guess what, the Unsharp Mask filter with Amount: 500 &#8211; Radius: 1.4px &#8211; Threshold: 0px.\n    \n  \n\n  \n    Pretty harsh, maybe? No problem, since as the great maestro Dan Margulis teaches, the light halos are the most disturbing ones: you just need a way to halve them:\n  \n\n  \n    \n      While keeping the USM layer&#8217;s L channel as the only one active, Apply the L channel from the HiRaLoAm layer, Darken blending mode, 50% opacity.\n    \n  \n\n  \n    This way, the USM darkening halos are still there, while the lightening ones have been lowered by 50% &#8211; does it make sense to you? The very last step is to apply a mask that cuts the effect in the background water &#8211; we may use the b channel as we did before, but there&#8217;s an alternative that may better fit our needs &#8211; or just show that there&#8217;s more than a single solution to any problem. Can you guess which alternative?\n  \n\n  \n    \n      Duplicate your entire document without flattening, call it RGB. Keep only its background layer, happily trash everything else. Convert to our good old friend the sRGB space.\n    \n    \n      Back to the original Lab document, add a layer mask to the USM layer, and apply into it the R channel from the RGB document.\n    \n    \n      Curve the Layer Mask to lighten it a bit (so it lets pass more of the sharpening effect &#8211; masks work this way).\n    \n  \n\n  \n    In this particular situation, we want a mask that&#8217;s pitch black in the water, so the R channel is a priori a good choice. Moreover, if you pick the R channel from a small RGB workspace, it will come already and nicely clipped in its shadows, so here: the smaller, the better.\n  \n\n  \n    \n\n    \n      Red channel as a mask\n    \n  \n\n  \n    \n\n    \n      Layers palette\n    \n  \n\n  \n    We&#8217;re happily done now! You&#8217;ve ended up with a three layers document with a color corrected background, a middle layer with HiRaLoAm targeted to the foreground, and a top layer with traditional USM, halved in its light halos, and masked appropriately. It&#8217;s been longer to describe it than to actually perform all the steps, believe me. Do you feel I went too far? You&#8217;re in good company but: No Problem, lower the opacity of the HiRaLoAm and/or USM layers to your taste &#8211; it&#8217;s always better to overdo, being able to step back.\n  \n\n  \n    Anyway, if you feel lost somewhere, add your question as a comment to this post and I&#8217;ll do my best to let you swim in shallow waters.\n  \n\n  \n    \n\n    \n      Final, processed version (click for a larger image) &#8211; © Paolo Fossati\n    \n  \n\n  \n    \n\n    \n      Details &#8211; click for a high-resolution preview on a new window\n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    Credits I wish to thank sooo much Davide D&#8217;Angelo for his help, and Paolo Fossati for the kind permission to show his picture. Please, do yourself a favor and visit their websites &#8211; wonderful images are there, waiting for you.\n  \n\n",
      tags: ["Sharpening"],
      id: 131
    });
    

    index.add({
      title: "Ahi! Un po&#8217; rude, direi&#8230; ;-)",
      category: ["Stampa"],
      content: "\n  \n    Ho visto la prima volta questo video qualche anno fa, e l&#8217;ho sempre ricordato come uno dei migliori esempi di comportamento rude nei confronti di una povera stampa 🙂\n  \n\n  \n    \n\n    \n      Clicca per avviare il video in una nuova finestra\n    \n  \n\n  \n    Considero Luminous Landscape di Michael Reichmann uno dei siti di riferimento per la fotografia professionale e per il mondo della stampa: Reichmann è indipendente e soprattutto ha uno stile sobrio, posato.\n  \n\n  \n    Per questo ho strabuzzato gli occhi quando gli ho visto fare *quello* ad una stampa&#8230; Voglio dire, si sentono i gemiti di sofferenza delle fibre della carta e del coating! Scrivo questo ora perché, la scorsa settimana, mi sono trovato letteralmente in lotta con una stampa di grande formato (164 x 230cm: siamo dovuti passare alla Hahnemuhle FineArt Baryta perché la PhotoRag era \n\n    momentaneamente non disponibile , e la FineArt è&#8230; particolarmente rigida), biecamente arrotolata come un tubo di stufa: mi è tornata in mente la performance di Michael. Ho mostrato il video all&#8217;artista per il quale stavo stampando e anche lui l&#8217;ha guardato come uno che stesse prendendo a ceffoni la mamma. E&#8217; vero, noi siamo tendenzialmente paranoici per quel che riguarda la qualità (ad esempio non si tocca la superficie della carta se non è assolutamente necessario &#8211; piccoli ritocchi con gli acrilici e poco più), certo che il colpo di matterello à la Reichmann è veramente notevole 🙂\n  \n\n  \n    Comunque, se ti interessa lo strumento (un po&#8217; caro, $250 all&#8217;epoca) che viene usato nel video, puoi leggerne una recensione.\n  \n\n  \n    Nota Bene: spero sia chiara l&#8217;innocente ironia che sottende il mio profondo rispetto per Michael Reichmann: del resto continuo a seguire regolarmente il suo sito anche dopo averlo visto brutalizzare una stampa in quel modo 🙂\n  \n\n",
      tags: [],
      id: 132
    });
    

    index.add({
      title: "Ouch! Being rude with a print :-)",
      category: ["Printing"],
      content: "\n  \n    I ran into this video some years ago, and I&#8217;ve always, vividly, remembered it as one of the worst examples of a rude behavior towards an innocent print 🙂\n  \n\n  \n    \n\n    \n      Click to play the video on a new window\n    \n  \n\n  \n    Actually, I&#8217;ve always considered Michael Reichmann&#8217;s Luminous Landscape one of the best websites around for professionals in the photography and printing business &#8211; both for his independency and, above all, for Michael&#8217;s sober style.\n  \n\n  \n    \n  \n\n  \n    That&#8217;s the why I jumped off my chair when I saw him doing *that* to a print. Man, I could almost hear the paper&#8217;s fibers and coating groan and whimper! So when, early this week, I had to literally fight with a very large format (64&#8243; x 94&#8243;), utterly curled print (we had to momentarily switch to Hahnemuhle FineArt Baryta, since PhotoRag went temporarily out of stock, being FineArt a very rigid paper), I recalled Reichmann&#8217;s performance and I showed it to the image&#8217;s author I was printing for. Our policy is to never touch the printed surface &#8211; which is quite delicate &#8211; except for micro-retouches with a fine brush and acrylic paint (we&#8217;re maniacs and paranoids, we know) so you may understand why he too opened his eyes wide just like if Michael were slapping his mother 🙂\n  \n\n  \n    That said, if you&#8217;re into mothers slapping and de-curling curled papers, please find the original product review.\n  \n\n  \n    Disclaimer: I hope you recognize here some innocent irony and a deep respect to Mr. Reichmann: I mean, I still follow his terrific website even after I&#8217;ve seen what he did to that print! 🙂\n  \n\n",
      tags: ["paper","printing"],
      id: 133
    });
    

    index.add({
      title: "Adobe 2011 Financial Analyst Meeting svela il futuro della Tecnologia al servizio dei Creativi",
      category: ["Coding @it","Photoshop @it"],
      content: "\n  \n    \n  \n\n  \n    E&#8217; proprio vero che ormai tutto fa capo alla Finanza. Ci sono più anteprime, demo e roadmap sulle tecnologie Adobe pubblicamente (di)mostrate nei video dell&#8217;Adobe Financial Analyst Meeting 2011 che in un anno di rumors su Photoshop, InDesign, Flash e Dreamweaver messi assieme.\n  \n\n  \n    Se hai a che fare con Adobe (come creativo, sviluppatore o utente) devi assolutamente vedere almeno le prime tre presentazioni di Shantanu Narayen (Adobe CEO), Kevin Lynch (Adobe CTO) e David Wadhwani (SVP/GM Digital Media) per capire in che direzione sta andando Adobe, e in che modo i loro software nuovi e vecchi cercheranno di trasformare il mondo della produzione e fruizione di contenuti creativi.\n  \n\n  \n    Ammetto di essere molto perplesso: Adobe sta affrontando una enorme trasformazione, e non mi è chiaro ora se stia andando in una direzione che mi facilita o complica le cose. Ho in cantiere altri post sull&#8217;argomento, ne parlerò molto in futuro. Intanto, un piccolo riassunto di quello che vedrai nei video (tolte le parti strettamente finanziarie).\n  \n\n  \n    \n       Vision and Strategy: Narayen presenta le aree di interesse di Adobe: Digital Media e Digital Marketing &#8211; in apparenza, equamente ripartite (!)\n    \n    \n      Innovation: Lynch entra nei dettagli di entrambe: Creative Services (come l&#8217;appena annunciata Creative Cloud), Creative Community e le Creative Apps (dando informazioni sui nuovi cicli di release, app su dispositivi mobili, ecc), più la Marketing Cloud che non mi interessa per niente se non per gli scenari da Grande Fratello che proietta sul mondo dell&#8217;e-commerce.\n    \n    \n      Digital Media Opportunity: Wadhwani racconta delle tecnologie che Adobe sta aggressivamente spingendo (su tutte HTML5), anche grazie a recenti acquisizioni (Typekit, PhoneGap) con una demo interessantissima attorno al minuto 09:00 sulle prossime features di InDesign, Edge, Flash per la distribuzione e repurposing dei contenuti su una moltitudine di dispositivi e browser diversi.\n    \n  \n\n  \n    E&#8217; un&#8217;enorme quantità di informazioni: che può aiutarti a capire dove mettere i tuoi soldi (come utente della Creative Suite, ad esempio) e su che tecnologie puntare (come developer) nel prossimo futuro. Ogni tuo commento è graditissimo come sempre! (Tra parentesi, buffo veder spuntare la faccia di Tom Waits nelle slide di Adobe 🙂 )\n  \n\n",
      tags: [],
      id: 134
    });
    

    index.add({
      title: "Adobe 2011 Financial Analyst Meeting unveils the future of Creatives and Technology",
      category: ["Coding","Photoshop"],
      content: "\n  \n    \n  \n\n  \n    Is it true that Finance rules the world now? Possibly. I&#8217;ve found more technology sneak peeks and roadmaps publicly disclosed in the November 2011 Adobe Financial Analyst Meeting videos than in a 365 days collection of blogpost rumors about Photoshop, InDesign, Flash and Dreamweaver. Wow. If you&#8217;re into Adobe (whether as a creative or developer) you must see at least the first three speeches by Shantanu Narayen (Adobe CEO), Kevin Lynch (Adobe CTO) and David Wadhwani (SVP/GM Digital Media), to properly understand where Adobe is headed to, and what their software will look like, and shape the creative world, in the years to come.\n  \n\n  \n    \n  \n\n  \n    I admit I&#8217;m still a bit puzzled: Adobe is undergoing an utterly deep transformation, and it&#8217;s not clear to me whether in a direction that better fits my needs or not. Anyway, I&#8217;ll write a lot more in the future about this, so stay tuned. A brief glance of what you&#8217;ll see in the videos &#8211; I&#8217;m going to skip most of the financial stuff.\n  \n\n  \n    \n       Vision and Strategy: Narayen presents Adobe&#8217;s (apparently equally bipartite!) commitment to Digital Media and Digital Marketing.\n    \n    \n      Innovation: Lynch enters the details of both worlds: Creative Services (like the recently announced Creative Cloud), Creative Community, and Creative Apps (with information about software release cycles, mobile devices, etc), plus the Marketing Cloud that I&#8217;m not interested into.\n    \n    \n      Digital Media Opportunity: Wadhwani talks about technologies Adobe is aggressively pushing (HTML5 above all), with the help of some of the recent company&#8217;s acquisitions (like Typekit, PhoneGag), and demoes coming features in InDesign, Edge, Flash for exporting (repurposing) digital content in a large array of devices.\n    \n  \n\n  \n    That&#8217;s a massive amount of information, and will help figuring how and where to put our money (as Creative Suite users) and time resources (as developers) in the future. Any comment from you is welcome as always!\n  \n\n  \n    (Fun to see Tom Waits face coming up as a drawing in the Adobe&#8217;s slides 🙂 )\n  \n\n",
      tags: ["Adobe","Creative Cloud","Creative Suite","Technology"],
      id: 135
    });
    

    index.add({
      title: "CS Extension Builder, aperto il programma di pre-release",
      category: ["Coding @it"],
      content: "\n  \n    Se stai pensando di sviluppare per la Adobe Creative Suite (in particolare estensioni), ci sono due notizie che ti devono interessare:\n  \n\n  \n    \n      \n        Il programma di prerelease per CS Extension Builder è finalmente aperto! Riempi questo form per chiedere di partecipare.\n      \n    \n\n    \n      \n        La scadenza della versione Trial di CS Extension Builder è stata spostata al 1 Dicembre.\n      \n    \n  \n\n  \n    Anche se non hai mai sviluppato per CS, ti sarà chiaro che ti occorre CS Extension Builder (è un plugin per Flash Builder / Eclipse).\n  \n\n  \n    &nbsp;\n  \n\n",
      tags: [],
      id: 136
    });
    

    index.add({
      title: "CS Extension Builder prerelease open",
      category: ["Coding"],
      content: "\n  \n    For those of you interested in Adobe Creative Suite developing (that is, building extensions for CS apps) there are two news that should interest you:\n  \n\n  \n    \n      \n        The CS Extension Builder prerelease program is now open! You can apply filling the participation form.\n      \n    \n\n    \n      \n        The CS Extension Builder trial expiration has been extended to December 1st &#8211; go download it!\n      \n    \n  \n\n  \n    Even if you&#8217;re not (yet!) into CS developing, you should have got now that CS Extension Builder is needed&#8230; (it&#8217;s a Flash Builder / Eclipse plugin, by the way).\n  \n\n",
      tags: ["Adobe Prerelease Programs","CS Extension Builder"],
      id: 137
    });
    

    index.add({
      title: "Color Correction Campus - slides on Sharpening",
      category: ["Photoshop"],
      content: "\n\nThe lecture about sharpening I did last October for the Color Correction Campus went apparently so well that I’ve decided to share the slides I’ve shown. Since I was just a support teacher I could afford to be totally unconventional, so I tried a daring approach to the subject.\n\nIf you feel brave enough to download the italian slides I wrote by hand with Noteshelf on my iPad, you’ll possibly have some fun too - even though I’ve been told the better part was me wandering through the class saying crazy things about sharpening techniques in Photoshop, based on the evolution of perception in human beings! Any comment (even crazy) is welcome as usual ;)\n",
      tags: ["Sharpening"],
      id: 138
    });
    

    index.add({
      title: "CCC &#8211; Bologna, slides sullo Sharpening",
      category: ["Photoshop @it"],
      content: "\n  \n     Visto l&#8217;inaspettato successo della mia breve lezione sulla maschera di contrasto per il CCC &#8211; Color Correction Campus di Marco Olivotto (tenuto lo scorso Ottobre a Bologna), ho deciso di rendere disponibili a tutti le slides che ho usato per la mia personale introduzione all&#8217;argomento.\n  \n\n  \n    \n  \n\n  \n    Nei due giorni di corso Marco ha dimostrato (come al solito) di essere un docente di talento: così io, che di mestiere invece non insegno, mi sono preso la libertà di impostare una lettura&#8230; meno convenzionale, affrontando dall&#8217;inizio il tema dal punto di vista (evolutivo) della percezione, fornendo via via strumenti di sharpening sia concettualmente che tecnicamente più avanzati, in Photoshop.\n  \n\n  \n    Puoi scaricare le slides in PDF (un po&#8217; particolari anche loro, visto che le ho disegnate a mano su iPad con Noteshelf &#8211; tra le migliori app del genere, a mio avviso) e lasciare se vuoi un tuo commento qui sotto!\n  \n\n",
      tags: [],
      id: 139
    });
    

    index.add({
      title: "Come scaricare Photoshop CS6 beta da Adobe",
      category: ["Photoshop @it"],
      content: "\n  \n    \n  \n\n  \n    I primi screenshot di Photoshop CS6 beta stanno cominciando a circolare su internet, assieme a commenti e notizie. In larga parte si tratta di indiscrezioni, e gli utenti hanno scaricato e installato il software per vie&#8230; non propriamente autorizzate. Visto che esiste un modo perfettamente legale di accedere alle pre-release di Adobe, te ne parlo qui perché mi sembra che in pochi sappiano come si fa &#8211; ed è davvero semplice!\n  \n\n  \n    Una cosa sono torrent e i forum warez (che frequentavo anch&#8217;io tempo fa): ma se sei un utente di Photoshop o qualsiasi altro software Adobe, e vorresti provare in anteprima le nuove versioni, magari contribuendo con la tua opinione, il modo più efficace è fare domanda nella\n\n    pagina ufficiale degli Adobe Prerelease Programs. E&#8217; gratis, è legale, non rischi niente.\n  \n\n  \n    \n  \n\n  \n    Ti vengono richieste alcune informazioni, piuttosto generiche: che computer hai, quanta RAM, da quanti anni usi Photoshop, se te ne intendi o no, ecc. Ma non ti fare spaventare! Adobe ha bisogno del parere di tutte le categorie di utenti, anche di quelli meno esperti. E soprattutto, puoi spiegare per quali motivi ti piacerebbe partecipare alla pre-release: e probabilmente ne hai di ottimi. Essere un betatester ufficiale da&#8217; molti vantaggi:\n  \n\n  \n    \n      Puoi fare delle richieste! Gli ingegneri Adobe sono lì, leggono e rispondono nei forum riservati.\n    \n    \n      Puoi far sentire la tua voce se qualcosa non ti piace, e suggerire come dal tuo punto di vista dovrebbe funzionare (e se siete in tanti a pensarla nello stesso modo&#8230; potrebbe cambiare davvero!)\n    \n    \n      Hai tutto il supporto per l&#8217;installazione (e soprattutto la disinstallazione, che a volte non è banale) delle varie beta-release.\n    \n  \n\n  \n    Più tutto il resto &#8211; contribuisci ai bug-report, vedi in anteprima che c&#8217;è di nuovo ed entri in contatto con professionisti di tutto il mondo. Non è fantastico?! 😉\n  \n\n  \n    Ovviamente non è detto che la tua domanda venga accettata, ma tentare non costa nulla &#8211; per cui vai e prova!\n  \n\n",
      tags: ["Adobe Prerelease Programs","Photoshop CS6"],
      id: 140
    });
    

    index.add({
      title: "How to download Photoshop CS6 beta from Adobe",
      category: ["Photoshop"],
      content: "\n  \n    Screenshots of Photoshop CS6 beta are starting to appear on the internet, with dressing of rumors and comments. By and large, they come from people who downloaded the software from unreliable sources, and usually got into vivid troubles installing and uninstalling it. Since there&#8217;s a perfectly legal way to access prerelease software from Adobe, I&#8217;d like to mention it here because it seems there&#8217;s a lot of misconception and lack of information about it.\n  \n\n  \n    If you&#8217;re into torrent downloads and warez forums, I&#8217;m fine &#8211; I used to hang there too, some years ago. But if you&#8217;re a Photoshop professional (or any other Adobe&#8217;s software user) who would like to have a look to the coming version, and maybe contribute with your suggestions and bug reports, go and apply in the official\n\n    Adobe Prerelease Programs page. It&#8217;s free, it&#8217;s legal, and fun.\n  \n\n  \n    \n  \n\n  \n    You&#8217;re required to give some basic information about yourself, nothing too personal &#8211; like how many years you&#8217;ve been in the industry, your expertise level, how much RAM your computer has, etc. Mind you, they&#8217;re looking for all kind of users with all kind of Mac/PC setup, so don&#8217;t feel intimidate if you&#8217;re just a beginner. You&#8217;re allowed to explain why you would like to join the prerelease program (and you may have some good reason to be in the number). Being an official beta tester is a privileged position!\n  \n\n  \n    \n      \n        You can request for features! Adobe engineers are there, listening and answering.\n      \n    \n\n    \n      \n        You can add your voice to the group of people who dislike some new behavior (they may change it, eventually) or suggest a better one from your point of view.\n      \n    \n\n    \n      \n        You have all the support for beta releases&#8217; installation and uninstallation (not a trivial task, sometimes).\n      \n    \n  \n\n  \n    Plus the usual stuff &#8211; contribute in the software testing and bug reporting, lurk the new stuff first and be able to ask what it&#8217;s for, be in touch with professionals around the world. Isn&#8217;t that wonderful?\n  \n\n  \n    I can&#8217;t assure your application will be accepted, but it&#8217;s worth a try and does no harm. Go now!\n  \n\n",
      tags: ["Adobe Prerelease Programs","Photoshop CS6"],
      id: 141
    });
    

    index.add({
      title: "Hahnemuhle fa marcia indietro sui 64&#8243;",
      category: ["Stampa"],
      content: "\n  \n    Pochissimo tempo dopo aver manifestato la volontà di interrompere le linee produttive a 64 pollici / 162 cm delle carte DFA (Digital FineArt) &#8211; decisione che ha causato molti malumori tra gli utenti professionali &#8211; Hahnemuhle con una comunicazione ufficiale ha fatto retromarcia. Con l&#8217;eccezione della FineArt Baryta, il cui taglio a 64&#8243; verrà effettivamente interrotto nel 2012.\n  \n\n  \n    Di seguito, l&#8217;originale del comunicato stampa.\n  \n\n  \n    \n  \n\n  \n    \n      Dear customers,\n    \n\n    \n      according to new market development and the positive trend of large fine art format printing we have decided NOT to eliminate all 64” rolls from our DFA range. Following fine art papers will remain on our stock in 64” as standard articles:\n    \n\n    \n      &#8211; Photo Rag Baryta 315, 64” Ref. 10 643 199 &#8211; Photo Rag US 305, 64” Ref. 10 643 277\n    \n\n    \n      Only FineArt Baryta 325, 64”, Ref. 10 643 477 will be discontinued next year. According to our actual stock level we are supposed to have the product till February on 64” format, so that you can still order this product (as long as stock lasts).\n    \n  \n\n  \n    Per quanto io sia scettico sul recente manifestarsi di un &#8220;positivo trend&#8221; (più che altro c&#8217;è stata una piccola rivolta a livello internazionale) mi fa piacere che il management Hahnemuhle abbia preso in considerazione la nostra opinione, dimostrando che cambiare idea non è reato.\n  \n\n  \n    &nbsp;\n  \n\n",
      tags: [],
      id: 142
    });
    

    index.add({
      title: "Hahnemuhle changes its mind about 64&#8243;!",
      category: ["Printing"],
      content: "\n  \n    After a short while from the decision to drop 64&#8243; production in the DFA (Digital Fine Art) line of papers, which caused a lot of buzz in the business, Hahnemuhle has changed its mind. With an official communication, they state that they&#8217;re going to discontinue only FineArt Baryta 64&#8243; in 2012.\n  \n\n  \n    Here&#8217;s the original letter they wrote.\n  \n\n  \n    \n  \n\n  \n    \n      Dear customers,\n    \n\n    \n      according to new market development and the positive trend of large fine art format printing we have decided NOT to eliminate all 64” rolls from our DFA range. Following fine art papers will remain on our stock in 64” as standard articles:\n    \n\n    \n      &#8211; Photo Rag Baryta 315, 64” Ref. 10 643 199 &#8211; Photo Rag US 305, 64” Ref. 10 643 277\n    \n\n    \n      Only FineArt Baryta 325, 64”, Ref. 10 643 477 will be discontinued next year. According to our actual stock level we are supposed to have the product till February on 64” format, so that you can still order this product (as long as stock lasts).\n    \n  \n\n  \n    I&#8217;m very happy to hear such a good news &#8211; even though I&#8217;m a bit skeptical about the &#8220;positive trend&#8221; they mention: it looks like a nice way to admit an error and apologize. Hahnemuhle, you&#8217;re welcome!\n  \n\n",
      tags: ["Hahnemuhle @en","print"],
      id: 143
    });
    

    index.add({
      title: "Hahnemuhle cessa la produzione di carta 64&#8243;",
      category: ["Stampa"],
      content: "\n  \n    Se stai cercando carta Hahnemuhle PhotoRag Baryta oppure FineArt Baryta in rotolo da  64 pollici (164 cm) puoi fermarti qui &#8211; la produzione sarà interrotta nel 2012.\n  \n\n  \n    A quanto dicono in Hahnemuhle, le scarse vendite del massimo formato non sono più in grado di assorbire i costi di produzione, e la linea a 64&#8243; sarà interamente sostituita (per tutta la serie Digital Fineart) dalla 60&#8243; (152 cm).\n  \n\n  \n    \n  \n\n  \n    Sentiti alcuni colleghi all&#8217;estero, hanno tutti manifestato una certa sorpresa. Hahnemuhle produce alcune tra le migliori carte sul mercato, e sembra piuttosto strano che a livello mondiale non ci sia una domanda sufficiente per garantirne la produzione.\n  \n\n  \n    Se stai leggendo fin qui, probabilmente sei nel business della stampa fineart di largo formato &#8211; vorrei condividere con te un paio di considerazioni.\n  \n\n  \n    \n      \n        Uno degli elementi chiave del marchio Hahnemuhle è sempre stata la qualità dei prodotti: se l&#8217;azienda è leader nel settore delle carte fineart (di per se una nicchia particolare nel mercato della stampa tout-court), dovrebbero mantenere aperte tutte le linee di produzione, anche se alcune non sono direttamente remunerative &#8211; altri prodotti possono e devono assorbire le eventuali perdite &#8211; ed in specie non chiudere quelle che sono il top della gamma.\n      \n    \n\n    \n      \n        Dal punto di vista del marketing, la scelta di abbandonare la produzione a 64&#8243; è a mio modo di vedere errata. I clienti di questo genere di prodotto sono professionisti al vertice nei loro rispettivi settori (stampa fineart di largo formato, fotografi, artisti) e lasciarli letteralmente senza scelta è assurdo.\n      \n    \n  \n\n  \n    Detto questo, è bizzarro che una stampante come la Epson 11880 abbia una decina di centimetri extra che nessuno userà più, se con non un paio di carte appunto Epson (la Exhibition Fiber e la Premium Luster, non esattamente le mie prime scelte). Tra le altre cose, la crisi economica ha portato al restringimento delle stampe in galleria di 4 pollici&#8230;\n  \n\n",
      tags: [],
      id: 144
    });
    

    index.add({
      title: "Hahnemuhle drops the production of 64&#8243; papers",
      category: ["Printing"],
      content: "\n  \n    Are you looking for Hahnemuhle PhotoRag Baryta or FineArt Baryta inkjet paper 64&#8243; rolls? Bad times, they&#8217;re going to discontinue the production in 2012.\n  \n\n  \n    Due to little interest about such a large format (Hahnemuhle representatives said) the sales don&#8217;t account for the production costs; and the well known fine-art paper company is going to stick to the more common 60&#8243; size as the wider available for the entire inkjet serie.\n  \n\n  \n    \n  \n\n  \n    I&#8217;ve personally been in touch with some printing professionals around the world, and they all showed, to some degree, astonishment. Hahnemuhle manufactures some of the best papers in the market, and it seems strange that the entire world isn&#8217;t able to absorb the 64&#8243; stocks that would account for keeping the product line alive.\n  \n\n  \n    If you&#8217;re reading this one, you&#8217;re probably in the business of large format printing, so let me share a couple of personal thoughts with you.\n  \n\n  \n    \n      \n        One of Hahnemuhle&#8217;s marketing strengths has always been the product&#8217;s quality. If they promote themselves as the leading company in the field (of fine art printing, which, per se, is a niche in the printing business as a whole), they should keep their products range intact even if the sub-niche of very-very-large-format isn&#8217;t completely financially rewarding. Other branches may support it.\n      \n    \n\n    \n      \n        From a marketing point of view, the choice to drop the 64&#8243; production is, in my humble opinion, damaging the company in a bad way. Top clients for this range of papers are top professionals in their own fields (whether printers, photographers, artists) and to leave them with no options at all is absurd.\n      \n    \n  \n\n  \n    That said, it&#8217;s funky that a printer like the Epson 11880 has now few extra inches that nobody will fill anymore, except for a couple of papers from&#8230; Epson (the Exhibition Fiber and, afaik, the Premium Luster, not my best choice for sure). So they&#8217;ll say that in 2012 the economy crisis got entire countries on their knees and shrank the print&#8217;s size in galleries by 4&#8243;.\n  \n\n",
      tags: ["Hahnemuhle @en","print"],
      id: 145
    });
    

    index.add({
      title: "Problemi con Adobe Extension Manager ed OSX Lion",
      category: ["Coding @it","Photoshop @it"],
      content: "\n  \n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    \n      &#8220;L&#8217;estensione non contiene una firma valida. L&#8217;estensione non sarà installata&#8221;.\n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    Se hai incontrato questo messaggio di errore tentando di installare un&#8217;estensione in Photoshop nella Creative Suite 5 e 5.5 sotto Mac OSX Lion sei incappato in un problema noto, per il quale Adobe ha rilasciato un fix temporaneo. E&#8217; disponibile anche un thread negli Adobe Forum.\n  \n\n  \n    Tra l&#8217;altro è ora disponibile (negli archivi dell&#8217;Adobe MAX 2011) un video molto interessante dal titolo How to Develop and Monetize your Creative Suite Extensions, che contiene alcuni succosi indizi sul futuro delle CS SDK direttamente dalla voce del Product Manager Gabriel Tavridis.\n  \n\n",
      tags: [],
      id: 146
    });
    

    index.add({
      title: "Adobe Extension Manager and OSX Lion issue",
      category: ["Coding","Photoshop"],
      content: "\n  \n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    \n      The extension does not contain valid signature. The extension will not be installed.\n    \n  \n\n  \n    &nbsp;\n  \n\n  \n    If you&#8217;ve run into this error message while trying to install a Photoshop CS5 / CS5.5 extension on a Mac (Lion &#8211; 10.7.2), be aware that this is an Adobe Extension Manager CS5 / CS5.5 known bug. Adobe will soon release an update, in the meantime they&#8217;ve provided a temporary fix. A full thread in Adobe Forum is available as well.\n  \n\n  \n    By the way, an interesting MAX 2011 archive video called How to Develop and Monetize your Creative Suite Extensions has been published, with sneak peeks of the CS SDKs future by product manager Gabriel Tavridis &#8211; get a look at it!\n  \n\n",
      tags: ["Adobe Extension Manager","Creative Suite","CS SDK","extension"],
      id: 147
    });
    

    index.add({
      title: "Codename: ZEBRA",
      category: ["Photoshop"],
      content: "\n\nI’m currently involved in a particularly challenging prepress job for a particularly prestigious publishing house, facing an astonishingly close deadline (kind of: has to be printed… yesterday). The good part is that I’ve managed to gather around it some of my best friends - who, by chance, are among the most talented retouchers in the country.\n\nI promise, if we’ll survive, I’ll tell you the whole story. In the meantime let me introduce you codename ZEBRA project’s team: MO, MD, TF and yours truly, aka the NWA (no weekend allowed) guys.\n",
      tags: ["prepress"],
      id: 148
    });
    

    index.add({
      title: "Sharpening session at CCC",
      category: ["Photoshop"],
      content: "\n\nNext weekend I’ll be teaching at the CCC - Color Correction Campus in Bologna, Italy (a two day long, intense, Dan Margulis’ style, hands on color correction class by Marco Olivotto) a session about Sharpening.\n\nBecause I know several professional teachers, I  know I’m not one of them - I lack their technique, I don’t pay attention to meta-language, I’m not that used to speech in public. I do consultancy, but it’s an entirely different job.  Since I rarely join conferences, my presence in the next CCC is raising some fuss: not from students, who are happily unaware, but from highly experienced colleagues and talented friends who plan to gather there and finally hear Davide teaching unsharp mask &amp; C (for, they say, “it’s what you do best!”) - with the net result to make me even more nervous.\n\nWhat would the world look like without friends?! Anyway, sharpening is a vast topic, and in a little time it is difficult to properly explore it - with students probably coming from very different backgrounds. So I’ve tried to focus on a… sort of concept-presentation: I’ll be describing first, and very quickly, some universal techniques, just because I think it’s fair to provide paying people with practical tips that can be inserted in anyone’s workflow - for there are things that are known to be effective with all kind of images. Then, since CCC philosophy is to promote an inquiring attitude (and to arouse thinking) rather than dispense Photoshop recipes, I’ll spend all my time asking questions about perception - as like in color correction, the need for sharpening comes from the different ways we, humans, perceive the world out there, compared to any possible super califragilistic device. Being the reason that our hardware (the eye) co-evolved with our software (the brain) for some hundreds of millions of years, and basically, like the great Dan Margulis once wrote:\n\n\n  “we see whatever we feel like to see”\n\n\nFrom this point of view, in my opinion, its easier to put in context and find the why of every sharpening technique - being them either:\n\n\n  the try to fill the gap between our perception and the camera’s eye;\n  a defense against sharpening side-effects (like noise, unwanted detail, color shifts, etc);\n\n\nKind of an ambitious plan, but possibly a viable one in such a short time! At least I hope so. I’ve just completed the session’s slides - which will help keeping myself in track; I usually hate slides, so this time I’ve been drawing them by hand on the iPad with Noteshelf and a Wacom Bamboo Stylus.\n\nIt’s not the best setup in the world for the professional presenter (who I’m not - Marco asked me naively: “are you using InDesign?”), but it ended up being so much fun that I recommend it - Noteshelf is the only iOS hand-writing app that features a zoom, as far as I know, plus a whole set of fluorescent markers and colored pens, several paper templates and good wrist protection. Give it a try!\n",
      tags: ["Sharpening"],
      id: 149
    });
    


var store = [{
    "title": "Photoshop JS Native Applications Course UPDATE",
    "link": "/development/Photoshop-native-applications-update.html",
    "image": "/wp-content/uploads/2020/05/AppIcon.png",
    "date": "May 27, 2020",
    "category": ["Development"],
    "excerpt": "As you have heard, I’ve spent the last few months working on my next video-course titled JavaScript Native Applications for..."
},{
    "title": "Photoshop JS Native Applications Course (work-in-progress)",
    "link": "/development/Photoshop-native-applications-wip.html",
    "image": "/wp-content/uploads/2020/03/NWJS.png",
    "date": "March 1, 2020",
    "category": ["Development"],
    "excerpt": "TL;DR I am working on a new series of video-tutorials on Photoshop Development. The subject is going to be Native..."
},{
    "title": "The state of Hasselblad Flextight scanners, FlexColor and the 3F format",
    "link": "/2019/12/2019-12-07/2019-12-07-hasselblad-flextight-flexcolor-3f/",
    "image": "/wp-content/uploads/2019/12/flexcolor.png",
    "date": "December 7, 2019",
    "category": ["Photoshop"],
    "excerpt": "In case you missed it, Flextight scanners have vanished from the Hasselblad website around May 2019 and macOS Catalina doesn’t..."
},{
    "title": "Leanpub alternatives, IMHO",
    "link": "/2019/10/2019-10-15-leanpub-alternatives-IMHO/",
    "image": "/wp-content/uploads/2019/10/leanpub.png",
    "date": "October 15, 2019",
    "category": ["Digital Publishing"],
    "excerpt": "If you’re into technical writing, read along. I’ve used Leanpub’s services for two books of mine out of three, and..."
},{
    "title": "Notarizing installers for macOS Catalina",
    "link": "/2019/04/notarizing-installers-for-macos-catalina/",
    "image": "/wp-content/uploads/2019/05/gatekeeper.png",
    "date": "September 20, 2019",
    "category": ["HTML Panels tips"],
    "excerpt": "According to Apple (read here the whole article): The Apple notary service is an automated system that scans your software..."
},{
    "title": "Printing on demand a Leanpub book with Lulu.com",
    "link": "/2019/04/printing-leanpub-books-on-lulu/",
    "image": "/wp-content/uploads/2019/04/lulu.png",
    "date": "April 13, 2019",
    "category": ["Personal"],
    "excerpt": "\"And this shall be mine, human...\" Authoring a book is awesome – especially when you’re done with it – but..."
},{
    "title": "Migrating from WordPress to Jekyll on Netlify",
    "link": "/2019/03/migrating-from-wordpress-to-jekyll-on-netlify/",
    "image": "/wp-content/uploads/2019/03/jekyll.png",
    "date": "March 5, 2019",
    "category": ["Personal"],
    "excerpt": "After years, I’ve been able to kiss WordPress bye-bye and migrate to a fully static site build with Jekyll and..."
},{
    "title": "Adobe Extension Manager Error Codes",
    "link": "/2019/02/adobe-extension-manager-error-codes/",
    "image": "/wp-content/uploads/2019/02/AEM.png",
    "date": "February 21, 2019",
    "category": ["CEP"],
    "excerpt": "Here is a list of Adobe Extension Manager ExManCmd error codes. I don’t use zxp files to install my products, but..."
},{
    "title": "Professional Photoshop Scripting is published!",
    "link": "/2019/02/professional-photoshop-scripting-v1/",
    "image": "/wp-content/uploads/2019/02/cover300.jpg",
    "date": "February 13, 2019",
    "category": ["Scripting"],
    "excerpt": "After 10 months since the launch of the Early Access Program, I am incredibly happy to announce that my book..."
},{
    "title": "Spectrum CSS VueJS Component: DropDown",
    "link": "/2018/12/spectrum-css-vue-js-component-dropdown/",
    "image": "/wp-content/uploads/2018/12/vuespectrum.png",
    "date": "December 16, 2018",
    "category": ["CEP"],
    "excerpt": "In a previous post I’ve introduced the recently open-sourced Spectrum CSS. Here, I’ll be demonstrating how to use them to build a..."
},{
    "title": "ESTK to be replaced by a Visual Studio Code plug-in",
    "link": "/2018/12/estk-to-be-replaced-by-a-visual-studio-code-plug-in/",
    "image": "/wp-content/uploads/2018/12/vsc.jpg",
    "date": "December 12, 2018",
    "category": ["Scripting"],
    "excerpt": "Please do yourself a favour and read this Medium post by Lead Technical Evangelist Ash Ryan Arnwine about the future..."
},{
    "title": "Adobe Spectrum CSS open-sourced!",
    "link": "/2018/11/adobe-spectrum-css-open-sourced/",
    "image": "/wp-content/uploads/2018/11/DB-2018-11-18-at-11.10.38.png",
    "date": "November 18, 2018",
    "category": ["CEP","Scripting"],
    "excerpt": "In case you’ve missed the news, Adobe has open-sourced the Spectrum CSS – the stylesheets they’re using for Photoshop’s own..."
},{
    "title": "Professional Photoshop Scripting EAP Update",
    "link": "/2018/06/professional-photoshop-scripting-eap-update/",
    "image": "/wp-content/uploads/2018/06/RkFdg4PI.png",
    "date": "June 11, 2018",
    "category": ["Scripting"],
    "excerpt": "I’ve added 60 pages to accommodate an amazing new chapter on Adobe Generator! The book is now 388 pages strong and..."
},{
    "title": "Automated check for corrupted image files with Python and ImageMagick",
    "link": "/2018/05/automated-check-for-corrupted-image-files-with-python-and-imagemagick/",
    "image": "/wp-content/uploads/2018/05/corrupted.png",
    "date": "May 8, 2018",
    "category": ["Coding"],
    "excerpt": "How do you check if an image file (tiff, psd, psb) is corrupted, other than looking at its thumbnail with..."
},{
    "title": "Professional Photoshop Scripting Course &#8211; Early Access Program",
    "link": "/2018/04/professional-photoshop-scripting-course-early-access-program/",
    "image": "/wp-content/uploads/2018/04/PSScriptingCover.png",
    "date": "April 20, 2018",
    "category": ["Scripting"],
    "excerpt": "I’m glad to announce that I have opened the Early Access Program to my new and very much awaited Professional Photoshop..."
},{
    "title": "Luminosity Masks: How Does It Really Work?",
    "link": "/2018/03/luminosity-masks-how-does-it-really-work/",
    "image": "/wp-content/uploads/2018/03/BlogPostIcon-1.png",
    "date": "March 10, 2018",
    "category": ["Photoshop"],
    "excerpt": "In this guest post, the Photoshop Plug In developer Scott Murdock tackles the apparently familiar topic of Luminosity Masks –..."
},{
    "title": "A New Course on Adobe CEP Panels: Native Installers and Automation!",
    "link": "/2018/01/ultimate-guide-native-installers-automated-build-system/",
    "image": "/wp-content/uploads/2018/01/UG_thumb.png",
    "date": "January 23, 2018",
    "category": ["CEP"],
    "excerpt": "After Adobe Photoshop HTML Panels Development, I’ve now published a new Course, titled “The Ultimate Guide to Native Installers and Automated..."
},{
    "title": "HTML Panel Tips #25: CC 2018 Survival Guide",
    "link": "/2017/10/html-panel-tips-25-cc-2018-survival-guide/",
    "image": "/wp-content/uploads/2015/06/CC2015.png",
    "date": "October 23, 2017",
    "category": ["CEP"],
    "excerpt": "That time of the year has come, and Photoshop CC 2018 is here. Read along to find out Everything You..."
},{
    "title": "Photoshop Scripting Course Update (August 2017)",
    "link": "/2017/08/photoshop-scripting-course-update-august-2017/",
    "image": "/wp-content/uploads/2017/08/Risultati-immagini-per-bob-the-builder.png",
    "date": "August 25, 2017",
    "category": ["Scripting"],
    "excerpt": "In October 2016 I’ve announced to be working on a course about Photoshop Scripting. Is it ready for release? According..."
},{
    "title": "Third-party Photoshop Panels: Configurator Reloaded",
    "link": "/2017/05/photoshop-panel-configurator-reloaded/",
    "image": "/wp-content/uploads/2017/05/configurator-reloaded.png",
    "date": "May 25, 2017",
    "category": ["CEP"],
    "excerpt": "I’m not used to talking about commercial products I’ve not personally developed on my blog, but I’ve decided it might..."
},{
    "title": "HTML Panel Tips #24: Fixing ZXP Timestamping errors",
    "link": "/2017/04/html-panel-tips-24-fixing-zxp-timestamping-errors/",
    "image": "/wp-content/uploads/2017/04/bug.png",
    "date": "April 29, 2017",
    "category": ["CEP"],
    "excerpt": "Recently, running the ZXPSignCmd command line utility to sign and timestamp HTML Panels has proved to cause errors. The problem lies..."
},{
    "title": "Double USM v2 for Photoshop has been released!",
    "link": "/2017/02/double-usm-v2-for-photoshop-has-been-released/",
    "image": "/wp-content/uploads/2017/02/Dark-Theme_2.png",
    "date": "February 20, 2017",
    "category": ["Photoshop"],
    "excerpt": "A major reworking of my Sharpening extension for Photoshop has been released. I’m happy to introduce you to the new..."
},{
    "title": "HTML Panel Tips #23: JavascriptObfuscator API Gulp.js Plugin",
    "link": "/2017/02/html-panel-tips-23-javascriptobfuscator-api-gulp-js-plugin/",
    "image": "/wp-content/uploads/2017/02/lock1.png",
    "date": "February 11, 2017",
    "category": ["CEP"],
    "excerpt": "When developing HTML Panels, I’m always quite fanatic about code privacy: for a variety of reasons, I don’t want users..."
},{
    "title": "Vue.js – Nonlinear Sliders with Computed Properties",
    "link": "/2017/01/vue-js-nonlinear-sliders-with-computed-properties/",
    "image": "/wp-content/uploads/2016/06/logo.png",
    "date": "January 14, 2017",
    "category": ["CEP"],
    "excerpt": "term definition another definition While working on the forthcoming version of my DoubleUSM script, which I’m porting to HTML Panels,..."
},{
    "title": "I am authoring a new Course! And it will be on Photoshop Scripting",
    "link": "/2016/10/new-course-photoshop-scripting/",
    "image": null,
    "date": "October 20, 2016",
    "category": ["Scripting"],
    "excerpt": "You might have noticed that updates on my blog are getting a bit sparse: no worries, I’m up and running as usual...."
},{
    "title": "Vue.js – Binding a Component in a v-for loop to the Parent model",
    "link": "/2016/08/vue-js-binding-a-component-in-a-v-for-loop-to-the-parent-model/",
    "image": "/wp-content/uploads/2016/06/logo.png",
    "date": "August 2, 2016",
    "category": ["CEP"],
    "excerpt": "Learning Vue.js is fun – if I run into a problem that has taken me some head scratching time to solve and/or..."
},{
    "title": "HTML Panel Tips #22: Protecting your Code",
    "link": "/2016/07/html-panel-tips-22-protecting-your-code/",
    "image": "/wp-content/uploads/2016/07/obfuscation.jpg",
    "date": "July 27, 2016",
    "category": ["CEP"],
    "excerpt": "About one year ago I had a so-called aha moment and decided to write a book. I had two or..."
},{
    "title": "HTML Panel Tips #21: Photoshop CC 2015.5 survival guide",
    "link": "/2016/06/html-panel-tips-21-photoshop-cc2015-5-2016-survival-guide/",
    "image": "/wp-content/uploads/2016/06/CC2015.png",
    "date": "June 22, 2016",
    "category": ["CEP"],
    "excerpt": "Today Photoshop CC 2015.5 has been released even if everybody wanted CC 2016. This will cause you some headaches: let..."
},{
    "title": "HTML Panel Tips #20: Javascript Frameworks",
    "link": "/2016/06/html-panel-tips-20-javascript-frameworks-angularjs-vuejs/",
    "image": "/wp-content/uploads/2016/06/logo.png",
    "date": "June 10, 2016",
    "category": ["CEP"],
    "excerpt": "Javascript definitely has a problem: too many frameworks! But as my Color Correction maestro Dan Margulis would put it: “the..."
},{
    "title": "The Adobe Photoshop HTML Panels Development Course is available!",
    "link": "/2016/03/adobe-photoshop-html-panels-development-course-released/",
    "image": "/wp-content/uploads/2016/03/BookVideo.jpg",
    "date": "March 28, 2016",
    "category": ["CEP"],
    "excerpt": "I’m super excited to officially announce that the Adobe Photoshop HTML Panels Development course – Build and Market Adobe Creative Cloud extensions – is..."
},{
    "title": "HTML Panels book: Website launched!",
    "link": "/2016/02/html-panels-book-website-launched/",
    "image": "/wp-content/uploads/2016/02/wpbook-1.png",
    "date": "February 11, 2016",
    "category": ["CEP"],
    "excerpt": "Panels people! From now you can subscribe to http://htmlpanelsbook.com and book (pun intended!) your discount code for the “Adobe Photoshop HTML Panels”..."
},{
    "title": "HTML Panels book Update (Dec 2015)",
    "link": "/2015/12/update-on-html-panels-book-december-2015/",
    "image": "/wp-content/uploads/2015/10/Copertina-small.png",
    "date": "December 20, 2015",
    "category": ["CEP"],
    "excerpt": "Hello readers, I’m happy to inform you that the “Adobe Photoshop HTML Panels Development” book is in good shape! I’ve..."
},{
    "title": "HTML Panel Tips #19: CC2015.1 (CEP6.1) Node.js Fixes",
    "link": "/2015/12/html-panel-tips-19-cc2015-1-cep6-1-node-js-fixes/",
    "image": "/wp-content/uploads/2015/12/1.0.0.png",
    "date": "December 1, 2015",
    "category": ["CEP"],
    "excerpt": "The latest release of Photoshop introduces, among the rest, a new version of the Common Extensibility Platform, CEP6.1. In turn,..."
},{
    "title": "Im writing the book 'Photoshop HTML Panels development'!",
    "link": "/2015/10/announcing-photoshop-html-panels-development-book-project/",
    "image": "/wp-content/uploads/2015/10/Copertina-small.png",
    "date": "October 18, 2015",
    "category": ["CEP"],
    "excerpt": "Hello readers, I’m happy to publicly announce that I’m writing a book (temporarily) titled: Photoshop HTML Panels developmentBuild and Market..."
},{
    "title": "De-uglify ExtendScript Toolkit on Retina Displays",
    "link": "/2015/09/de-uglify-extendscript-toolkit-on-retina-displays/",
    "image": "/wp-content/uploads/2015/09/ESTK_retina.png",
    "date": "September 20, 2015",
    "category": ["Scripting"],
    "excerpt": "I’m no big fan of ESTK for a variety of reasons; when I finally replaced my old MacBookPro with a..."
},{
    "title": "HTML Panel Tips #18: Photoshop JSON Callback",
    "link": "/2015/09/html-panel-tips-18-photoshop-json-callback/",
    "image": "/wp-content/uploads/2015/09/PhotoshopJSONCallback.png",
    "date": "September 3, 2015",
    "category": ["CEP"],
    "excerpt": "CC 2015 previews a new Photoshop Event listening system and deprecates the \"com.adobe.PhotoshopCallback\", due to a bug that makes all the extension receiving..."
},{
    "title": "In memory of Mike Hale and PS-Scripts.com",
    "link": "/2015/08/in-memory-of-mike-hale-and-ps-scripts-com/",
    "image": "/wp-content/uploads/2015/08/1398164_profile_pic.jpg",
    "date": "August 11, 2015",
    "category": ["Scripting"],
    "excerpt": "As you might have noticed, PS-Scripts – the independent forum devoted to Photoshop scripting - is offline since May 2015..."
},{
    "title": "Partial Serial Number Verification System in Javascript",
    "link": "/2015/07/partial-serial-number-verification-system-in-javascript/",
    "image": "/wp-content/uploads/2015/07/rsa_encrypt_decrypt.png",
    "date": "July 5, 2015",
    "category": ["Scripting"],
    "excerpt": "Back in 2007, developer Brandon Staggs wrote a brilliant article about software licensing, showing how to implement what he calls..."
},{
    "title": "HTML Panel Tips #17: CC2015 Survival Guide",
    "link": "/2015/06/html-panel-tips-17-cc2015-survival-guide/",
    "image": "/wp-content/uploads/2015/06/CC2015.png",
    "date": "June 16, 2015",
    "category": ["CEP"],
    "excerpt": "Superstitious people in Italy think that seventeen is a bad luck number - yet I’m not gullible and I think it must..."
},{
    "title": "HTML Panels Tips #16 AngularJS Binding bug patch",
    "link": "/2014/12/html-panels-tips-16-angularjs-input-binding-bug-patch/",
    "image": "/wp-content/uploads/2014/12/AngularJS.png",
    "date": "December 5, 2014",
    "category": ["CEP"],
    "excerpt": "CEP 5.2 has a bug (affecting Macs with non US keyboards) that prevents AngularJS to realize binding in HTML Panels..."
},{
    "title": "HTML Panels Tips #15 Asynchronous vs. Synchronous",
    "link": "/2014/12/html-panels-tips-15-asynchronous-vs-synchronous-evalscript/",
    "image": "/wp-content/uploads/2014/12/asyncsync.gif",
    "date": "December 2, 2014",
    "category": ["CEP"],
    "excerpt": "Today’s tip focuses on the Asynchronous nature of CSInterface.evalScript() calls, and on ways to make it work in a synchronous..."
},{
    "title": "HTML Panels Tips: #14 Flyout Menu",
    "link": "/2014/10/html-panels-tips-14-flyout-menu/",
    "image": "/wp-content/uploads/2014/10/screenshot.png",
    "date": "October 17, 2014",
    "category": ["CEP","HTML Panels"],
    "excerpt": "With Photoshop CC 2014.2 (implementing CEP 5.2) it’s finally possible to have flyout menus in HTML Panels too - as..."
},{
    "title": "HTML Panels Tips: #13 Automate ZXP Packaging with Gulp.js",
    "link": "/2014/08/html-panels-tips-13-automate-zxp-packaging-with-gulp-js/",
    "image": "/wp-content/uploads/2014/08/gulp.png",
    "date": "August 13, 2014",
    "category": ["CEP"],
    "excerpt": "As a HTML Panels Tips: #10 Packaging / ZXP Installers follow-up, in this tip I will show you how to automate (i.e...."
},{
    "title": "Introducing Fixel Detailizer 2 PS",
    "link": "/2014/07/introducing-fixel-detailizer/",
    "image": "/wp-content/uploads/2014/06/Detailizer-GUI.jpg",
    "date": "July 26, 2014",
    "category": ["Photoshop"],
    "excerpt": "I’m glad to announce the release on Adobe Add-ons of a powerful contrast booster for Photoshop: Fixel Detailizer 2 PS. Let’s..."
},{
    "title": "HTML Panels Tips: #12 CEP Application Events",
    "link": "/2014/07/html-panels-tips-12-cep-application-events/",
    "image": "/wp-content/uploads/2014/07/Log.png",
    "date": "July 10, 2014",
    "category": ["CEP"],
    "excerpt": "HTML Panels can listen to CEP Application Events - that is to say, Events dispatched by the Host (Photoshop, InDesign…) when..."
},{
    "title": "HTML Panels Tips: #11 CEP Events (ExternalObject)",
    "link": "/2014/07/html-panels-tips-11-externalobject-cep-events/",
    "image": "/wp-content/uploads/2014/07/JSX2CEP-panel.png",
    "date": "July 10, 2014",
    "category": "– CEP",
    "excerpt": "The new CEP5 (from Photoshop CC 2014 onwards) has introduced the possibility to dispatch Custom Events from JSX and listen..."
},{
    "title": "Photoshop CC 2014 (v15.0) ready!",
    "link": "/2014/06/photoshop-cc-2014-v15-ready-extensions/",
    "image": "/wp-content/uploads/2011/11/HTML5.png",
    "date": "June 22, 2014",
    "category": ["Photoshop"],
    "excerpt": "The support of Flash panels has been officially dropped with the release or Photoshop CC 2014 (internal version: 15.0) - meaning that..."
},{
    "title": "PS-Panels-Boilerplate on GitHub!",
    "link": "/2014/06/ps-panels-boilerplate-on-github/",
    "image": "/wp-content/uploads/2014/06/Octocat.jpg",
    "date": "June 22, 2014",
    "category": ["CEP"],
    "excerpt": "I’ve created a new project on my GitHub account where I will collect both Boilerplate code for HTML Panels and..."
},{
    "title": "HTML Panels Tips: #10 Packaging / ZXP Installers",
    "link": "/2014/05/html-panels-tips-10-packaging-zxp-installers/",
    "image": "/wp-content/uploads/2014/05/mxi.png",
    "date": "May 22, 2014",
    "category": ["CEP"],
    "excerpt": "While there is a more user friendly app such as Adobe Packager which is a great simple app for simple needs, I..."
},{
    "title": "VitaminBW 1.0 &#8211; User Manual",
    "link": "/2014/02/vitamin-bw-photoshop-user-manual/",
    "image": "/wp-content/uploads/2014/02/VitaminBW.jpg",
    "date": "February 15, 2014",
    "category": ["Photoshop"],
    "excerpt": "This is the User Manual for VitaminBW 1.0, the Photoshop CC/CS6 script that spices up your black and white conversions!..."
},{
    "title": "Introducing Vitamin BW for Photoshop CC / CS6",
    "link": "/2014/02/vitamin-bw-script-extension-for-photoshop-cc-cs6/",
    "image": "/wp-content/uploads/2013/11/VitaminBW_icon_120.png",
    "date": "February 15, 2014",
    "category": ["Photoshop"],
    "excerpt": "Give a fresh breath to your Black &amp; White conversions with Vitamin BW, a script for Photoshop CC / CS6..."
},{
    "title": "Epson 11880 printer head deep cleaning",
    "link": "/2014/02/epson-11880-printer-head-deep-cleaning/",
    "image": "/wp-content/uploads/2014/02/Parking.jpg",
    "date": "February 11, 2014",
    "category": ["Photoshop"],
    "excerpt": "I’ve been printing large format with an Epson 11880 for some years now, and I know when it’s time to..."
},{
    "title": "HTML Panels Tips: #9 Persistence",
    "link": "/2014/02/html-panels-tips-9-persistence/",
    "image": "/wp-content/uploads/2014/02/persistent.png",
    "date": "February 8, 2014",
    "category": ["CEP"],
    "excerpt": "How do you make the HTML Panel’s session persist even if the panel is closed in Photoshop? Each time an..."
},{
    "title": "HTML Panels Tips: #8 Photoshop Events, Take 2",
    "link": "/2014/02/html-panels-tips-8-photoshop-events-pshostadapter-libraries/",
    "image": "/wp-content/uploads/2014/02/PSEvents.png",
    "date": "February 7, 2014",
    "category": ["CEP"],
    "excerpt": "There’s a second way to listen to Photoshop Events from an HTML Panel - i.e. using the PSHostAdapters libraries -..."
},{
    "title": "HTML Panels Tips: #7 Photoshop Events, Take 1",
    "link": "/2014/02/html-panels-tips-7-events-photoshopregisterevent-photoshopcallback/",
    "image": "/wp-content/uploads/2014/02/PSEvents1.png",
    "date": "February 7, 2014",
    "category": ["CEP"],
    "excerpt": "This tip shows you the first way to make an HTML Panel listen and react to Photoshop Events: via PhotoshopRegisterEvent...."
},{
    "title": "HTML Panels Tips: #6 integrating Topcoat CSS",
    "link": "/2014/02/html-panels-tips-6-integrating-topcoat-css/",
    "image": "/wp-content/uploads/2014/02/Topcoat.png",
    "date": "February 2, 2014",
    "category": ["CEP"],
    "excerpt": "Today’s tip is about integration of the Topcoat CSS library with themeManager.js in Photoshop HTML Panels. Adobe’s Topcoat is: a brand..."
},{
    "title": "HTML Panels Tips: #5 passing Objects from JSX to HTML",
    "link": "/2014/02/html-panels-tips-5-passing-objects-from-jsx-to-html-json/",
    "image": "/wp-content/uploads/2014/02/Console.png",
    "date": "February 1, 2014",
    "category": ["CEP"],
    "excerpt": "This tip covers how to pass Objects from Photoshop (the JSX file) to the HTML Panel. Panels technology doesn’t allow..."
},{
    "title": "HTML Panels Tips: #4 passing Objects from HTML to JSX",
    "link": "/2014/01/html-panels-tips-4-passing-objects-from-html-to-jsx/",
    "image": "/wp-content/uploads/2014/01/Object.png",
    "date": "January 31, 2014",
    "category": ["CEP"],
    "excerpt": "How do you pass complex Objects from the HTML Panel to a Photoshop’s ExtendScript function in a JSX file? You..."
},{
    "title": "HTML Panels Tips: #3 Get data from JSX and send it to HTML",
    "link": "/2014/01/html-panels-tips-3-get-data-from-jsx-send-it-to-html/",
    "image": "/wp-content/uploads/2014/01/JSX_2_HTML.png",
    "date": "January 31, 2014",
    "category": ["CEP"],
    "excerpt": "This is how Photoshop (i.e. the JSX) sends back data to the HTML Panel. The HTML Panel communicates with Photoshop..."
},{
    "title": "HTML Panels Tips: #3 Get data from JSX and send it to HTML",
    "link": "/2014/01/html-panels-tips-3-get-data-from-jsx-send-it-to-html/",
    "image": "/wp-content/uploads/2014/01/JSX_2_HTML.png",
    "date": "January 31, 2014",
    "category": ["CEP"],
    "excerpt": "This is how Photoshop (i.e. the JSX) sends back data to the HTML Panel. The HTML Panel communicates with Photoshop..."
},{
    "title": "HTML Panels Tips: #2 Including multiple JSX",
    "link": "/2014/01/html-panels-tips-2-including-multiple-jsx/",
    "image": "/wp-content/uploads/2014/01/jsx.jpg",
    "date": "January 30, 2014",
    "category": ["CEP"],
    "excerpt": "A simple way to dynamically evaluate multiple JSX files in the Scripting context of Photoshop within HTML Panels. Back in..."
},{
    "title": "HTML Panels Tips: #1 Debugging",
    "link": "/2014/01/html-panels-tips-1-debugging/",
    "image": "/wp-content/uploads/2014/01/ChromeDebuggingTool.jpg",
    "date": "January 30, 2014",
    "category": ["CEP"],
    "excerpt": "Since latest updates, Photoshop (and possibly other Adobe apps too) need remote connecting for debugging HTML Extensions - here’s a..."
},{
    "title": "2014 Code Learning Wishlist (and All the Rest)",
    "link": "/2014/01/2014-wishlist/",
    "image": "/wp-content/uploads/2014/01/2014.jpg",
    "date": "January 27, 2014",
    "category": ["Coding","Photoshop"],
    "excerpt": "I&#8217;ve thought that &#8220;New Year Resolutions&#8221; sounds bad compared to &#8220;Wishlist&#8221; &#8211; doesn&#8217;t it? Just an excuse to post a..."
},{
    "title": "Up to 10 Color Samplers &#8211; Photoshop CC (14.2)",
    "link": "/2014/01/up-to-10-color-samplers-photoshop-cc-14-2/",
    "image": "/wp-content/uploads/2014/01/PS_CC_10ColorSamplers.jpg",
    "date": "January 16, 2014",
    "category": ["Photoshop"],
    "excerpt": "Thanks to last Photoshop World Codeathon in Las Vegas, we&#8217;ve finally broken the 4 Color Samplers limit in the Info..."
},{
    "title": "Adobe Scripting: collaborative code sharing",
    "link": "/2013/12/adobe-scripting-collaborative-code-sharing-github/",
    "image": "/wp-content/uploads/2013/12/ExtendScriptToolkit.png",
    "date": "December 8, 2013",
    "category": ["Coding"],
    "excerpt": "An open letter to fellow devs If you&#8217;re involved building third party solution for Adobe applications (mainly Scripts/Extensions) and you..."
},{
    "title": "Photoshop Script Boilerplate Code on GitHub",
    "link": "/2013/11/photoshop-script-boilerplate-code-on-github/",
    "image": "/wp-content/uploads/2013/11/GitHub-Mark-120px-plus.png",
    "date": "November 18, 2013",
    "category": ["Coding","CoffeeScript","ExtendScript / Javascript","Photoshop"],
    "excerpt": "Back when I started writing Scripts for Photoshop, uhm, five or six years ago, I&#8217;d have loved having a (set..."
},{
    "title": "Sublime Text snippet: try/catch wrapping in CoffeeScript",
    "link": "/2013/11/sublime-text-snippet-try-catch-block-wrap-coffeescript/",
    "image": "/wp-content/uploads/2013/11/wrap_try_catch_coffeescript.png",
    "date": "November 13, 2013",
    "category": ["Coding","CoffeeScript"],
    "excerpt": "One of the Sublime Text editor&#8217;s cool features are Snippets (bits of code you use very often and want to..."
},{
    "title": "&#8216;PS Projects&#8217; for Photoshop CC/CS6",
    "link": "/2013/10/introducing-ps-projects-for-photoshop-cc-cs6/",
    "image": "/wp-content/uploads/2013/09/PSProjects_MainDialog.png",
    "date": "October 4, 2013",
    "category": ["Extensions and Scripts","Photoshop","PS Projects"],
    "excerpt": "I&#8217;m glad to announce my latest script for Photoshop CC / CS6, called PS Projects. What is it? PS Projects..."
},{
    "title": "PS Projects for Photoshop CC / CS6 User Manual",
    "link": "/2013/10/ps-project-for-photoshop-cc-cs6-user-manual/",
    "image": "/wp-content/uploads/2013/09/PSProjects_UserManualThumb.png",
    "date": "October 4, 2013",
    "category": ["Scripting"],
    "excerpt": "PS Projects is a Photoshop CC / CS6 script that implements the idea of Project Files (a lightweight container of..."
},{
    "title": "BridgeTalk Export as Binary failure",
    "link": "/2013/09/bridgetalk-export-as-binary-failures/",
    "image": "/wp-content/uploads/2013/09/Export_as_Binary.png",
    "date": "September 16, 2013",
    "category": ["Coding","ExtendScript / Javascript"],
    "excerpt": "BridgeTalk, the system that enables messaging between applications in the Adobe scripting ecosystem, is prone to failure when evaluating functions..."
},{
    "title": "Photoshop CC Color Range Enhancements",
    "link": "/2013/09/photoshop-cc-color-range-enhancements/",
    "image": "/wp-content/uploads/2013/09/Photoshop_CC_Color_Range.png",
    "date": "September 10, 2013",
    "category": ["Photoshop"],
    "excerpt": "Latest Photoshop CC update (version 14.1) has introduced a good deal of new features. One of them is the refined..."
},{
    "title": "ScriptUI tip: decoupling components&#8217; Event handling",
    "link": "/2013/09/scriptui-tip-decoupling-components-event-handling/",
    "image": "/wp-content/uploads/2013/09/ScriptUI_Event_propagation.png",
    "date": "September 7, 2013",
    "category": ["Coding","ExtendScript / Javascript"],
    "excerpt": "In a ScriptUI Window different components are usually registered for Events, and fire their own Handlers. You can build some..."
},{
    "title": "Testing minified JS Libraries in ExtendScript",
    "link": "/2013/08/testing-minified-js-libraries-in-extendscript/",
    "image": "/wp-content/uploads/2013/08/TestingJSMinify.png",
    "date": "August 29, 2013",
    "category": ["Coding","ExtendScript / Javascript"],
    "excerpt": "It&#8217;s not uncommon, when scripting for Adobe applications, to borrow JS libraries that have been originally written for web development...."
},{
    "title": "ScriptUI Events: call(), dispatchEvent(), notify()",
    "link": "/2013/08/extendscript-scriptui-events-call-notify-dispatchevent/",
    "image": "/wp-content/uploads/2013/08/ScriptUI_Events.png",
    "date": "August 4, 2013",
    "category": ["Coding","ExtendScript / Javascript"],
    "excerpt": "In your code you may need to run a ScriptUI component&#8217;s callback, or simulate a user interaction, maybe as a..."
},{
    "title": "L&#8217;App per iPad di TIAB: Sharpening in Photoshop",
    "link": "/2013/07/app-ipad-tiab-sharpening-photoshop/",
    "image": "/wp-content/uploads/2013/07/TIAB.jpg",
    "date": "July 25, 2013",
    "category": ["Photoshop @it"],
    "excerpt": "Teacher in a Box ha pubblicato su App Store la versione per iPad del suo &#8220;Videocorso per Photoshop Sharpening: Dettaglio e..."
},{
    "title": "CPT &#8211; Channel Power Tools for Photoshop CC",
    "link": "/2013/06/cpt-channel-power-tools-update-photoshop-cc/",
    "image": "/wp-content/uploads/2013/06/icona-Installer_CPT.png",
    "date": "June 29, 2013",
    "category": ["Extensions and Scripts","Photoshop"],
    "excerpt": "I&#8217;m happy to report that my dear friend and talented coder Giuliana Abbiati aka Cromaline has just updated to version..."
},{
    "title": "Adobe Extension Manager CC and Exchange issues",
    "link": "/2013/06/adobe-extension-manager-cc-and-exchange-issues/",
    "image": "/wp-content/uploads/2013/06/cc_app.png",
    "date": "June 21, 2013",
    "category": ["Extensions and Scripts","Photoshop"],
    "excerpt": "Soon after the public launch of Creative Cloud applications, some users reported to have run into troubles installing extensions from..."
},{
    "title": "Sublime Text 2 and 3 ExtendScript Photoshop package (Updated)",
    "link": "/2013/05/sublime-text-2-extendscript-photoshop-build-package/",
    "image": "/wp-content/uploads/2013/05/logo.jpg",
    "date": "May 6, 2013",
    "category": ["Coding","ExtendScript / Javascript"],
    "excerpt": "This post contains a Sublime Text 2 custom build package that lets you run ExtendScript code targeting Adobe Photoshop. I&#8217;ve..."
},{
    "title": "DropBox, Git and SublimeText on the Mac",
    "link": "/2013/05/dropbox-git-sublime-text-mac-osx/",
    "image": "/wp-content/uploads/2013/05/Git_DropBox_SublimeText.jpg",
    "date": "May 1, 2013",
    "category": ["Coding"],
    "excerpt": "This post will describe how to setup a local Git repository, sync it with a private DropBox repository for cloud..."
},{
    "title": "Theme Switcher &#8211; free extension for Photoshop",
    "link": "/2013/04/theme-switcher-free-extension-for-photoshop/",
    "image": "/wp-content/uploads/2013/04/Featured.png",
    "date": "April 23, 2013",
    "category": ["Extensions and Scripts","Theme Switcher"],
    "excerpt": "It&#8217;s now available through Adobe Exchange my latest, and most minimal so far, free extension for Photoshop called Theme Switcher. It..."
},{
    "title": "Dan Margulis&#8217; Modern Photoshop Color Workflow, beta-readers interview",
    "link": "/2013/04/dan-margulis-modern-color-workflow-book-interview/",
    "image": "/wp-content/uploads/2013/04/ModernPhotoshopColorWorkflow_cover.jpg",
    "date": "April 3, 2013",
    "category": ["Photoshop"],
    "excerpt": "Color Correction Maestro Dan Margulis has just published a new book about his latest research, titled &#8220;Modern Photoshop Color Workflow..."
},{
    "title": "Parametric Curves free script User Guide",
    "link": "/2013/03/parametric-curves-script-for-photoshop-user-guide/",
    "image": "/wp-content/uploads/2013/03/ParametricCurves_Featured.jpg",
    "date": "March 26, 2013",
    "category": ["Coding","Extensions and Scripts","Parametric Curves","Photoshop"],
    "excerpt": "Parametric Curves is a free Photoshop script that lets you plot mathematically defined (Javascript) Curves Adjustment layers. If you wonder..."
},{
    "title": "Gradients and Parametric Curves in Photoshop",
    "link": "/2013/03/gradients-and-parametric-curves-in-photoshop/",
    "image": "/wp-content/uploads/2013/03/CurvesGradients.jpg",
    "date": "March 5, 2013",
    "category": ["Extensions and Scripts","Parametric Curves","Photoshop"],
    "excerpt": "I&#8217;m not a FX kind of guy, but as a developer I&#8217;m fascinated by the astonishing complex results you can..."
},{
    "title": "Presets management with DropDownList and XML in ExtendScript",
    "link": "/2013/03/presets-management-with-dropdownlist-and-xml-in-extendscript/",
    "image": "/wp-content/uploads/2013/03/XML_DropDownList_Preset_demo.jpg",
    "date": "March 2, 2013",
    "category": ["Coding"],
    "excerpt": "ScriptUI windows can contain several controls (checkboxes, sliders, etc), and to setup a Preset system is a very handy way..."
},{
    "title": "How to open and retouch Hasselblad 3F scanner files in Photoshop",
    "link": "/2013/02/how-to-open-and-retouch-hasselblad-3f-scanner-files-in-photoshop-cs6/",
    "image": "/wp-content/uploads/2013/02/FCIconApp.jpg",
    "date": "February 23, 2013",
    "category": ["Photoshop"],
    "excerpt": "[Updated on March 2019] If you try to open in Photoshop a 3F (a file with extension .fff, the raw..."
},{
    "title": "Introducing Filter Forge",
    "link": "/2013/02/introducing-filter-forge-for-photoshop/",
    "image": "/wp-content/uploads/2013/02/FilterForge_SixtiesSwirl.jpg",
    "date": "February 17, 2013",
    "category": ["Filter Forge","Photoshop"],
    "excerpt": "Filter Forge is a PS plugin; actually it&#8217;s more than a PS plugin, Filter Forge is a PS plugins&#8217; container..."
},{
    "title": "Double USM #3: Examples",
    "link": "/2013/01/double-usm-photoshop-sharpening-script-3-examples/",
    "image": "/wp-content/uploads/2013/01/Sharpening_example_03_DoubleUSM.jpg",
    "date": "January 28, 2013",
    "category": ["Double USM","Extensions and Scripts","Photoshop"],
    "excerpt": "Double USM is a brand new Sharpening script for Photoshop; in this third post of the series I&#8217;ll show you..."
},{
    "title": "Double USM #2: Features",
    "link": "/2013/01/double-usm-photoshop-sharpening-script-2-features/",
    "image": "/wp-content/uploads/2013/01/DoubleUSM_interface.jpg",
    "date": "January 28, 2013",
    "category": ["Double USM","Extensions and Scripts","Photoshop"],
    "excerpt": "Double USM is the brand new Sharpening script for Photoshop that I&#8217;ve coded; in this second post of the series..."
},{
    "title": "Double USM Photoshop Sharpening Script #1: Introduction",
    "link": "/2013/01/double-usm-photoshop-sharpening-script-1-introduction/",
    "image": "/wp-content/uploads/2013/01/HighFrequencyDetail.jpg",
    "date": "January 27, 2013",
    "category": ["Double USM","Extensions and Scripts","Photoshop"],
    "excerpt": "Double USM is the brand new Sharpening script for Photoshop that I&#8217;ve coded, let me introduce it to you in..."
},{
    "title": "Selling digital books with Apple: iBooksAuthor, InDesign, Digital Publishing Suite",
    "link": "/2013/01/selling-digital-books-with-apple-ibooksauthor-indesign-adobe-digital-publishing-suite/",
    "image": "/wp-content/uploads/2013/01/iBookStore.jpg",
    "date": "January 14, 2013",
    "category": ["Adobe DPS","Digital Publishing","iBooksAuthor"],
    "excerpt": "If you&#8217;re willing to produce digital books to be sold by Apple, be aware that each one of the available..."
},{
    "title": "CS Extensions: Photoshop CS6 Extensions and Scripts website",
    "link": "/2013/01/cs-extensions-photoshop-cs6-panels-scripts-resource/",
    "image": "/wp-content/uploads/2013/01/home_next.jpg",
    "date": "January 11, 2013",
    "category": ["Coding","Extensions and Scripts","Photoshop"],
    "excerpt": "Are you looking for Photoshop Extensions? I&#8217;m pleased to announce that CS Extensions, a brand new website entirely dedicated to..."
},{
    "title": "Capture One 7 DB &#8211; Lens Correction bug",
    "link": "/2012/11/capture-one-7-db-lens-correction-bug/",
    "image": "/wp-content/uploads/2012/11/PhaseOneLensCorrection.png",
    "date": "November 8, 2012",
    "category": ["Photography Post-Production"],
    "excerpt": "The recently released Capture One Pro 7.0 software (from PhaseOne) shows a bug when dealing with Lens Correction &#8211; particularly..."
},{
    "title": "Action recordable scripts in Photoshop",
    "link": "/2012/11/action-recordable-scripts-in-photoshop/",
    "image": "/wp-content/uploads/2012/11/DB_-2012-11-04-at-17.24.34.png",
    "date": "November 7, 2012",
    "category": ["Coding","ExtendScript / Javascript"],
    "excerpt": "Scripts, by default, leave traces in the History palette; a common practice is to pack and hide into a labeled,..."
},{
    "title": "ScriptUI &#8211; BridgeTalk persistent Window examples",
    "link": "/2012/11/scriptui-bridgetalk-persistent-window-examples/",
    "image": "/wp-content/uploads/2012/10/ESTK_palette.png",
    "date": "November 1, 2012",
    "category": ["Coding","ExtendScript / Javascript"],
    "excerpt": "In a previous post I&#8217;ve shown how to use either app.refresh() or the waitForRedraw() function in a loop in order..."
},{
    "title": "ScriptUI Window in Photoshop – Palette vs. Dialog",
    "link": "/2012/10/scriptui-window-in-photoshop-palette-vs-dialog/",
    "image": "/wp-content/uploads/2012/10/PS_palettes.png",
    "date": "October 28, 2012",
    "category": ["Scripting"],
    "excerpt": "ExtendScript - one of the scripting languages supported by Creative Suite applications, and the only one cross-platform - has several..."
},{
    "title": "CryptoJS Tutorial For Dummies",
    "link": "/2012/10/crypto-js-tutorial-cryptography-for-dummies/",
    "image": "/wp-content/uploads/2012/10/DB_-2012-10-13-at-11.05.08.png",
    "date": "October 14, 2012",
    "category": ["Scripting"],
    "excerpt": "[Updated October 16th 2012 with corrected information from CryptoJS author Jeff Mott - look for the UPDATED tag below] The..."
},{
    "title": "Hahnemühle PhotoRag Baryta paper issues",
    "link": "/2012/10/hahnemuhle-photorag-baryta-paper-issues-surface-imperfections/",
    "image": "/wp-content/uploads/2012/10/hahnemuhle.jpg",
    "date": "October 1, 2012",
    "category": ["Printing"],
    "excerpt": "&lt;/p&gt; &lt;/p&gt; This is what you get right in the middle of a 64 inches (162 cm) roll of Hahnemuhle..."
},{
    "title": "Decomposing Sharpening #5 Arithmetic matters",
    "link": "/2012/09/decomposing-sharpening-part5-arithmetic-matters/",
    "image": "/wp-content/uploads/2012/09/Arithmetic.jpg",
    "date": "September 15, 2012",
    "category": ["Decomposing Sharpening","Photoshop"],
    "excerpt": "The previous post of the series (#4 The Lab way) introduced a different strategy for splitting in an updatable fashion..."
},{
    "title": "Decomposing Sharpening #4 The Lab way",
    "link": "/2012/09/decomposing-sharpening-part4-the-lab-way/",
    "image": "/wp-content/uploads/2012/09/FinalResult1.jpg",
    "date": "September 8, 2012",
    "category": ["Decomposing Sharpening","Photoshop"],
    "excerpt": "If you&#8217;ve followed this series, you know that I&#8217;m involved in a scripting project for Photoshop that deals with sharpening,..."
},{
    "title": "Decomposing Sharpening #3 Workaround",
    "link": "/2012/09/decomposing-sharpening-part3-workaround/",
    "image": "/wp-content/uploads/2012/09/FinalResult.jpg",
    "date": "September 8, 2012",
    "category": ["Decomposing Sharpening","Photoshop"],
    "excerpt": "In the previous post I&#8217;ve approached appealing yet wrong solutions to the problem of splitting in a scriptable-friendly way Dark..."
},{
    "title": "Decomposing Sharpening #2 Mistakes",
    "link": "/2012/09/decomposing-sharpening-part2-mistakes/",
    "image": "/wp-content/uploads/2012/09/Original.png",
    "date": "September 8, 2012",
    "category": ["Decomposing Sharpening","Photoshop"],
    "excerpt": "In the previous post I cited a tool for Photoshop I&#8217;m working on &#8211; which is currently on its beta..."
},{
    "title": "Decomposing Sharpening #1 Introduction",
    "link": "/2012/09/decomposing_sharpening_part_1/",
    "image": "/wp-content/uploads/2012/09/HighPass.jpg",
    "date": "September 8, 2012",
    "category": ["Decomposing Sharpening","Photoshop"],
    "excerpt": "I&#8217;m currently working on a project of mine (Photoshop script) that involves sharpening. This has been giving me the opportunity..."
},{
    "title": "Adobe Exchange &#8211; featuring ALCE",
    "link": "/2012/07/adobe-exchange-cs-extensions-featuring-alce/",
    "image": "/wp-content/uploads/2012/07/ALCE.png",
    "date": "July 13, 2012",
    "category": ["Photoshop"],
    "excerpt": "Today Adobe has released in the Adobe Labs a preview of the new Adobe Exchange! Quoting the official website: The..."
},{
    "title": "Earthquake down time",
    "link": "/2012/06/earthquake-italy-emilia-romagna/",
    "image": "/wp-content/uploads/2012/06/ChiesaCamposanto.jpg",
    "date": "June 23, 2012",
    "category": ["Personal"],
    "excerpt": "Have you heard about Italy&#8217;s recent earthquakes (May 2012) in the Emilia-Romagna region? I did, because I live(d) some 15..."
},{
    "title": "Dorsi digitali Phase One e Adobe Camera Raw &#8211; Paneling &#8220;bug&#8221;",
    "link": "/2012/04/dorsi-digitali-phase-one-iiq-adobe-camera-raw-paneling-bug/",
    "image": "/wp-content/uploads/2012/04/PhaseOne_IQ180_paneling_bug3.jpg",
    "date": "April 29, 2012",
    "category": ["Post-produzione"],
    "excerpt": "Se usi un dorso digitale Phase One (serie IQ oppure P/P+) ti è stato detto di adoperare esclusivamente Capture One..."
},{
    "title": "Phase One digital backs and Adobe Camera Raw &#8211; Paneling &#8220;bug&#8221;",
    "link": "/2012/04/phase-one-digital-back-adobe-camera-raw-7-cs6-paneling-bug/",
    "image": "/wp-content/uploads/2012/04/PhaseOne_IQ180_paneling_bug3.jpg",
    "date": "April 26, 2012",
    "category": ["Photography Post-Production"],
    "excerpt": "&nbsp;&lt;/p&gt; If you&#8217;re a Phase One digital back owner (IQ and P+ series) you&#8217;ve been told that Phase One Capture..."
},{
    "title": "iBooks Author &#8211; Compressione delle immagini",
    "link": "/2012/04/ibooks-author-compressione-jpg-conversione-colore-ridimensionamento-delle-immagini/",
    "image": "/wp-content/uploads/2012/04/iBooksAuthor-opening1.jpg",
    "date": "April 7, 2012",
    "category": ["Digital Publishing @it","iBooksAuthor @it"],
    "excerpt": "Se sei seriamente interessato alla qualità delle immagini per l&#8217;editoria elettronica, vuoi sapere quello che accade loro quando iBooks Author..."
},{
    "title": "iBooks Author &#8211; Immagini a pieno schermo",
    "link": "/2012/04/ibooks-author-immagini-pieno-schermo-fullscreen-senza-bordi/",
    "image": "/wp-content/uploads/2012/04/iBooksAuthor-fullscreen.jpg",
    "date": "April 6, 2012",
    "category": ["Digital Publishing @it","iBooksAuthor @it"],
    "excerpt": "Una delle più interessanti (e ovvie) possibilità degli iBooks è l&#8217;ingrandimento delle immagini a pieno schermo, in modo che anche..."
},{
    "title": "iBooks Author &#8211; Image compression",
    "link": "/2012/04/ibooks-author-image-compression-color-comparison/",
    "image": "/wp-content/uploads/2012/04/iBooksAuthor-opening1.jpg",
    "date": "April 6, 2012",
    "category": ["Digital Publishing","iBooksAuthor"],
    "excerpt": "If you&#8217;re serious about image quality, you want to know what happens to your pictures when iBooks Author exports a..."
},{
    "title": "iBooks Author &#8211; Fullscreen images",
    "link": "/2012/04/ibooks-author-fullscreen-images/",
    "image": "/wp-content/uploads/2012/04/iBooksAuthor-fullscreen.jpg",
    "date": "April 5, 2012",
    "category": ["Digital Publishing","iBooksAuthor"],
    "excerpt": "One of the most interesting yet obvious features of iBooks is the possibility to make an image to pop up..."
},{
    "title": "Panoramiche col medio formato&#8230; lo stato dell&#8217;arte?",
    "link": "/2012/03/phaseone-iq180-schneider-kreuznach-reallyrightstuff-panorama-stitching/",
    "image": null,
    "date": "March 23, 2012",
    "category": ["Post-produzione"],
    "excerpt": "Sono abbastanza fortunato da avere clienti che usano il Medio Formato (con dorsi digitali) per unire più scatti in panoramiche..."
},{
    "title": "Medium format Stitching, what a tough business!",
    "link": "/2012/03/phaseone-iq180-schneider-kreuznach-reallyrightstuff-panorama-stitching/",
    "image": "/wp-content/uploads/2012/03/PhaseOne.png",
    "date": "March 22, 2012",
    "category": ["Photography Post-Production"],
    "excerpt": "I&#8217;m lucky enough, as a post-producer and consultant, to be paid by photographers who want to use Medium Format (i.e...."
},{
    "title": "Media di più livelli in Photoshop",
    "link": "/2012/03/media-di-piu-livelli-in-photoshop-opacita-oggetto-avanzato/",
    "image": null,
    "date": "March 4, 2012",
    "category": ["Photoshop @it"],
    "excerpt": "Tre versioni di una singola immagine, da sinistra: MO, DB, GA. In Correzione Colore si è recentemente scoperto che produrre..."
},{
    "title": "How to stack layers in Photoshop as a Mean",
    "link": "/2012/03/how-to-stack-layers-in-photoshop-as-mean/",
    "image": "/wp-content/uploads/2012/03/Layered-Stonehenge.jpg",
    "date": "March 4, 2012",
    "category": ["Photoshop"],
    "excerpt": "Three versions of a single image stitched together. Left to right: MO, DB, GA. In Color Correction it is a..."
},{
    "title": "Photoshop Touch per iPad",
    "link": "/2012/02/photoshop-touch-per-ipad/",
    "image": "/wp-content/uploads/2012/02/Photoshop_V1.jpg",
    "date": "February 27, 2012",
    "category": ["Photoshop @it"],
    "excerpt": "Photoshop Touch per iPad è finalmente uscito. E ora Adobe, cortesemente, potresti far evolvere Photoshop per Mac/Win in un software leggermente..."
},{
    "title": "Photoshop Touch for iPad",
    "link": "/2012/02/photoshop-touch-for-ipad/",
    "image": "/wp-content/uploads/2012/02/Photoshop_V1.jpg",
    "date": "February 27, 2012",
    "category": ["Photoshop"],
    "excerpt": "Photoshop Touch for iPad is finally out. And now please Adobe, could you gently make Photoshop for Mac/Win to evolve..."
},{
    "title": "Adobe Configurator 3, prerelease aperta",
    "link": "/2012/02/adobe-configurator-3-prerelease-aperta/",
    "image": null,
    "date": "February 22, 2012",
    "category": ["Photoshop @it"],
    "excerpt": "In questo post, Jonathan Ferman (Creative Suite Solutions Product Manager di Adobe), annuncia la disponibilità per il download della prima..."
},{
    "title": "Adobe Configurator 3 prerelease open!",
    "link": "/2012/02/adobe-configurator-3-prerelease-open/",
    "image": "/wp-content/uploads/2012/02/AdobeConfigurator3.png",
    "date": "February 22, 2012",
    "category": ["Photoshop"],
    "excerpt": "According to this post by Jonathan Ferman (Creative Suite Solutions Product Manager at Adobe), engineers are at work on the..."
},{
    "title": "PP nella Fotografia d&#8217;Arte Contemporanea (#1: Dettagli)",
    "link": "/2012/01/post-produzione-fotografia-arte-contemporanea-dettagli/",
    "image": "/wp-content/uploads/2012/01/RetouchMap.png",
    "date": "January 31, 2012",
    "category": ["Post-produzione"],
    "excerpt": "Fotografia d&#8217;Arte Contemporanea significa, tra le altre cose, che il lavoro del tuo cliente l&#8217;Artista finirà stampato, montato e incorniciato..."
},{
    "title": "ALCE con panoramiche 360°, un tutorial",
    "link": "/2012/01/alce-panorama-360-equirettangolare-tutorial/",
    "image": null,
    "date": "January 31, 2012",
    "category": ["Photoshop @it","Post-produzione"],
    "excerpt": "Cappella di Somma, chiesa di San Giovanni a Carbonara (Napoli) © Alfonso Grotta - www.alfonsogrotta.com Ho appena pubblicato un tutorial..."
},{
    "title": "ALCE and 360° panoramic images, a tutorial",
    "link": "/2012/01/alce-and-360-panoramic-images-tutorial/",
    "image": null,
    "date": "January 31, 2012",
    "category": ["Photography Post-Production","Photoshop"],
    "excerpt": "Somma chapel, San Giovanni a Carbonara church (Naples, Italy) Photography © Alfonso Grotta &#8211; www.alfonsogrotta.com I&#8217;ve just posted a tutorial..."
},{
    "title": "PP in the Contemporary Art Photography business (#1: Details)",
    "link": "/2012/01/post-production-in-the-contemporary-art-photography-business-details/",
    "image": "/wp-content/uploads/2012/01/Friends.jpg",
    "date": "January 18, 2012",
    "category": ["Photography Post-Production"],
    "excerpt": "Contemporary Art Photography means, among the rest, that the work of your client the Artist will end up printed, framed..."
},{
    "title": "Post-Produzione nel business della Fotografia d&#8217;Arte Contemporanea (Intro)",
    "link": "/2012/01/post-produzione-nel-business-della-fotografia-darte-contemporanea-intro/",
    "image": null,
    "date": "January 17, 2012",
    "category": ["Post-produzione"],
    "excerpt": "Il mio lavoro principale è la Post-Produzione, in gran parte per un artista nel business della Fotografia d&#8217;Arte Contemporanea. Bizzarra..."
},{
    "title": "Post-Production in the Contemporary Art Photography business (Intro)",
    "link": "/2012/01/post-production-in-the-contemporary-art-photography-business-intro/",
    "image": null,
    "date": "January 17, 2012",
    "category": ["Photography Post-Production"],
    "excerpt": "My fulltime job is Post-Production, most of the time for an artist in the Contemporary Art Photography business. Weird niche..."
},{
    "title": "Subclipse Version Control in Flash Builder: installazione e setup",
    "link": "/2011/12/version-control-in-flash-builder-subclipse-subversion-installazione-e-setup/",
    "image": null,
    "date": "December 23, 2011",
    "category": ["Coding @it"],
    "excerpt": "&#8220;Salva con Nome&#8221; è la più semplice ed usata forma di Version Control &#8211; ovvero la gestione delle revisioni di..."
},{
    "title": "Version Control in Flash Builder: installation and setup",
    "link": "/2011/12/version-control-installation-setup-subclipse-flash-builder/",
    "image": null,
    "date": "December 21, 2011",
    "category": ["Coding"],
    "excerpt": "&#8220;Save As&#8221; is the very first, basic, form of Version Control &#8211; the business of tracking and/or reverting the changes..."
},{
    "title": "Sviluppare per Creative Suite e Photoshop",
    "link": "/2011/12/sviluppare-estensioni-creative-suite-photoshop/",
    "image": null,
    "date": "December 12, 2011",
    "category": ["Coding @it","Photoshop @it"],
    "excerpt": "Dopo aver letto e condiviso le riflessioni di Gabe Harbs (noto sviluppatore InDesign) sull&#8217;argomento, mi piacerebbe aggiungere qui un paio di considerazioni..."
},{
    "title": "Creative Suite extensibility and Photoshop",
    "link": "/2011/12/creative-suite-extensibility-and-photoshop/",
    "image": null,
    "date": "December 11, 2011",
    "category": ["Coding","Photoshop"],
    "excerpt": "Having read the (InDesign developer) Gabe Harbs&#8217; post on the subject, I&#8217;d like to add here a couple of personal points..."
},{
    "title": "Is Adobe at a crossroads?",
    "link": "/2011/11/adobe-crossroads/",
    "image": "/wp-content/uploads/2011/11/Crossroad.jpg",
    "date": "November 28, 2011",
    "category": ["Photoshop"],
    "excerpt": "Adobe Systems is undergoing a crucial, fast (and some would add: questionable) shift that will reverberate on us all &#8211;..."
},{
    "title": "Adobe al bivio",
    "link": "/2011/11/adobe-al-bivio-quali-cambiamenti-ci-aspettano/",
    "image": "/wp-content/uploads/2011/11/Crossroad.jpg",
    "date": "November 26, 2011",
    "category": ["Photoshop @it"],
    "excerpt": "Adobe sta cambiando rotta in maniera risoluta, veloce e, nell’opinione di molti, discutibile. Te ne sei accorto? Te ne accorgerai..."
},{
    "title": "Sharpening &#8211; uno studio",
    "link": "/2011/11/sharpening-uno-studio/",
    "image": null,
    "date": "November 23, 2011",
    "category": ["Photoshop @it"],
    "excerpt": "L&#8217;ultimo esempio che ho presentato nella mia lezione sullo Sharpening al CCC in Ottobre è stata la correzione di un&#8217;immagine subacquea...."
},{
    "title": "Sharpening case study",
    "link": "/2011/11/sharpening-case-study/",
    "image": "/wp-content/uploads/2011/11/03-Final_L.jpg",
    "date": "November 21, 2011",
    "category": ["Photoshop"],
    "excerpt": "As a last example of my CCC Sharpening lecture in October, I did a correction of an underwater image. Later,..."
},{
    "title": "Ahi! Un po&#8217; rude, direi&#8230; ;-)",
    "link": "/2011/11/cosa-non-fare-stampa-arrotolata/",
    "image": null,
    "date": "November 20, 2011",
    "category": ["Stampa"],
    "excerpt": "Ho visto la prima volta questo video qualche anno fa, e l&#8217;ho sempre ricordato come uno dei migliori esempi di..."
},{
    "title": "Ouch! Being rude with a print :-)",
    "link": "/2011/11/curl-paper-how-to-not-decurl/",
    "image": null,
    "date": "November 18, 2011",
    "category": ["Printing"],
    "excerpt": "I ran into this video some years ago, and I&#8217;ve always, vividly, remembered it as one of the worst examples..."
},{
    "title": "Adobe 2011 Financial Analyst Meeting svela il futuro della Tecnologia al servizio dei Creativi",
    "link": "/2011/11/adobe-2011-financial-analyst-meeting-svela-futuro-tecnologia-creativi/",
    "image": "/wp-content/uploads/2011/11/Adobe-Financial-Analyst-Meeting.jpg",
    "date": "November 16, 2011",
    "category": ["Coding @it","Photoshop @it"],
    "excerpt": "E&#8217; proprio vero che ormai tutto fa capo alla Finanza. Ci sono più anteprime, demo e roadmap sulle tecnologie Adobe..."
},{
    "title": "Adobe 2011 Financial Analyst Meeting unveils the future of Creatives and Technology",
    "link": "/2011/11/adobe-2011-financial-analyst-meeting-unveils-the-future-creative-technology/",
    "image": "/wp-content/uploads/2011/11/Adobe-Financial-Analyst-Meeting.jpg",
    "date": "November 16, 2011",
    "category": ["Coding","Photoshop"],
    "excerpt": "Is it true that Finance rules the world now? Possibly. I&#8217;ve found more technology sneak peeks and roadmaps publicly disclosed..."
},{
    "title": "CS Extension Builder, aperto il programma di pre-release",
    "link": "/2011/11/cs-extension-builder-programma-prerelease/",
    "image": null,
    "date": "November 13, 2011",
    "category": ["Coding @it"],
    "excerpt": "Se stai pensando di sviluppare per la Adobe Creative Suite (in particolare estensioni), ci sono due notizie che ti devono..."
},{
    "title": "CS Extension Builder prerelease open",
    "link": "/2011/11/cs-extension-builder-prerelease-open/",
    "image": null,
    "date": "November 13, 2011",
    "category": ["Coding"],
    "excerpt": "For those of you interested in Adobe Creative Suite developing (that is, building extensions for CS apps) there are two..."
},{
    "title": "Color Correction Campus - slides on Sharpening",
    "link": "/2011/11/color-correction-campus-slides-on-sharpening-in-photoshop/",
    "image": null,
    "date": "November 8, 2011",
    "category": ["Photoshop"],
    "excerpt": "The lecture about sharpening I did last October for the Color Correction Campus went apparently so well that I’ve decided..."
},{
    "title": "CCC &#8211; Bologna, slides sullo Sharpening",
    "link": "/2011/11/ccc-bologna-slides-sullo-sharpening/",
    "image": null,
    "date": "November 8, 2011",
    "category": ["Photoshop @it"],
    "excerpt": "Visto l&#8217;inaspettato successo della mia breve lezione sulla maschera di contrasto per il CCC &#8211; Color Correction Campus di Marco Olivotto..."
},{
    "title": "Come scaricare Photoshop CS6 beta da Adobe",
    "link": "/2011/11/come-scaricare-photoshop-cs6-beta/",
    "image": null,
    "date": "November 6, 2011",
    "category": ["Photoshop @it"],
    "excerpt": "I primi screenshot di Photoshop CS6 beta stanno cominciando a circolare su internet, assieme a commenti e notizie. In larga..."
},{
    "title": "How to download Photoshop CS6 beta from Adobe",
    "link": "/2011/11/how-to-download-photoshop-cs6-beta-from-adobe/",
    "image": null,
    "date": "November 6, 2011",
    "category": ["Photoshop"],
    "excerpt": "Screenshots of Photoshop CS6 beta are starting to appear on the internet, with dressing of rumors and comments. By and..."
},{
    "title": "Hahnemuhle fa marcia indietro sui 64&#8243;",
    "link": "/2011/11/hahnemuhle-fa-marcia-indietro-sui-64/",
    "image": null,
    "date": "November 4, 2011",
    "category": ["Stampa"],
    "excerpt": "Pochissimo tempo dopo aver manifestato la volontà di interrompere le linee produttive a 64 pollici / 162 cm delle carte..."
},{
    "title": "Hahnemuhle changes its mind about 64&#8243;!",
    "link": "/2011/11/hahnemuhle-changes-its-mind-about-64/",
    "image": null,
    "date": "November 4, 2011",
    "category": ["Printing"],
    "excerpt": "After a short while from the decision to drop 64&#8243; production in the DFA (Digital Fine Art) line of papers,..."
},{
    "title": "Hahnemuhle cessa la produzione di carta 64&#8243;",
    "link": "/2011/11/hahnemuhle-cessa-la-produzione-di-carta-64/",
    "image": null,
    "date": "November 3, 2011",
    "category": ["Stampa"],
    "excerpt": "Se stai cercando carta Hahnemuhle PhotoRag Baryta oppure FineArt Baryta in rotolo da  64 pollici (164 cm) puoi fermarti qui &#8211; la..."
},{
    "title": "Hahnemuhle drops the production of 64&#8243; papers",
    "link": "/2011/11/hahnemuhle-64-inch-inkjet-paper/",
    "image": null,
    "date": "November 3, 2011",
    "category": ["Printing"],
    "excerpt": "Are you looking for Hahnemuhle PhotoRag Baryta or FineArt Baryta inkjet paper 64&#8243; rolls? Bad times, they&#8217;re going to discontinue the..."
},{
    "title": "Problemi con Adobe Extension Manager ed OSX Lion",
    "link": "/2011/11/problemi-con-adobe-extension-manager-ed-osx-lion/",
    "image": null,
    "date": "November 3, 2011",
    "category": ["Coding @it","Photoshop @it"],
    "excerpt": "&nbsp; &#8220;L&#8217;estensione non contiene una firma valida. L&#8217;estensione non sarà installata&#8221;. &nbsp; Se hai incontrato questo messaggio di errore tentando..."
},{
    "title": "Adobe Extension Manager and OSX Lion issue",
    "link": "/2011/10/adobe-extension-manager-and-lion-issue/",
    "image": null,
    "date": "October 31, 2011",
    "category": ["Coding","Photoshop"],
    "excerpt": "&nbsp; The extension does not contain valid signature. The extension will not be installed. &nbsp; If you&#8217;ve run into this..."
},{
    "title": "Codename: ZEBRA",
    "link": "/2011/10/codename-zebra/",
    "image": null,
    "date": "October 11, 2011",
    "category": ["Photoshop"],
    "excerpt": "I’m currently involved in a particularly challenging prepress job for a particularly prestigious publishing house, facing an astonishingly close deadline..."
},{
    "title": "Sharpening session at CCC",
    "link": "/2011/09/sharpening-session-at-ccc/",
    "image": null,
    "date": "September 29, 2011",
    "category": ["Photoshop"],
    "excerpt": "Next weekend I’ll be teaching at the CCC - Color Correction Campus in Bologna, Italy (a two day long, intense, Dan..."
}]

$(document).ready(function() {
    $('#search-input').on('keyup', function () {
        var resultdiv = $('#results-container');
        if (!resultdiv.is(':visible'))
            resultdiv.show();
        var query = $(this).val();
        var result = index.search(query);
        resultdiv.empty();
        $('.show-results-count').text(result.length + ' Results');
        for (var item in result) {
            var ref = result[item].ref;
            var searchitem = '<li><a href="'+ hostname + store[ref].link+'">'+store[ref].title+'</a></li>';
            resultdiv.append(searchitem);
        }
    });
});